abstract,acl_url,anthology,anthology_id,arxiv_url,authors,month,title,venue,year
,http://aclweb.org/anthology/D17-1000,D17-1,D17-1000,,"('Martha Palmer', 'Rebecca Hwa', 'Sebastian Riedel')",9,"
      Proceedings of the 2017 Conference on Empirical Methods in Natural
      Language Processing
    ",EMNLP,2017
"In this paper, we propose a novel BTG-forest-based alignment method. Based on a fast unsupervised initialization of parameters using variational IBM models, we synchronously parse parallel sentences top-down and align hierarchically under the constraint of BTG. Our two-step method can achieve the same run-time and comparable translation performance as fast_align while it yields smaller phrase tables. Final SMT results show that our method even outperforms in the experiment of distantly related languages, e.g., English-Japanese.",http://aclweb.org/anthology/D17-1001,D17-1,D17-1001,https://arxiv.org/abs/1711.07265,"('Yuki Arase', ""Jun'ichi Tsujii"")",9,Monolingual Phrase Alignment on Parse Forests,EMNLP,2017
,http://aclweb.org/anthology/D17-1002,D17-1,D17-1002,,"('Tianze Shi', 'Liang Huang', 'Lillian Lee')",9,"
      Fast(er) Exact Decoding and Global Training for Transition-Based
      Dependency Parsing via a Minimal Feature Set
    ",EMNLP,2017
"We propose a framework for parsing video and text jointly for understanding events and answering user queries. Our framework produces a parse graph that represents the compositional structures of spatial information (objects and scenes), temporal information (actions and events) and causal information (causalities between events and fluents) in the video and text. The knowledge representation of our framework is based on a spatial-temporal-causal And-Or graph (S/T/C-AOG), which jointly models possible hierarchical compositions of objects, scenes and events as well as their interactions and mutual contexts, and specifies the prior probabilistic distribution of the parse graphs. We present a probabilistic generative model for joint parsing that captures the relations between the input video/text, their corresponding parse graphs and the joint parse graph. Based on the probabilistic model, we propose a joint parsing system consisting of three modules: video parsing, text parsing and joint inference. Video parsing and text parsing produce two parse graphs from the input video and text respectively. The joint inference module produces a joint parse graph by performing matching, deduction and revision on the video and text parse graphs. The proposed framework has the following objectives: Firstly, we aim at deep semantic parsing of video and text that goes beyond the traditional bag-of-words approaches; Secondly, we perform parsing and reasoning across the spatial, temporal and causal dimensions based on the joint S/T/C-AOG representation; Thirdly, we show that deep joint parsing facilitates subsequent applications such as generating narrative text descriptions and answering queries in the forms of who, what, when, where and why. We empirically evaluated our system based on comparison against ground-truth as well as accuracy of query answering and obtained satisfactory results.",http://aclweb.org/anthology/D17-1003,D17-1,D17-1003,https://arxiv.org/abs/1308.6628,"('Junjie Cao', 'Sheng Huang', 'Weiwei Sun', 'Xiaojun Wan')",9,"Quasi-Second-Order Parsing for 1-Endpoint-Crossing, Pagenumber-2 Graphs",EMNLP,2017
"Spoken Language Systems at Saarland University (LSV) participated this year with 5 runs at the TAC KBP English slot filling track. Effective algorithms for all parts of the pipeline, from document retrieval to relation prediction and response post-processing, are bundled in a modular end-to-end relation extraction system called RelationFactory. The main run solely focuses on shallow techniques and achieved significant improvements over LSV's last year's system, while using the same training data and patterns. Improvements mainly have been obtained by a feature representation focusing on surface skip n-grams and improved scoring for extracted distant supervision patterns. Important factors for effective extraction are the training and tuning scheme for distant supervision classifiers, and the query expansion by a translation model based on Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, the submitted main run of the LSV RelationFactory system achieved the top-ranked F1-score of 37.3%.",http://aclweb.org/anthology/D17-1004,D17-1,D17-1004,https://arxiv.org/abs/1401.1158,"('Yuhao Zhang', 'Victor Zhong', 'Danqi Chen', 'Gabor Angeli', 'Christopher D. Manning')",9,Position-aware Attention and Supervised Data Improve Slot Filling,EMNLP,2017
,http://aclweb.org/anthology/D17-1005,D17-1,D17-1005,,"('Liyuan Liu', 'Xiang Ren', 'Qi Zhu', 'Shi Zhi', 'Huan Gui', 'Heng Ji', 'Jiawei Han')",9,"
      Heterogeneous Supervision for Relation Extraction: A Representation
      Learning Approach
    ",EMNLP,2017
"Script knowledge plays a central role in text understanding and is relevant for a variety of downstream tasks. In this paper, we consider two recent datasets which provide a rich and general representation of script events in terms of paraphrase sets. We introduce the task of mapping event mentions in narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible.",http://aclweb.org/anthology/D17-1006,D17-1,D17-1006,https://arxiv.org/abs/1710.05709,"('Zhongqing Wang', 'Yue Zhang', 'Ching-Yun Chang')",9,Integrating Order Information and Event Relation for Script Event Prediction,EMNLP,2017
"We present a simple yet effective approach for linking entities in queries. The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query. Then, we employ a rich set of features, such as link-probability, context-matching, word embeddings, and relatedness among candidate entities as well as their related entities, to rank the candidates under a regression based framework. The advantages of our approach lie in two aspects, which contribute to the ranking process and final linking result. First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query. Second, we can obtain the query sensitive prior probability in addition to the static link-probability derived from all Wikipedia articles. We conduct experiments on two benchmark datasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ dataset. Experimental results show that our method outperforms state-of-the-art systems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ dataset.",http://aclweb.org/anthology/D17-1007,D17-1,D17-1007,https://arxiv.org/abs/1704.02788,"('Chuanqi Tan', 'Furu Wei', 'Pengjie Ren', 'Weifeng Lv', 'Ming Zhou')",9,Entity Linking for Queries by Searching Wikipedia Sentences,EMNLP,2017
,http://aclweb.org/anthology/D17-1008,D17-1,D17-1008,,"('Tommaso Pasini', 'Roberto Navigli')",9,"
      Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in
      Multiple Languages without Manual Training Data
    ",EMNLP,2017
"The ability to consolidate information of different types is at the core of intelligence, and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others. In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings.",http://aclweb.org/anthology/D17-1009,D17-1,D17-1009,https://arxiv.org/abs/1805.00287,"('Siva Reddy', 'Oscar Täckström', 'Slav Petrov', 'Mark Steedman', 'Mirella Lapata')",9,Universal Semantic Parsing,EMNLP,2017
"Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings. Unlike prior work, MIMICK does not require re-training on the original word embedding corpus; instead, learning is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes. It is competitive with (and complementary to) a supervised character-based model in low-resource settings.",http://aclweb.org/anthology/D17-1010,D17-1,D17-1010,https://arxiv.org/abs/1707.06961,"('Yuval Pinter', 'Robert Guthrie', 'Jacob Eisenstein')",9,Mimicking Word Embeddings using Subword RNNs,EMNLP,2017
,http://aclweb.org/anthology/D17-1011,D17-1,D17-1011,,"('Ehsaneddin Asgari', 'Hinrich Schütze')",9,"
      Past, Present, Future: A Computational Investigation of the Typology of
      Tense in 1000 Languages
    ",EMNLP,2017
"This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, and thus the parser is optimized according to the translation objective. In experiments, we first show that our model compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models. We also show that the performance of our model can be further improved by pre-training it with a small amount of treebank annotations. Our final ensemble model significantly outperforms the previous best models on the standard English-to-Japanese translation dataset.",http://aclweb.org/anthology/D17-1012,D17-1,D17-1012,https://arxiv.org/abs/1702.02265,"('Kazuma Hashimoto', 'Yoshimasa Tsuruoka')",9,Neural Machine Translation with Source-Side Latent Graph Parsing,EMNLP,2017
"Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively.",http://aclweb.org/anthology/D17-1013,D17-1,D17-1013,https://arxiv.org/abs/1808.09006,"('Rongxiang Weng', 'Shujian Huang', 'Zaixiang Zheng', 'XIN-YU DAI', 'Jiajun CHEN')",9,Neural Machine Translation with Word Predictions,EMNLP,2017
"We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. We convert decoding - basically a discrete optimization problem - into a continuous optimization problem. The resulting constrained continuous optimisation problem is then tackled using gradient-based methods. Our powerful decoding framework enables decoding intractable models such as the intersection of left-to-right and right-to-left (bidirectional) as well as source-to-target and target-to-source (bilingual) NMT models. Our empirical results show that our decoding framework is effective, and leads to substantial improvements in translations generated from the intersected models where the typical greedy or beam search is not feasible. We also compare our framework against reranking, and analyse its advantages and disadvantages.",http://aclweb.org/anthology/D17-1014,D17-1,D17-1014,https://arxiv.org/abs/1701.02854,"('Cong Duy Vu Hoang', 'Gholamreza Haffari', 'Trevor Cohn')",9,Towards Decoding as Continuous Optimisation in Neural Machine Translation,EMNLP,2017
"The MISTY1 algorithm, proposed by Matsui in FSE 1997, is a block cipher with a 64-bit block size and a 128-bit key size. It was recommended by the European NESSIE project and the CRYPTREC project, and became one RFC in 2002 and an ISO standard in 2005, respectively. In this paper, we first investigate the properties of the FL linear function and identify 232 subkey- dependent zero-correlation linear approximations over 5-round MISTY1 with 3 FL layers. Fur- thermore, some observations on the FL, FO and FI functions are founded and based upon those observations, we select 27 subkey-dependent zero-correlation linear approximations and then, pro- pose the zero-correlation linear attacks on 7-round MISTY1 with 4 FL layers. Besides, for the case without FL layers, 27 zero-correlation linear approximations over 5-round MISTY1 are employed to the analysis of 7-round MISTY1. The zero-correlation linear attack on the 7-round with 4 FL layers needs about 2^{119:5} encryptions with 2^{62.9} known plaintexts and 2^61 memory bytes. For the attack on 7-round without FL layers, the data complexity is about 2^{63.9} known plaintexts, the time complexity is about 2^{81} encryptions and the memory requirements are about 2^{93} bytes. Both have lower time complexity than previous attacks.",http://aclweb.org/anthology/D17-1015,D17-1,D17-1015,https://arxiv.org/abs/1410.4312,"('Nikita Kitaev', 'Dan Klein')",9,Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space,EMNLP,2017
,http://aclweb.org/anthology/D17-1016,D17-1,D17-1016,,"('Afshin Rahimi', 'Timothy Baldwin', 'Trevor Cohn')",9,"
      Continuous Representation of Location for Geolocation and Lexical
      Dialectology using Mixture Density Networks
    ",EMNLP,2017
"Generating captions for images is a task that has recently received considerable attention. In this work we focus on caption generation for abstract scenes, or object layouts where the only information provided is a set of objects and their locations. We propose OBJ2TEXT, a sequence-to-sequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model. We show that our model, despite encoding object layouts as a sequence, can represent spatial relationships between objects, and generate descriptions that are globally coherent and semantically relevant. We test our approach in a task of object-layout captioning by using only object annotations as inputs. We additionally show that our model, combined with a state-of-the-art object detector, improves an image captioning model from 0.863 to 0.950 (CIDEr score) in the test benchmark of the standard MS-COCO Captioning task.",http://aclweb.org/anthology/D17-1017,D17-1,D17-1017,https://arxiv.org/abs/1707.07102,"('Xuwang Yin', 'Vicente Ordonez')",9,Obj2Text: Generating Visually Descriptive Language from Object Layouts,EMNLP,2017
"Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on the CoNLL dataset, they struggle to generalize properly to new domains or datasets. In this paper, we investigate the role of linguistic features in building more generalizable coreference resolvers. We show that generalization improves only slightly by merely using a set of additional linguistic features. However, employing features and subsets of their values that are informative for coreference resolution, considerably improves generalization. Thanks to better generalization, our system achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our system, which is trained on CoNLL, achieves on-par performance with a system designed for this dataset.",http://aclweb.org/anthology/D17-1018,D17-1,D17-1018,https://arxiv.org/abs/1708.00160,"('Kenton Lee', 'Luheng He', 'Mike Lewis', 'Luke Zettlemoyer')",9,End-to-end Neural Coreference Resolution,EMNLP,2017
"Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on measuring individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) in narrow domains.   In this paper, we describe domain-independent neural models of discourse coherence that are capable of measuring multiple aspects of coherence in existing sentences and can maintain coherence while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latent-variable Markovian generative model that captures the latent discourse dependencies between sentences in a text.   Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts.",http://aclweb.org/anthology/D17-1019,D17-1,D17-1019,https://arxiv.org/abs/1606.01545,"('Jiwei Li', 'Dan Jurafsky')",9,Neural Net Models of Open-domain Discourse Coherence,EMNLP,2017
"The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.",http://aclweb.org/anthology/D17-1020,D17-1,D17-1020,https://arxiv.org/abs/1507.02907,"('Kexiang Wang', 'Tianyu Liu', 'Zhifang Sui', 'Baobao Chang')",9,Affinity-Preserving Random Walk for Multi-Document Summarization,EMNLP,2017
"Resolving abstract anaphora is an important, but difficult task for text understanding. Yet, with recent advances in representation learning this task becomes a more tangible aim. A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent. We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net. We overcome the lack of training data by generating artificial anaphoric sentence--antecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus. This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders. We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors. Our model selects syntactically plausible candidates and -- if disregarding syntax -- discriminates candidates using deeper features.",http://aclweb.org/anthology/D17-1021,D17-1,D17-1021,https://arxiv.org/abs/1706.02256,"('Ana Marasovic', 'Leo Born', 'Juri Opitz', 'Anette Frank')",9,A Mention-Ranking Model for Abstract Anaphora Resolution,EMNLP,2017
"We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym$-$hyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state$-$of$-$the$-$art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.",http://aclweb.org/anthology/D17-1022,D17-1,D17-1022,https://arxiv.org/abs/1707.07273,"('Kim Anh Nguyen', 'Maximilian Köper', 'Sabine Schulte im Walde', 'Ngoc Thang Vu')",9,Hierarchical Embeddings for Hypernymy Detection and Directionality,EMNLP,2017
,http://aclweb.org/anthology/D17-1023,D17-1,D17-1023,,"('Zhe Zhao', 'Tao Liu', 'Shen Li', 'Bofang Li', 'Xiaoyong Du')",9,"
      Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence
      Statistics
    ",EMNLP,2017
"Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions. We present two applications of these architectures: ""reverse dictionaries"" that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing models that understand phrases and sentences.",http://aclweb.org/anthology/D17-1024,D17-1,D17-1024,https://arxiv.org/abs/1504.00548,"('Julien Tissier', 'Christopher Gravier', 'Amaury Habrard')",9,Dict2vec : Learning Word Embeddings using Lexical Dictionaries,EMNLP,2017
"In this paper, we propose new methods to learn Chinese word representations. Chinese characters are composed of graphical components, which carry rich semantics. It is common for a Chinese learner to comprehend the meaning of a word from these graphical components. As a result, we propose models that enhance word representations by character glyphs. The character glyph features are directly learned from the bitmaps of characters by convolutional auto-encoder(convAE), and the glyph features improve Chinese word representations which are already enhanced by character embeddings. Another contribution in this paper is that we created several evaluation datasets in traditional Chinese and made them public.",http://aclweb.org/anthology/D17-1025,D17-1,D17-1025,https://arxiv.org/abs/1708.04755,"('Tzu-ray Su', 'Hung-yi Lee')",9,Learning Chinese Word Representations From Glyphs Of Characters,EMNLP,2017
"We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use neural machine translation to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embeddings. We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains. We experiment with several language pairs and data sources, and develop a variety of data filtering techniques. In the process, we explore how neural machine translation output differs from human-written sentences, finding clear differences in length, the amount of repetition, and the use of rare words.",http://aclweb.org/anthology/D17-1026,D17-1,D17-1026,https://arxiv.org/abs/1706.01847,"('John Wieting', 'Jonathan Mallinson', 'Kevin Gimpel')",9,Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext,EMNLP,2017
,http://aclweb.org/anthology/D17-1027,D17-1,D17-1027,,"('Jinxing Yu', 'Xun Jian', 'Hao Xin', 'Yangqiu Song')",9,"
      Joint Embeddings of Chinese Words, Characters, and Fine-grained
      Subcharacter Components
    ",EMNLP,2017
"Neural network techniques are widely applied to obtain high-quality distributed representations of words, i.e., word embeddings, to address text mining, information retrieval, and natural language processing tasks. Recently, efficient methods have been proposed to learn word embeddings from context that captures both semantic and syntactic relationships between words. However, it is challenging to handle unseen words or rare words with insufficient context. In this paper, inspired by the study on word recognition process in cognitive psychology, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges. In particular, we introduce a novel neural network architecture called KNET that leverages both contextual information and morphological word similarity built based on morphological knowledge to learn word embeddings. Meanwhile, the learning architecture is also able to refine the pre-defined morphological knowledge and obtain more accurate word similarity. Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings.",http://aclweb.org/anthology/D17-1028,D17-1,D17-1028,https://arxiv.org/abs/1407.1687,"('Arihant Gupta', 'Syed Sarfaraz Akhtar', 'Avijit Vajpayee', 'Arjit Srivastava', 'Madan Gopal Jhanwar', 'Manish Shrivastava')",9,Exploiting Morphological Regularities in Distributional Word Representations,EMNLP,2017
"Chinese word segmentation (CWS) is an important task for Chinese NLP. Recently, many neural network based methods have been proposed for CWS. However, these methods require a large number of labeled sentences for model training, and usually cannot utilize the useful information in Chinese dictionary. In this paper, we propose two methods to exploit the dictionary information for CWS. The first one is based on pseudo labeled data generation, and the second one is based on multi-task learning. The experimental results on two benchmark datasets validate that our approach can effectively improve the performance of Chinese word segmentation, especially when training data is insufficient.",http://aclweb.org/anthology/D17-1029,D17-1,D17-1029,https://arxiv.org/abs/1807.05849,"('Shaonan Wang', 'Jiajun Zhang', 'Chengqing Zong')",9,Exploiting Word Internal Structures for Generic Chinese Sentence Representation,EMNLP,2017
"Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn 'a good vector' for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences' worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.",http://aclweb.org/anthology/D17-1030,D17-1,D17-1030,https://arxiv.org/abs/1707.06556,"('Aurélie Herbelot', 'Marco Baroni')",9,High-risk learning: acquiring new word vectors from tiny data,EMNLP,2017
"As a generative model for building end-to-end dialogue systems, Hierarchical Recurrent Encoder-Decoder (HRED) consists of three layers of Gated Recurrent Unit (GRU), which from bottom to top are separately used as the word-level encoder, the sentence-level encoder, and the decoder. Despite performing well on dialogue corpora, HRED is computationally expensive to train due to its complexity. To improve the training efficiency of HRED, we propose a new model, which is named as Simplified HRED (SHRED), by making each layer of HRED except the top one simpler than its upper layer. On the one hand, we propose Scalar Gated Unit (SGU), which is a simplified variant of GRU, and use it as the sentence-level encoder. On the other hand, we use Fixed-size Ordinally-Forgetting Encoding (FOFE), which has no trainable parameter at all, as the word-level encoder. The experimental results show that compared with HRED under the same word embedding size and the same hidden state size for each layer, SHRED reduces the number of trainable parameters by 25\%--35\%, and the training time by more than 50\%, but still achieves slightly better performance.",http://aclweb.org/anthology/D17-1031,D17-1,D17-1031,https://arxiv.org/abs/1809.02790,"('Joseph Sanu', 'Mingbin Xu', 'Hui Jiang', 'Quan Liu')",9,Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding,EMNLP,2017
"Topic models have been widely used in discovering latent topics which are shared across documents in text mining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-art methods on various NLP tasks.",http://aclweb.org/anthology/D17-1032,D17-1,D17-1032,https://arxiv.org/abs/1702.07117,"('Jared Fernandez', 'Zhaocheng Yu', 'Doug Downey')",9,VecShare: A Framework for Sharing Word Representation Vectors,EMNLP,2017
"This paper is concerned with the training of recurrent neural networks as goal-oriented dialog agents using reinforcement learning. Training such agents with policy gradients typically requires a large amount of samples. However, the collection of the required data in form of conversations between chat-bots and human agents is time-consuming and expensive. To mitigate this problem, we describe an efficient policy gradient method using positive memory retention, which significantly increases the sample-efficiency. We show that our method is 10 times more sample-efficient than policy gradients in extensive experiments on a new synthetic number guessing game. Moreover, in a real-word visual object discovery game, the proposed method is twice as sample-efficient as policy gradients and shows state-of-the-art performance.",http://aclweb.org/anthology/D17-1033,D17-1,D17-1033,https://arxiv.org/abs/1810.01371,"('Souleiman Hasan', 'Edward Curry')",9,Word Re-Embedding via Manifold Dimensionality Retention,EMNLP,2017
"Unsupervised learned representations of polysemous words generate a large of pseudo multi senses since unsupervised methods are overly sensitive to contextual variations. In this paper, we address the pseudo multi-sense detection for word embeddings by dimensionality reduction of sense pairs. We propose a novel principal analysis method, termed Ex-RPCA, designed to detect both pseudo multi senses and real multi senses. With Ex-RPCA, we empirically show that pseudo multi senses are generated systematically in unsupervised method. Moreover, the multi-sense word embeddings can by improved by a simple linear transformation based on Ex-RPCA. Our improved word embedding outperform the original one by 5.6 points on Stanford contextual word similarity (SCWS) dataset. We hope our simple yet effective approach will help the linguistic analysis of multi-sense word embeddings in the future.",http://aclweb.org/anthology/D17-1034,D17-1,D17-1034,https://arxiv.org/abs/1803.01255,"('Guang-He Lee', 'Yun-Nung Chen')",9,MUSE: Modularizing Unsupervised Sense Embeddings,EMNLP,2017
,http://aclweb.org/anthology/D17-1035,D17-1,D17-1035,,"('Nils Reimers', 'Iryna Gurevych')",9,"
      Reporting Score Distributions Makes a Difference: Performance Study of
      LSTM-networks for Sequence Tagging
    ",EMNLP,2017
"Machine learning methods incorporating deep neural networks have been the subject of recent proposals for new hadronic resonance taggers. These methods require training on a dataset produced by an event generator where the true class labels are known. However, training a network on a specific event generator may bias the network towards learning features associated with the approximations to QCD used in that generator which are not present in real data. We therefore investigate the effects of variations in the modelling of the parton shower on the performance of deep neural network taggers using jet images from hadronic W-bosons at the LHC, including detector-related effects. By investigating network performance on samples from the Pythia, Herwig and Sherpa generators, we find differences of up to fifty percent in background rejection for fixed signal efficiency. We also introduce and study a method, which we dub zooming, for implementing scale-invariance in neural network-based taggers. We find that this leads to an improvement in performance across a wide range of jet transverse momenta. Our results emphasise the importance gaining a detailed understanding what aspects of jet physics these methods are exploiting.",http://aclweb.org/anthology/D17-1036,D17-1,D17-1036,https://arxiv.org/abs/1609.00607,"('André F. T. Martins', 'Julia Kreutzer')",9,Learning What's Easy: Fully Differentiable Neural Easy-First Taggers,EMNLP,2017
"This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SGNS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SGNS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.",http://aclweb.org/anthology/D17-1037,D17-1,D17-1037,https://arxiv.org/abs/1704.03956,"('Nobuhiro Kaji', 'Hayato Kobayashi')",9,Incremental Skip-gram Model with Negative Sampling,EMNLP,2017
"Domain similarity measures can be used to gauge adaptability and select suitable data for transfer learning, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to \emph{learn} data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are -- to some degree -- transferable across models, domains, and even tasks.",http://aclweb.org/anthology/D17-1038,D17-1,D17-1038,https://arxiv.org/abs/1707.05246,"('Sebastian Ruder', 'Barbara Plank')",9,Learning to select data for transfer learning with Bayesian Optimization,EMNLP,2017
"We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a ""pretraining"" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.",http://aclweb.org/anthology/D17-1039,D17-1,D17-1039,https://arxiv.org/abs/1511.01432,"('Prajit Ramachandran', 'Peter Liu', 'Quoc Le')",9,Unsupervised Pretraining for Sequence to Sequence Learning,EMNLP,2017
"The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative attention mechanism based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20% for real-world translation tasks and more for tasks with longer sequences. By visualizing attention scores we demonstrate that our models learn distinct, meaningful alignments.",http://aclweb.org/anthology/D17-1040,D17-1,D17-1040,https://arxiv.org/abs/1707.00110,"('Denny Britz', 'Melody Guan', 'Minh-Thang Luong')",9,Efficient Attention using a Fixed-Size Memory Representation,EMNLP,2017
"Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches.",http://aclweb.org/anthology/D17-1041,D17-1,D17-1041,https://arxiv.org/abs/1506.05230,"('Sungjoon Park', 'JinYeong Bak', 'Alice Oh')",9,Rotated Word Vector Representations and their Interpretability,EMNLP,2017
,http://aclweb.org/anthology/D17-1042,D17-1,D17-1042,,"('David Alvarez-Melis', 'Tommi Jaakkola')",9,"
      A causal framework for explaining the predictions of black-box
      sequence-to-sequence models
    ",EMNLP,2017
"Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.",http://aclweb.org/anthology/D17-1043,D17-1,D17-1043,https://arxiv.org/abs/1612.00377,"('Iulian Vlad Serban', 'Alexander G. Ororbia', 'Joelle Pineau', 'Aaron Courville')",9,Piecewise Latent Variables for Neural Variational Text Processing,EMNLP,2017
"The problem of learning a manifold structure on a dataset is framed in terms of a generative model, to which we use ideas behind autoencoders (namely adversarial/Wasserstein autoencoders) to fit deep neural networks. From a machine learning perspective, the resulting structure, an atlas of a manifold, may be viewed as a combination of dimensionality reduction and ""fuzzy"" clustering.",http://aclweb.org/anthology/D17-1044,D17-1,D17-1044,https://arxiv.org/abs/1803.00156,"('Thomas Lavergne', 'François Yvon')",9,Learning the Structure of Variable-Order CRFs: a finite-state perspective,EMNLP,2017
"We make distributed stochastic gradient descent faster by exchanging sparse updates instead of dense updates. Gradient updates are positively skewed as most updates are near zero, so we map the 99% smallest updates (by absolute value) to zero then exchange sparse matrices. This method can be combined with quantization to further improve the compression. We explore different configurations and apply them to neural machine translation and MNIST image classification tasks. Most configurations work on MNIST, whereas different configurations reduce convergence rate on the more complex translation task. Our experiments show that we can achieve up to 49% speed up on MNIST and 22% on NMT without damaging the final accuracy or BLEU.",http://aclweb.org/anthology/D17-1045,D17-1,D17-1045,https://arxiv.org/abs/1704.05021,"('Alham Fikri Aji', 'Kenneth Heafield')",9,Sparse Communication for Distributed Gradient Descent,EMNLP,2017
"Adaptive stochastic gradient descent methods, such as AdaGrad, RMSProp, Adam, AMSGrad, etc., have been demonstrated efficacious in solving non-convex stochastic optimization, such as training deep neural networks. However, their convergence rates have not been touched under the non-convex stochastic circumstance except recent breakthrough results on AdaGrad, perturbed AdaGrad and AMSGrad. In this paper, we propose two new adaptive stochastic gradient methods called AdaHB and AdaNAG which integrate a novel weighted coordinate-wise AdaGrad with heavy ball momentum and Nesterov accelerated gradient momentum, respectively. The $\mathcal{O}(\frac{\log{T}}{\sqrt{T}})$ non-asymptotic convergence rates of AdaHB and AdaNAG in non-convex stochastic setting are also jointly established by leveraging a newly developed unified formulation of these two momentum mechanisms. Moreover, comparisons have been made between AdaHB, AdaNAG, Adam and RMSProp, which, to a certain extent, explains the reasons why Adam and RMSProp are divergent. In particular, when momentum term vanishes we obtain convergence rate of coordinate-wise AdaGrad in non-convex stochastic setting as a byproduct.",http://aclweb.org/anthology/D17-1046,D17-1,D17-1046,https://arxiv.org/abs/1808.03408,"('You Lu', 'Jeffrey Lund', 'Jordan Boyd-Graber')",9,Why ADAGRAD Fails for Online Topic Modeling,EMNLP,2017
"With the development of the Internet, natural language processing (NLP), in which sentiment analysis is an important task, became vital in information processing.Sentiment analysis includes aspect sentiment classification. Aspect sentiment can provide complete and in-depth results with increased attention on aspect-level. Different context words in a sentence influence the sentiment polarity of a sentence variably, and polarity varies based on the different aspects in a sentence. Take the sentence, 'I bought a new camera. The picture quality is amazing but the battery life is too short.'as an example. If the aspect is picture quality, then the expected sentiment polarity is 'positive', if the battery life aspect is considered, then the sentiment polarity should be 'negative'; therefore, aspect is important to consider when we explore aspect sentiment in the sentence. Recurrent neural network (RNN) is regarded as a good model to deal with natural language processing, and RNNs has get good performance on aspect sentiment classification including Target-Dependent LSTM (TD-LSTM) ,Target-Connection LSTM (TC-LSTM) (Tang, 2015a, b), AE-LSTM, AT-LSTM, AEAT-LSTM (Wang et al., 2016).There are also extensive literatures on sentiment classification utilizing convolutional neural network, but there is little literature on aspect sentiment classification using convolutional neural network. In our paper, we develop attention-based input layers in which aspect information is considered by input layer. We then incorporate attention-based input layers into convolutional neural network (CNN) to introduce context words information. In our experiment, incorporating aspect information into CNN improves the latter's aspect sentiment classification performance without using syntactic parser or external sentiment lexicons in a benchmark dataset from Twitter but get better performance compared with other models.",http://aclweb.org/anthology/D17-1047,D17-1,D17-1047,https://arxiv.org/abs/1807.01704,"('Peng Chen', 'Zhongqian Sun', 'Lidong Bing', 'Wei Yang')",9,Recurrent Attention Network on Memory for Aspect Sentiment Analysis,EMNLP,2017
"Sentiment analysis on large-scale social media data is important to bridge the gaps between social media contents and real world activities including political election prediction, individual and public emotional status monitoring and analysis, and so on. Although textual sentiment analysis has been well studied based on platforms such as Twitter and Instagram, analysis of the role of extensive emoji uses in sentiment analysis remains light. In this paper, we propose a novel scheme for Twitter sentiment analysis with extra attention on emojis. We first learn bi-sense emoji embeddings under positive and negative sentimental tweets individually, and then train a sentiment classifier by attending on these bi-sense emoji embeddings with an attention-based long short-term memory network (LSTM). Our experiments show that the bi-sense embedding is effective for extracting sentiment-aware embeddings of emojis and outperforms the state-of-the-art models. We also visualize the attentions to show that the bi-sense emoji embedding provides better guidance on the attention mechanism to obtain a more robust understanding of the semantics and sentiments.",http://aclweb.org/anthology/D17-1048,D17-1,D17-1048,https://arxiv.org/abs/1807.07961,"('Yunfei Long', 'Lu Qin', 'Rong Xiang', 'Minglei Li', 'Chu-Ren Huang')",9,A Cognition Based Attention Model for Sentiment Analysis,EMNLP,2017
,http://aclweb.org/anthology/D17-1049,D17-1,D17-1049,,"('Lahari Poddar', 'Wynne Hsu', 'Mong Li Lee')",9,"
      Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions
      from Reviews
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1050,D17-1,D17-1050,,"('Aniruddha Ghosh', 'Tony Veale')",9,"
      Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very
      Personal
    ",EMNLP,2017
"The tremendous amount of user generated data through social networking sites led to the gaining popularity of automatic text classification in the field of computational linguistics over the past decade. Within this domain, one problem that has drawn the attention of many researchers is automatic humor detection in texts. In depth semantic understanding of the text is required to detect humor which makes the problem difficult to automate. With increase in the number of social media users, many multilingual speakers often interchange between languages while posting on social media which is called code-mixing. It introduces some challenges in the field of linguistic analysis of social media content (Barman et al., 2014), like spelling variations and non-grammatical structures in a sentence. Past researches include detecting puns in texts (Kao et al., 2016) and humor in one-lines (Mihalcea et al., 2010) in a single language, but with the tremendous amount of code-mixed data available online, there is a need to develop techniques which detects humor in code-mixed tweets. In this paper, we analyze the task of humor detection in texts and describe a freely available corpus containing English-Hindi code-mixed tweets annotated with humorous(H) or non-humorous(N) tags. We also tagged the words in the tweets with Language tags (English/Hindi/Others). Moreover, we describe the experiments carried out on the corpus and provide a baseline classification system which distinguishes between humorous and non-humorous texts.",http://aclweb.org/anthology/D17-1051,D17-1,D17-1051,https://arxiv.org/abs/1806.05513,"('Alex Morales', 'Chengxiang Zhai')",9,Identifying Humor in Reviews using Background Text Sources,EMNLP,2017
,http://aclweb.org/anthology/D17-1052,D17-1,D17-1052,,"('Leyi Wang', 'Rui Xia')",9,"
      Sentiment Lexicon Construction with Representation Learning Based on
      Hierarchical Sentiment Supervision
    ",EMNLP,2017
"Sentiment Analysis aims to get the underlying viewpoint of the text, which could be anything that holds a subjective opinion, such as an online review, Movie rating, Comments on Blog posts etc. This paper presents a novel approach that classify text in two-dimensional Emotional space, based on the sentiments of the author. The approach uses existing lexical resources to extract feature set, which is trained using Supervised Learning techniques.",http://aclweb.org/anthology/D17-1053,D17-1,D17-1053,https://arxiv.org/abs/1406.2022,"('Kui Xu', 'Xiaojun Wan')",9,Towards a Universal Sentiment Classifier in Multiple languages,EMNLP,2017
,http://aclweb.org/anthology/D17-1054,D17-1,D17-1054,,['Zi-Yi Dou'],9,"
      Capturing User and Product Information for Document Level Sentiment
      Analysis with Deep Memory Network
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1055,D17-1,D17-1055,,"('Min Yang', 'Jincheng Mei', 'Heng Ji', 'zhao wei', 'Zhou Zhao', 'Xiaojun Chen')",9,"
      Identifying and Tracking Sentiments and Topics from Social Media Texts
      during Natural Disasters
    ",EMNLP,2017
"Most of existing work learn sentiment-specific word representation for improving Twitter sentiment classification, which encoded both n-gram and distant supervised tweet sentiment information in learning process. They assume all words within a tweet have the same sentiment polarity as the whole tweet, which ignores the word its own sentiment polarity. To address this problem, we propose to learn sentiment-specific word embedding by exploiting both lexicon resource and distant supervised information. We develop a multi-level sentiment-enriched word embedding learning method, which uses parallel asymmetric neural network to model n-gram, word level sentiment and tweet level sentiment in learning process. Experiments on standard benchmarks show our approach outperforms state-of-the-art methods.",http://aclweb.org/anthology/D17-1056,D17-1,D17-1056,https://arxiv.org/abs/1611.00126,"('Liang-Chih Yu', 'Jin Wang', 'K. Robert Lai', 'Xuejie Zhang')",9,Refining Word Embeddings for Sentiment Analysis,EMNLP,2017
,http://aclweb.org/anthology/D17-1057,D17-1,D17-1057,,"('Md Shad Akhtar', 'Abhishek Kumar', 'Deepanway Ghosal', 'Asif Ekbal', 'Pushpak Bhattacharyya')",9,"
      A Multilayer Perceptron based Ensemble Technique for Fine-grained
      Financial Sentiment Analysis
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1058,D17-1,D17-1058,,"('Raksha Sharma', 'Arpan Somani', 'Lakshya Kumar', 'Pushpak Bhattacharyya')",9,"
      Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word
      Embeddings
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1059,D17-1,D17-1059,,"('Yasheng Wang', 'Yang Zhang', 'Bing Liu')",9,"
      Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary
      Lookup, and Polarity Association
    ",EMNLP,2017
"We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.",http://aclweb.org/anthology/D17-1060,D17-1,D17-1060,https://arxiv.org/abs/1707.06690,"('Wenhan Xiong', 'Thien Hoang', 'William Yang Wang')",9,DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning,EMNLP,2017
"Search engines play an important role in our everyday lives by assisting us in finding the information we need. When we input a complex query, however, results are often far from satisfactory. In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned. We train this neural network with reinforcement learning. The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall. We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20% in terms of recall. Furthermore, we present a simple method to estimate a conservative upper-bound performance of a model in a particular environment and verify that there is still large room for improvements.",http://aclweb.org/anthology/D17-1061,D17-1,D17-1061,https://arxiv.org/abs/1704.04572,"('Rodrigo Nogueira', 'Kyunghyun Cho')",9,Task-Oriented Query Reformulation with Reinforcement Learning,EMNLP,2017
"Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call {\sc Dress} (as shorthand for {\bf D}eep {\bf RE}inforcement {\bf S}entence {\bf S}implification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.",http://aclweb.org/anthology/D17-1062,D17-1,D17-1062,https://arxiv.org/abs/1703.10931,"('Xingxing Zhang', 'Mirella Lapata')",9,Sentence Simplification with Deep Reinforcement Learning,EMNLP,2017
"Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.",http://aclweb.org/anthology/D17-1063,D17-1,D17-1063,https://arxiv.org/abs/1708.02383,"('Meng Fang', 'Yuan Li', 'Trevor Cohn')",9,Learning how to Active Learn: A Deep Reinforcement Learning Approach,EMNLP,2017
"Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in NLP. We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89% of the unique simple sentences from the validation and test sets. To aid this, we present a new train-development-test data split and neural models augmented with a copy-mechanism, outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task.",http://aclweb.org/anthology/D17-1064,D17-1,D17-1064,https://arxiv.org/abs/1805.01035,"('Shashi Narayan', 'Claire Gardent', 'Shay B. Cohen', 'Anastasia Shimorina')",9,Split and Rephrase,EMNLP,2017
"We present a study on the design, growth and optical characterization of a GaN/AlGaN microcavity for the enhancement of second order non linear effects. The proposed system exploits the high second order nonlinear optical response of GaN due to the non centrosymmetric crystalline structure of this material. It consists of a GaN cavity embedded between two GaN/AlGaN Distributed Bragg Reflectors designed for a reference mode coincident with a second harmonic field generated in the near UV region (~ 400 nm). Critical issues for this target are the crystalline quality of the material, together with sharp and abrupt interfaces among the multi-stacked layers. A detailed investigation on the growth evolution of GaN and AlGaN epilayers in such a configuration is reported, with the aim to obtain high quality factor in the desiderated spectral range. Non linear second harmonic generation experiments have been performed and the results were compared with bulk GaN sample, highlighting the effect of the microcavity on the non linear optical response of this material.",http://aclweb.org/anthology/D17-1065,D17-1,D17-1065,https://arxiv.org/abs/1104.2277,"('Zhen Xu', 'Bingquan Liu', 'Baoxun Wang', 'Chengjie SUN', 'Xiaolong Wang', 'Zhuoran Wang', 'Chao Qi')",9,Neural Response Generation via GAN with an Approximate Embedding Layer,EMNLP,2017
"In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data.",http://aclweb.org/anthology/D17-1066,D17-1,D17-1066,https://arxiv.org/abs/1702.02390,"('Stanislau Semeniuta', 'Aliaksei Severyn', 'Erhardt Barth')",9,A Hybrid Convolutional Variational Autoencoder for Text Generation,EMNLP,2017
"Computational Humor involves several tasks, such as humor recognition, humor generation, and humor scoring, for which it is useful to have human-curated data. In this work we present a corpus of 27,000 tweets written in Spanish and crowd-annotated by their humor value and funniness score, with about four annotations per tweet, tagged by 1,300 people over the Internet. It is equally divided between tweets coming from humorous and non-humorous accounts. The inter-annotator agreement Krippendorff's alpha value is 0.5710. The dataset is available for general use and can serve as a basis for humor detection and as a first step to tackle subjectivity.",http://aclweb.org/anthology/D17-1067,D17-1,D17-1067,https://arxiv.org/abs/1710.00477,"('Nabil Hossain', 'John Krumm', 'Lucy Vanderwende', 'Eric Horvitz', 'Henry Kautz')",9,Filling the Blanks (hint: plural noun) for Mad Libs Humor,EMNLP,2017
"In this paper, we introduce a new distributional method for modeling predicate-argument thematic fit judgments. We use a syntax-based DSM to build a prototypical representation of verb-specific roles: for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes. Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-the-art system, and achieves better or comparable results to those reported in the literature for the other unsupervised systems. Moreover, it provides an explicit representation of the features characterizing verb-specific semantic roles.",http://aclweb.org/anthology/D17-1068,D17-1,D17-1068,https://arxiv.org/abs/1707.05967,"('Enrico Santus', 'Emmanuele Chersoni', 'Alessandro Lenci', 'Philippe Blache')",9,Measuring Thematic Fit with Distributional Feature Overlap,EMNLP,2017
,http://aclweb.org/anthology/D17-1069,D17-1,D17-1069,,"('Dheeraj Mekala', 'Vivek Gupta', 'Bhargavi Paranjape', 'Harish Karnick')",9,"
      SCDV : Sparse Composite Document Vectors using soft clustering over
      distributional representations
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1070,D17-1,D17-1070,,"('Alexis Conneau', 'Douwe Kiela', 'Holger Schwenk', 'Loïc Barrault', 'Antoine Bordes')",9,"
      Supervised Learning of Universal Sentence Representations from Natural
      Language Inference Data
    ",EMNLP,2017
"Determining semantic textual similarity is a core research subject in natural language processing. Since vector-based models for sentence representation often use shallow information, capturing accurate semantics is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for determining semantic textual similarity by combining shallow features with features extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higher-order automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our system was able to outperform other logic-based systems and that features derived from the proofs are effective for learning textual similarity.",http://aclweb.org/anthology/D17-1071,D17-1,D17-1071,https://arxiv.org/abs/1707.08713,"('Hitomi Yanaka', 'Koji Mineshima', 'Pascual Martínez-Gómez', 'Daisuke Bekki')",9,Determining Semantic Textual Similarity using Natural Deduction Proofs,EMNLP,2017
"We present an iterative procedure to build a Chinese language model (LM). We segment Chinese text into words based on a word-based Chinese language model. However, the construction of a Chinese LM itself requires word boundaries. To get out of the chicken-and-egg problem, we propose an iterative procedure that alternates two operations: segmenting text into words and building an LM. Starting with an initial segmented corpus and an LM based upon it, we use a Viterbi-liek algorithm to segment another set of data. Then, we build an LM based on the second set and use the resulting LM to segment again the first corpus. The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors. Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation, but discovers unseen words surprisingly well. The resulting word-based LM has a perplexity of 188 for a general Chinese corpus.",http://aclweb.org/anthology/D17-1072,D17-1,D17-1072,https://arxiv.org/abs/cmp-lg/9606021,"('Chen Gong', 'Zhenghua Li', 'Min Zhang', 'Xinzhou Jiang')",9,Multi-Grained Chinese Word Segmentation,EMNLP,2017
,http://aclweb.org/anthology/D17-1073,D17-1,D17-1073,,"('Nasser Zalmout', 'Nizar Habash')",9,"
      Don't Throw Those Morphological Analyzers Away Just Yet: Neural
      Morphological Disambiguation for Arabic
    ",EMNLP,2017
"The generation of complex derived word forms has been an overlooked problem in NLP; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a parallel to inflectional paradigm completion. State-of-the-art neural models, adapted from the inflection task, are able to learn a range of derivation patterns, and outperform a non-neural baseline by 16.4%. However, due to semantic, historical, and lexical considerations involved in derivational morphology, future work will be needed to achieve performance parity with inflection-generating systems.",http://aclweb.org/anthology/D17-1074,D17-1,D17-1074,https://arxiv.org/abs/1708.09151,"('Ryan Cotterell', 'Ekaterina Vylomova', 'Huda Khayrallah', 'Christo Kirov', 'David Yarowsky')",9,Paradigm Completion for Derivational Morphology,EMNLP,2017
"Spoken language processing requires speech and natural language integration. Moreover, spoken Korean calls for unique processing methodology due to its linguistic characteristics. This paper presents SKOPE, a connectionist/symbolic spoken Korean processing engine, which emphasizes that: 1) connectionist and symbolic techniques must be selectively applied according to their relative strength and weakness, and 2) the linguistic characteristics of Korean must be fully considered for phoneme recognition, speech and language integration, and morphological/syntactic processing. The design and implementation of SKOPE demonstrates how connectionist/symbolic hybrid architectures can be constructed for spoken agglutinative language processing. Also SKOPE presents many novel ideas for speech and language processing. The phoneme recognition, morphological analysis, and syntactic analysis experiments show that SKOPE is a viable approach for the spoken Korean processing.",http://aclweb.org/anthology/D17-1075,D17-1,D17-1075,https://arxiv.org/abs/cmp-lg/9504008,['Karl Stratos'],9,A Sub-Character Architecture for Korean Language Processing,EMNLP,2017
"There is a recent trend in handwritten text recognition with deep neural networks to replace 2D recurrent layers with 1D, and in some cases even completely remove the recurrent layers, relying on simple feed-forward convolutional only architectures. The most used type of recurrent layer is the Long-Short Term Memory (LSTM). The motivations to do so are many: there are few open-source implementations of 2D-LSTM, even fewer supporting GPU implementations (currently cuDNN only implements 1D-LSTM); 2D recurrences reduce the amount of computations that can be parallelized, and thus possibly increase the training/inference time; recurrences create global dependencies with respect to the input, and sometimes this may not be desirable.   Many recent competitions were won by systems that employed networks that use 2D-LSTM layers. Most previous work that compared 1D or pure feed-forward architectures to 2D recurrent models have done so on simple datasets or did not fully optimize the ""baseline"" 2D model compared to the challenger model, which was dully optimized.   In this work, we aim at a fair comparison between 2D and competing models and also extensively evaluate them on more complex datasets that are more representative of challenging ""real-world"" data, compared to ""academic"" datasets that are more restricted in their complexity. We aim at determining when and why the 1D and 2D recurrent models have different results. We also compare the results with a language model to assess if linguistic constraints do level the performance of the different networks.   Our results show that for challenging datasets, 2D-LSTM networks still seem to provide the highest performances and we propose a visualization strategy to explain it.",http://aclweb.org/anthology/D17-1076,D17-1,D17-1076,https://arxiv.org/abs/1811.10899,"('Tobias Horsmann', 'Torsten Zesch')",9,Do LSTMs really work so well for PoS tagging? – A replication study,EMNLP,2017
"A semi-automatic open-source tool for layout analysis on early printed books is presented. LAREX uses a rule based connected components approach which is very fast, easily comprehensible for the user and allows an intuitive manual correction if necessary. The PageXML format is used to support integration into existing OCR workflows. Evaluations showed that LAREX provides an efficient and flexible way to segment pages of early printed books.",http://aclweb.org/anthology/D17-1077,D17-1,D17-1077,https://arxiv.org/abs/1701.07396,"('Lara McConnaughey', 'Jennifer Dai', 'David Bamman')",9,The Labeled Segmentation of Printed Books,EMNLP,2017
"Neural morphological tagging has been regarded as an extension to POS tagging task, treating each morphological tag as a monolithic label and ignoring its internal structure. We propose to view morphological tags as composite labels and explicitly model their internal structure in a neural sequence tagger. For this, we explore three different neural architectures and compare their performance with both CRF and simple neural multiclass baselines. We evaluate our models on 49 languages and show that the neural architecture that models the morphological labels as sequences of morphological category values performs significantly better than both baselines establishing state-of-the-art results in morphological tagging for most languages.",http://aclweb.org/anthology/D17-1078,D17-1,D17-1078,https://arxiv.org/abs/1810.08815,"('Ryan Cotterell', 'Georg Heigold')",9,Cross-lingual Character-Level Neural Morphological Tagging,EMNLP,2017
"Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation. However, both training and working procedures of the current neural models are computationally inefficient. This paper presents a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.",http://aclweb.org/anthology/D17-1079,D17-1,D17-1079,https://arxiv.org/abs/1704.07047,"('Hao Zhou', 'Zhenting Yu', 'Yue Zhang', 'Shujian Huang', 'XIN-YU DAI', 'Jiajun Chen')",9,Word-Context Character Embeddings for Chinese Word Segmentation,EMNLP,2017
"Applying conventional word embedding models to unsegmented languages, where word boundary is not clear, requires word segmentation as preprocessing. However, word segmentation is difficult and expensive to conduct without errors. Segmentation error degrades the quality of word embeddings, leading to performance degradation in downstream applications. In this paper, we propose a simple segmentation-free method to obtain unsupervised vector representations for words, phrases and sentences from an unsegmented raw corpus. Our model is based on subword information skip-gram model, but embedding targets and contexts are character $n$-grams instead of segmented words. We consider all possible character $n$-grams in a corpus as targets, and every target is modeled as the sum of its compositional sub-$n$-grams. Our method completely ignores word boundaries in a corpus and is not word-segmentation dependent. This approach may sound reckless, but it was found to work well through experiments on real-world datasets and benchmarks.",http://aclweb.org/anthology/D17-1080,D17-1,D17-1080,https://arxiv.org/abs/1809.00918,['Takamasa Oshikiri'],9,Segmentation-Free Word Embedding for Unsegmented Languages,EMNLP,2017
,http://aclweb.org/anthology/D17-1081,D17-1,D17-1081,,"('Mrinmaya Sachan', 'Kumar Dubey', 'Eric Xing')",9,"
      From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic
      Knowledge from Textbooks to Solve Geometry Problems
    ",EMNLP,2017
"We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, instead, we endeavor to generate longer and semantic-rich distractors which are closer to distractors in real reading comprehension from examinations. Taking a reading comprehension article, a pair of question and its correct option as input, our goal is to generate several distractors which are somehow related to the answer, consistent with the semantic context of the question and have some trace in the article. We propose a hierarchical encoder-decoder framework with static and dynamic attention mechanisms to tackle this task. Specifically, the dynamic attention can combine sentence-level and word-level attention varying at each recurrent time step to generate a more readable sequence. The static attention is to modulate the dynamic attention not to focus on question irrelevant sentences or sentences which contribute to the correct option. Our proposed framework outperforms several strong baselines on the first prepared distractor generation dataset of real reading comprehension questions. For human evaluation, compared with those distractors generated by baselines, our generated distractors are more functional to confuse the annotators.",http://aclweb.org/anthology/D17-1082,D17-1,D17-1082,https://arxiv.org/abs/1809.02768,"('Guokun Lai', 'Qizhe Xie', 'Hanxiao Liu', 'Yiming Yang', 'Eduard Hovy')",9,RACE: Large-scale ReAding Comprehension Dataset From Examinations,EMNLP,2017
,http://aclweb.org/anthology/D17-1083,D17-1,D17-1083,,"('Mark Hopkins', 'Cristian Petrescu-Prahova', 'Roie Levin', 'Ronan Le Bras', 'Alvaro Herrasti', 'Vidur Joshi')",9,"
      Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade
      of Tree Transducers
    ",EMNLP,2017
"Math word problems form a natural abstraction to a range of quantitative reasoning problems, such as understanding financial news, sports results, and casualties of war. Solving such problems requires the understanding of several mathematical concepts such as dimensional analysis, subset relationships, etc. In this paper, we develop declarative rules which govern the translation of natural language description of these concepts to math expressions. We then present a framework for incorporating such declarative knowledge into word problem solving. Our method learns to map arithmetic word problem text to math expressions, by learning to select the relevant declarative knowledge for each operation of the solution expression. This provides a way to handle multiple concepts in the same problem while, at the same time, support interpretability of the answer expression. Our method models the mapping to declarative knowledge as a latent variable, thus removing the need for expensive annotations. Experimental evaluation suggests that our domain knowledge based solver outperforms all other systems, and that it generalizes better in the realistic case where the training data it is exposed to is biased in a different way than the test data.",http://aclweb.org/anthology/D17-1084,D17-1,D17-1084,https://arxiv.org/abs/1712.09391,"('Danqing Huang', 'Shuming Shi', 'Chin-Yew Lin', 'Jian Yin')",9,Learning Fine-Grained Expressions to Solve Math Word Problems,EMNLP,2017
"Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.",http://aclweb.org/anthology/D17-1085,D17-1,D17-1085,https://arxiv.org/abs/1703.00572,"('Rui Liu', 'Junjie Hu', 'Wei Wei', 'Zi Yang', 'Eric Nyberg')",9,Structural Embedding of Syntactic Trees for Machine Comprehension,EMNLP,2017
,http://aclweb.org/anthology/D17-1086,D17-1,D17-1086,,"('Teng Long', 'Emmanuel Bengio', 'Ryan Lowe', 'Jackie Chi Kit Cheung', 'Doina Precup')",9,"
      World Knowledge for Reading Comprehension: Rare Entity Prediction with
      Hierarchical LSTMs Using External Descriptions
    ",EMNLP,2017
"We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed SynNet with a pretrained model from the SQuAD dataset on the challenging NewsQA dataset, we achieve an F1 measure of 44.3% with a single model and 46.6% with an ensemble, approaching performance of in-domain models (F1 measure of 50.0%) and outperforming the out-of-domain baseline of 7.6%, without use of provided annotations.",http://aclweb.org/anthology/D17-1087,D17-1,D17-1087,https://arxiv.org/abs/1706.09789,"('David Golub', 'Po-Sen Huang', 'Xiaodong He', 'Li Deng')",9,Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension,EMNLP,2017
"Solving math word problems is a challenging task that requires accurate natural language understanding to bridge natural language texts and math expressions. Motivated by the intuition about how human generates the equations given the problem texts, this paper presents a neural approach to automatically solve math word problems by operating symbols according to their semantic meanings in texts. This paper views the process of generating equation as a bridge between the semantic world and the symbolic world, where the proposed neural math solver is based on an encoder-decoder framework. In the proposed model, the encoder is designed to understand the semantics of problems, and the decoder focuses on tracking semantic meanings of the generated symbols and then deciding which symbol to generate next. The preliminary experiments are conducted in a dataset Math23K, and our model significantly outperforms both the state-of-the-art single model and the best non-retrieval-based model over about 10% accuracy, demonstrating the effectiveness of bridging the symbolic and semantic worlds from math word problems.",http://aclweb.org/anthology/D17-1088,D17-1,D17-1088,https://arxiv.org/abs/1811.00720,"('Yan Wang', 'Xiaojiang Liu', 'Shuming Shi')",9,Deep Neural Solver for Math Word Problems,EMNLP,2017
"Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they represent the meaning of word occurrences. While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such ""two-step"" methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner. STE naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the STE model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way.",http://aclweb.org/anthology/D17-1089,D17-1,D17-1089,https://arxiv.org/abs/1706.07276,"('Deepak P', 'Dinesh Garg', 'Shirish Shevade')",9,Latent Space Embedding for Retrieval in Question-Answer Archives,EMNLP,2017
"Answering open-ended questions is an essential capability for any intelligent agent. One of the most interesting recent open-ended question answering challenges is Visual Question Answering (VQA) which attempts to evaluate a system's visual understanding through its answers to natural language questions about images. There exist many approaches to VQA, the majority of which do not exhibit deeper semantic understanding of the candidate answers they produce. We study the importance of generating plausible answers to a given question by introducing the novel task of `Answer Proposal': for a given open-ended question, a system should generate a ranked list of candidate answers informed by the semantics of the question. We experiment with various models including a neural generative model as well as a semantic graph matching one. We provide both intrinsic and extrinsic evaluations for the task of Answer Proposal, showing that our best model learns to propose plausible answers with a high recall and performs competitively with some other solutions to VQA.",http://aclweb.org/anthology/D17-1090,D17-1,D17-1090,https://arxiv.org/abs/1610.06620,"('Nan Duan', 'Duyu Tang', 'Peng Chen', 'Ming Zhou')",9,Question Generation for Question Answering,EMNLP,2017
"Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-to-end using question-answer pairs as a supervision signal. A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on QA over Freebase and answer sentence selection. Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.",http://aclweb.org/anthology/D17-1091,D17-1,D17-1091,https://arxiv.org/abs/1708.06022,"('Li Dong', 'Jonathan Mallinson', 'Siva Reddy', 'Mirella Lapata')",9,Learning to Paraphrase for Question Answering,EMNLP,2017
,http://aclweb.org/anthology/D17-1092,D17-1,D17-1092,,"('Yuanliang Meng', 'Anna Rumshisky', 'Alexey Romanov')",9,"
      Temporal Information Extraction for Question Answering Using Syntactic
      Dependencies in an LSTM-based Architecture
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1093,D17-1,D17-1093,,"('Kateryna Tymoshenko', 'Daniele Bonadiman', 'Alessandro Moschitti')",9,"
      Ranking Kernels for Structures and Embeddings: A Hybrid Preference and
      Classification Model
    ",EMNLP,2017
"We consider answering queries where the underlying data is available only over limited interfaces which provide lookup access to the tuples matching a given binding, but possibly restricting the number of output tuples returned. Interfaces imposing such ""result bounds"" are common in accessing data via the web. Given a query over a set of relations as well as some integrity constraints that relate the queried relations to the data sources, we examine the problem of deciding if the query is answerable over the interfaces; that is, whether there exists a plan that returns all answers to the query, assuming the source data satisfies the integrity constraints.   The first component of our analysis of answerability is a reduction to a query containment problem with constraints. The second component is a set of ""schema simplification"" theorems capturing limitations on how interfaces with result bounds can be useful to obtain complete answers to queries. These results also help to show decidability for the containment problem that captures answerability, for many classes of constraints. The final component in our analysis of answerability is a ""linearization"" method, showing that query containment with certain guarded dependencies -- including those that emerge from answerability problems -- can be reduced to query containment for a well-behaved class of linear dependencies. Putting these components together, we get a detailed picture of how to check answerability over result-bounded services.",http://aclweb.org/anthology/D17-1094,D17-1,D17-1094,https://arxiv.org/abs/1706.07936,"('Semih Yavuz', 'Izzeddin Gur', 'Yu Su', 'Xifeng Yan')",9,Recovering Question Answering Errors via Query Revision,EMNLP,2017
,http://aclweb.org/anthology/D17-1095,D17-1,D17-1095,,"('Jean-Benoit Delbrouck', 'Stéphane Dupont')",9,"
      An empirical study on the effectiveness of images in Multimodal Neural
      Machine Translation
    ",EMNLP,2017
"To be able to interact better with humans, it is crucial for machines to understand sound - a primary modality of human perception. Previous works have used sound to learn embeddings for improved generic textual similarity assessment. In this work, we treat sound as a first-class citizen, studying downstream textual tasks which require aural grounding. To this end, we propose sound-word2vec - a new embedding scheme that learns specialized word embeddings grounded in sounds. For example, we learn that two seemingly (semantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make. Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering foley sound effects (used in movies). Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex.",http://aclweb.org/anthology/D17-1096,D17-1,D17-1096,https://arxiv.org/abs/1703.01720,"('Ashwin Vijayakumar', 'Ramakrishna Vedantam', 'Devi Parikh')",9,Sound-Word2Vec: Learning Word Representations Grounded in Sounds,EMNLP,2017
,http://aclweb.org/anthology/D17-1097,D17-1,D17-1097,,"('Aroma Mahendru', 'Viraj Prabhu', 'Akrit Mohapatra', 'Dhruv Batra', 'Stefan Lee')",9,"
      The Promise of Premise: Harnessing Question Premises in Visual Question
      Answering
    ",EMNLP,2017
"Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.",http://aclweb.org/anthology/D17-1098,D17-1,D17-1098,https://arxiv.org/abs/1612.00576,"('Peter Anderson', 'Basura Fernando', 'Mark Johnson', 'Stephen Gould')",9,Guided Open Vocabulary Image Captioning with Constrained Beam Search,EMNLP,2017
"In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb ""salute"" has several properties, such as being a light movement, a social act, and short in duration. We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action. In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes inferred from language can provide a predictive signal for zero-shot prediction of previously unseen activities.",http://aclweb.org/anthology/D17-1099,D17-1,D17-1099,https://arxiv.org/abs/1707.09468,"('Rowan Zellers', 'Yejin Choi')",9,Zero-Shot Activity Recognition with Verb Attribute Induction,EMNLP,2017
,http://aclweb.org/anthology/D17-1100,D17-1,D17-1100,,"('Sina Zarrieß', 'David Schlangen')",9,"
      Deriving continous grounded meaning representations from referentially
      structured multimodal contexts
    ",EMNLP,2017
"We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.",http://aclweb.org/anthology/D17-1101,D17-1,D17-1101,https://arxiv.org/abs/1708.02977,"('Licheng Yu', 'Mohit Bansal', 'Tamara Berg')",9,Hierarchically-Attentive RNN for Album Summarization and Storytelling,EMNLP,2017
"Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both English and traditional Chinese. We present a novel dataset based on League of Legends championships recorded from North American and Taiwanese Twitch.tv channels (will be released for further research), and demonstrate strong results on these using multimodal, character-level CNN-RNN model architectures.",http://aclweb.org/anthology/D17-1102,D17-1,D17-1102,https://arxiv.org/abs/1707.08559,"('Cheng-Yang Fu', 'Joon Lee', 'Mohit Bansal', 'Alexander Berg')",9,Video Highlight Prediction Using Audience Chat Reactions,EMNLP,2017
"Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.",http://aclweb.org/anthology/D17-1103,D17-1,D17-1103,https://arxiv.org/abs/1708.02300,"('Ramakanth Pasunuru', 'Mohit Bansal')",9,Reinforced Video Captioning with Entailment Rewards,EMNLP,2017
"Using a corpus of over 17,000 financial news reports (involving over 10M words), we perform an analysis of the argument-distributions of the UP- and DOWN-verbs used to describe movements of indices, stocks, and shares. Using measures of the overlap in the argument distributions of these verbs and k-means clustering of their distributions, we advance evidence for the proposal that the metaphors referred to by these verbs are organised into hierarchical structures of superordinate and subordinate groups.",http://aclweb.org/anthology/D17-1104,D17-1,D17-1104,https://arxiv.org/abs/1212.3138,"('Jesse Mu', 'Joshua K. Hartshorne', ""Timothy O'Donnell"")",9,Evaluating Hierarchies of Verb Argument Structure with Hierarchical Clustering,EMNLP,2017
,http://aclweb.org/anthology/D17-1105,D17-1,D17-1105,,"('Iacer Calixto', 'Qun Liu')",9,"
      Incorporating Global Visual Features into Attention-based Neural Machine
      Translation.
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1106,D17-1,D17-1106,,"('Dipendra Misra', 'John Langford', 'Yoav Artzi')",9,"
      Mapping Instructions and Visual Observations to Actions with Reinforcement
      Learning
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1107,D17-1,D17-1107,,"('Kathleen C. Fraser', 'Kristina Lundholm Fors', 'Dimitrios Kokkinakis', 'Arto Nordlund')",9,"
      An analysis of eye-movements during reading for the detection of mild
      cognitive impairment
    ",EMNLP,2017
"Automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as Question Answering, Information Extraction, and Summarization. Since most existing methods are supervised and require large corpora, which for many languages do not exist, we have concentrated our efforts to reduce the need for annotated data as much as possible. This paper presents two different algorithms towards this goal. The first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events. In the first stage, the algorithm learns a general classifier from an annotated corpus. Then, inspired by the hypothesis of ""one type of temporal relation per discourse, it extracts useful information from a cluster of topically related documents. We show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier can be built to extract temporal relations between events. Our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is higher than that of several previous successful systems. The second proposed method for temporal relation extraction is based on the expectation maximization (EM) algorithm. Within EM, we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal. We think that the experimental results of our EM based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging.",http://aclweb.org/anthology/D17-1108,D17-1,D17-1108,https://arxiv.org/abs/1401.6427,"('Qiang Ning', 'Zhili Feng', 'Dan Roth')",9,A Structured Learning Approach to Temporal Relation Extraction,EMNLP,2017
,http://aclweb.org/anthology/D17-1109,D17-1,D17-1109,,"('Arun Chaganty', 'Ashwin Paranjape', 'Percy Liang', 'Christopher D. Manning')",9,"
      Importance sampling for unbiased on-demand evaluation of knowledge base
      population
    ",EMNLP,2017
"Neural IR models, such as DRMM and PACRR, have achieved strong results by successfully capturing relevance matching signals. We argue that the context of these matching signals is also important. Intuitively, when extracting, modeling, and combining matching signals, one would like to consider the surrounding text (local context) as well as other signals from the same document that can contribute to the overall relevance score. In this work, we highlight three potential shortcomings caused by not considering context information and propose three neural ingredients to address them: a disambiguation component, cascade k-max pooling, and a shuffling combination layer. Incorporating these components into the PACRR model yields Co-PACRR, a novel context-aware neural IR model. Extensive comparisons with established models on Trec Web Track data confirm that the proposed model can achieve superior search results. In addition, an ablation analysis is conducted to gain insights into the impact of and interactions between different components. We release our code to enable future comparisons.",http://aclweb.org/anthology/D17-1110,D17-1,D17-1110,https://arxiv.org/abs/1706.10192,"('Kai Hui', 'Andrew Yates', 'Klaus Berberich', 'Gerard de Melo')",9,PACRR: A Position-Aware Neural IR Model for Relevance Matching,EMNLP,2017
"Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bi-directional attention mechanisms or score all possible answer spans, limiting scalability. We propose instead to cast extractive QA as an iterative search problem: select the answer's sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths. We show that globally normalizing the decision process and back-propagating through beam search makes this representation viable and learning efficient. We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a knowledge base and swapping them with new entities of the same type. This method improves the performance of all models considered in this work and is of independent interest for a variety of NLP tasks.",http://aclweb.org/anthology/D17-1111,D17-1,D17-1111,https://arxiv.org/abs/1709.02828,"('Jonathan Raiman', 'John Miller')",9,Globally Normalized Reader,EMNLP,2017
"Segmental models are an alternative to frame-based models for sequence prediction, where hypothesized path weights are based on entire segment scores rather than a single frame at a time. Neural segmental models are segmental models that use neural network-based weight functions. Neural segmental models have achieved competitive results for speech recognition, and their end-to-end training has been explored in several studies. In this work, we review neural segmental models, which can be viewed as consisting of a neural network-based acoustic encoder and a finite-state transducer decoder. We study end-to-end segmental models with different weight functions, including ones based on frame-level neural classifiers and on segmental recurrent neural networks. We study how reducing the search space size impacts performance under different weight functions. We also compare several loss functions for end-to-end training. Finally, we explore training approaches, including multi-stage vs. end-to-end training and multitask training that combines segmental and frame-level losses.",http://aclweb.org/anthology/D17-1112,D17-1,D17-1112,https://arxiv.org/abs/1708.00531,"('Micha Elsner', 'Cory Shain')",9,Speech segmentation with a neural encoder model of working memory,EMNLP,2017
,http://aclweb.org/anthology/D17-1113,D17-1,D17-1113,,"('Luana Bulat', 'Stephen Clark', 'Ekaterina Shutova')",9,"
      Speaking, Seeing, Understanding: Correlating semantic models with
      conceptual representation in the brain
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1114,D17-1,D17-1114,,"('Haoran Li', 'Junnan Zhu', 'Cong Ma', 'Jiajun Zhang', 'Chengqing Zong')",9,"
      Multi-modal Summarization for Asynchronous Collection of Text, Image,
      Audio and Video
    ",EMNLP,2017
"Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.",http://aclweb.org/anthology/D17-1115,D17-1,D17-1115,https://arxiv.org/abs/1707.07250,"('Amir Zadeh', 'Minghai Chen', 'Soujanya Poria', 'Erik Cambria', 'Louis-Philippe Morency')",9,Tensor Fusion Network for Multimodal Sentiment Analysis,EMNLP,2017
"Manual annotations are a prerequisite for many applications of machine learning. However, weaknesses in the annotation process itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as sentiment analysis, sarcasm, and stance detection, such choices can impact results. Here, for the task of political stance detection on Twitter, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a context may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general model for reasoning about annotations across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a classifier for new instances. We show that the classifier learned by ConStance outperforms a variety of baselines at predicting political stance, while the model's interpretable parameters shed light on the effects of each context.",http://aclweb.org/anthology/D17-1116,D17-1,D17-1116,https://arxiv.org/abs/1708.06309,"('Kenneth Joseph', 'Lisa Friedland', 'William Hobbs', 'David Lazer', 'Oren Tsur')",9,ConStance: Modeling Annotation Contexts to Improve Stance Classification,EMNLP,2017
"Online communities have gained considerable importance in recent years due to the increasing number of people connected to the Internet. Moderating user content in online communities is mainly performed manually, and reducing the workload through automatic methods is of great financial interest for community maintainers. Often, the industry uses basic approaches such as bad words filtering and regular expression matching to assist the moderators. In this article, we consider the task of automatically determining if a message is abusive. This task is complex since messages are written in a non-standardized way, including spelling errors, abbreviations, community-specific codes... First, we evaluate the system that we propose using standard features of online messages. Then, we evaluate the impact of the addition of pre-processing strategies, as well as original specific features developed for the community of an online in-browser strategy game. We finally propose to analyze the usefulness of this wide range of features using feature selection. This work can lead to two possible applications: 1) automatically flag potentially abusive messages to draw the moderator's attention on a narrow subset of messages ; and 2) fully automate the moderation process by deciding whether a message is abusive without any human intervention.",http://aclweb.org/anthology/D17-1117,D17-1,D17-1117,https://arxiv.org/abs/1704.03289,"('John Pavlopoulos', 'Prodromos Malakasiotis', 'Ion Androutsopoulos')",9,Deeper Attention to Abusive User Content Moderation,EMNLP,2017
,http://aclweb.org/anthology/D17-1118,D17-1,D17-1118,,"('Haim Dubossarsky', 'Daphna Weinshall', 'Eitan Grossman')",9,"
      Outta Control: Laws of Semantic Change and Inherent Biases in Word
      Representation Models
    ",EMNLP,2017
"First derived from human intuition, later adapted to machine translation for automatic token alignment, attention mechanism, a simple method that can be used for encoding sequence data based on the importance score each element is assigned, has been widely applied to and attained significant improvement in various tasks in natural language processing, including sentiment classification, text summarization, question answering, dependency parsing, etc. In this paper, we survey through recent works and conduct an introductory summary of the attention mechanism in different NLP problems, aiming to provide our readers with basic knowledge on this widely used method, discuss its different variants for different tasks, explore its association with other techniques in machine learning, and examine methods for evaluating its performance.",http://aclweb.org/anthology/D17-1119,D17-1,D17-1119,https://arxiv.org/abs/1811.05544,"('Veronica Lynn', 'Youngseo Son', 'Vivek Kulkarni', 'Niranjan Balasubramanian', 'H. Andrew Schwartz')",9,Human Centered NLP with User-Factor Adaptation,EMNLP,2017
"Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or ""senses"". Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8% average error reduction in unlabeled attachment scores across 6 languages.",http://aclweb.org/anthology/D17-1120,D17-1,D17-1120,https://arxiv.org/abs/1511.06388,"('Alessandro Raganato', 'Claudio Delli Bovi', 'Roberto Navigli')",9,Neural Sequence Learning Models for Word Sense Disambiguation,EMNLP,2017
"We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, although ""eats"" and ""stares at"" seem unrelated in text, they share semantics visually. When people are eating something, they also tend to stare at the food. Grounding diverse relations like ""eats"" and ""stares at"" into vision remains challenging, despite recent progress in vision. We note that the visual grounding of words depends on semantics, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained, visually grounded notions of semantic relatedness. We show improvements over text-only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets are available online.",http://aclweb.org/anthology/D17-1121,D17-1,D17-1121,https://arxiv.org/abs/1511.07067,"('Guy D. Rosin', 'Eytan Adar', 'Kira Radinsky')",9,Learning Word Relatedness over Time,EMNLP,2017
"We present a simple neural network for word alignment that builds source and target word window representations to compute alignment scores for sentence pairs. To enable unsupervised training, we use an aggregation operation that summarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on English-Czech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment.",http://aclweb.org/anthology/D17-1122,D17-1,D17-1122,https://arxiv.org/abs/1606.09560,"('Gehui Shen', 'Yunlun Yang', 'Zhi-Hong Deng')",9,Inter-Weighted Alignment Network for Sentence Pair Modeling,EMNLP,2017
,http://aclweb.org/anthology/D17-1123,D17-1,D17-1123,,"('Chengyu Wang', 'Xiaofeng He', 'Aoying Zhou')",9,"
      A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources
      and Recent Advances
    ",EMNLP,2017
"Different semantic interpretation tasks such as text entailment and question answering require the classification of semantic relations between terms or entities within text. However, in most cases it is not possible to assign a direct semantic relation between entities/terms. This paper proposes an approach for composite semantic relation classification, extending the traditional semantic relation classification task. Different from existing approaches, which use machine learning models built over lexical and distributional word vector features, the proposed model uses the combination of a large commonsense knowledge base of binary relations, a distributional navigational algorithm and sequence classification to provide a solution for the composite semantic relation classification problem.",http://aclweb.org/anthology/D17-1124,D17-1,D17-1124,https://arxiv.org/abs/1805.06521,"('Pengfei Liu', 'Kaiyu Qian', 'Xipeng Qiu', 'Xuanjing Huang')",9,Idiom-Aware Compositional Distributed Semantics,EMNLP,2017
"To learn a semantic parser from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing model to improve the state-of-the-art accuracy from 38.7% to 42.7%, and then use macro grammars and holistic triggering to achieve an 11x speedup and an accuracy of 43.7%.",http://aclweb.org/anthology/D17-1125,D17-1,D17-1125,https://arxiv.org/abs/1707.07806,"('Yuchen Zhang', 'Panupong Pasupat', 'Percy Liang')",9,Macro Grammars and Holistic Triggering for Efficient Semantic Parsing,EMNLP,2017
"A major challenge in paraphrase research is the lack of parallel corpora. In this paper, we present a new method to collect large-scale sentential paraphrases from Twitter by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the classifier or human in the loop needed to select data before annotation and subsequent application of paraphrase identification algorithms in the previous work. We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification. In addition, we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at ~70% precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction. We make our code and data freely available.",http://aclweb.org/anthology/D17-1126,D17-1,D17-1126,https://arxiv.org/abs/1708.00391,"('Wuwei Lan', 'Siyu Qiu', 'Hua He', 'Wei Xu')",9,A Continuously Growing Dataset of Sentential Paraphrases,EMNLP,2017
"Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pre-trained word embeddings that hinder their direct use in neural networks, and propose standardization techniques as a remedy. On the popular Overnight dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embeddings can bring significant improvement.",http://aclweb.org/anthology/D17-1127,D17-1,D17-1127,https://arxiv.org/abs/1704.05974,"('Yu Su', 'Xifeng Yan')",9,Cross-domain Semantic Parsing via Paraphrasing,EMNLP,2017
"We propose a framework for parsing video and text jointly for understanding events and answering user queries. Our framework produces a parse graph that represents the compositional structures of spatial information (objects and scenes), temporal information (actions and events) and causal information (causalities between events and fluents) in the video and text. The knowledge representation of our framework is based on a spatial-temporal-causal And-Or graph (S/T/C-AOG), which jointly models possible hierarchical compositions of objects, scenes and events as well as their interactions and mutual contexts, and specifies the prior probabilistic distribution of the parse graphs. We present a probabilistic generative model for joint parsing that captures the relations between the input video/text, their corresponding parse graphs and the joint parse graph. Based on the probabilistic model, we propose a joint parsing system consisting of three modules: video parsing, text parsing and joint inference. Video parsing and text parsing produce two parse graphs from the input video and text respectively. The joint inference module produces a joint parse graph by performing matching, deduction and revision on the video and text parse graphs. The proposed framework has the following objectives: Firstly, we aim at deep semantic parsing of video and text that goes beyond the traditional bag-of-words approaches; Secondly, we perform parsing and reasoning across the spatial, temporal and causal dimensions based on the joint S/T/C-AOG representation; Thirdly, we show that deep joint parsing facilitates subsequent applications such as generating narrative text descriptions and answering queries in the forms of who, what, when, where and why. We empirically evaluated our system based on comparison against ground-truth as well as accuracy of query answering and obtained satisfactory results.",http://aclweb.org/anthology/D17-1128,D17-1,D17-1128,https://arxiv.org/abs/1308.6628,"('Bishan Yang', 'Tom Mitchell')",9,A Joint Sequential and Relational Model for Frame-Semantic Parsing,EMNLP,2017
"The ability to consolidate information of different types is at the core of intelligence, and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others. In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings.",http://aclweb.org/anthology/D17-1129,D17-1,D17-1129,https://arxiv.org/abs/1805.00287,"('Chuan Wang', 'Nianwen Xue')",9,Getting the Most out of AMR Parsing,EMNLP,2017
"We present a transition-based AMR parser that directly generates AMR parses from plain text. We use Stack-LSTMs to represent our parser state and make decisions greedily. In our experiments, we show that our parser achieves very competitive scores on English using only AMR training data. Adding additional information, such as POS tags and dependency trees, improves the results further.",http://aclweb.org/anthology/D17-1130,D17-1,D17-1130,https://arxiv.org/abs/1707.07755,"('Miguel Ballesteros', 'Yaser Al-Onaizan')",9,AMR Parsing using Stack-LSTMs,EMNLP,2017
,http://aclweb.org/anthology/D17-1131,D17-1,D17-1131,,"('Jie Zhao', 'Yu Su', 'Ziyu Guan', 'Huan Sun')",9,"
      An End-to-End Deep Framework for Answer Triggering with a Novel
      Group-Level Objective
    ",EMNLP,2017
"We evaluate 8 different word embedding models on their usefulness for predicting the neural activation patterns associated with concrete nouns. The models we consider include an experiential model, based on crowd-sourced association data, several popular neural and distributional models, and a model that reflects the syntactic context of words (based on dependency parses). Our goal is to assess the cognitive plausibility of these various embedding models, and understand how we can further improve our methods for interpreting brain imaging data.   We show that neural word embedding models exhibit superior performance on the tasks we consider, beating experiential word representation model. The syntactically informed model gives the overall best performance when predicting brain activation patterns from word embeddings; whereas the GloVe distributional method gives the overall best performance when predicting in the reverse direction (words vectors from brain images). Interestingly, however, the error patterns of these different models are markedly different. This may support the idea that the brain uses different systems for processing different kinds of words. Moreover, we suggest that taking the relative strengths of different embedding models into account will lead to better models of the brain activity associated with words.",http://aclweb.org/anthology/D17-1132,D17-1,D17-1132,https://arxiv.org/abs/1711.09285,"('Andrew Cattle', 'Xiaojuan Ma')",9,Predicting Word Association Strengths,EMNLP,2017
,http://aclweb.org/anthology/D17-1133,D17-1,D17-1133,,"('Yang Liu', 'Mirella Lapata')",9,"
      Learning Contextually Informed Representations for Linear-Time Discourse
      Parsing
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1134,D17-1,D17-1134,,"('Man Lan', 'Jianxiang Wang', 'Yuanbin Wu', 'Zheng-Yu Niu', 'Haifeng Wang')",9,"
      Multi-task Attention-based Neural Networks for Implicit Discourse
      Relationship Representation and Identification
    ",EMNLP,2017
"Existing approaches for Chinese zero pronoun resolution overlook semantic information. This is because zero pronouns have no descriptive information, which results in difficulty in explicitly capturing their semantic similarities with antecedents. Moreover, when dealing with candidate antecedents, traditional systems simply take advantage of the local information of a single candidate antecedent while failing to consider the underlying information provided by the other candidates from a global perspective. To address these weaknesses, we propose a novel zero pronoun-specific neural network, which is capable of representing zero pronouns by utilizing the contextual information at the semantic level. In addition, when dealing with candidate antecedents, a two-level candidate encoder is employed to explicitly capture both the local and global information of candidate antecedents. We conduct experiments on the Chinese portion of the OntoNotes 5.0 corpus. Experimental results show that our approach substantially outperforms the state-of-the-art method in various experimental settings.",http://aclweb.org/anthology/D17-1135,D17-1,D17-1135,https://arxiv.org/abs/1604.05800,"('Qingyu Yin', 'Yu Zhang', 'Weinan Zhang', 'Ting Liu')",9,Chinese Zero Pronoun Resolution with Deep Memory Network,EMNLP,2017
,http://aclweb.org/anthology/D17-1136,D17-1,D17-1136,,"('Mathieu Morey', 'Philippe Muller', 'Nicholas Asher')",9,"
      How much progress have we made on RST discourse parsing? A replication
      study of recent results on the RST-DT
    ",EMNLP,2017
"We present a tableau calculus for reasoning in fragments of natural language. We focus on the problem of pronoun resolution and the way in which it complicates automated theorem proving for natural language processing. A method for explicitly manipulating contextual information during deduction is proposed, where pronouns are resolved against this context during deduction. As a result, pronoun resolution and deduction can be interleaved in such a way that pronouns are only resolved if this is licensed by a deduction rule; this helps us to avoid the combinatorial complexity of total pronoun disambiguation.",http://aclweb.org/anthology/D17-1137,D17-1,D17-1137,https://arxiv.org/abs/cs/0009017,"('Sharid Loáiciga', 'Liane Guillou', 'Christian Hardmeier')",9,What is it? Disambiguating the different readings of the pronoun ‘it’,EMNLP,2017
"Selectional preferences have long been claimed to be essential for coreference resolution. However, they are mainly modeled only implicitly by current coreference resolvers. We propose a dependency-based embedding model of selectional preferences which allows fine-grained compatibility judgments with high coverage. We show that the incorporation of our model improves coreference resolution performance on the CoNLL dataset, matching the state-of-the-art results of a more complex system. However, it comes with a cost that makes it debatable how worthwhile such improvements are.",http://aclweb.org/anthology/D17-1138,D17-1,D17-1138,https://arxiv.org/abs/1707.06456,"('Benjamin Heinzerling', 'Nafise Sadat Moosavi', 'Michael Strube')",9,Revisiting Selectional Preferences for Coreference Resolution,EMNLP,2017
"Topic models are evaluated based on their ability to describe documents well (i.e. low perplexity) and to produce topics that carry coherent semantic meaning. In topic modeling so far, perplexity is a direct optimization target. However, topic coherence, owing to its challenging computation, is not optimized for and is only evaluated after training. In this work, under a neural variational inference framework, we propose methods to incorporate a topic coherence objective into the training process. We demonstrate that such a coherence-aware topic model exhibits a similar level of perplexity as baseline models but achieves substantially higher topic coherence.",http://aclweb.org/anthology/D17-1139,D17-1,D17-1139,https://arxiv.org/abs/1809.02687,"('Liang Wang', 'Sujian Li', 'Yajuan Lv', 'Houfeng WANG')",9,Learning to Rank Semantic Coherence for Topic Segmentation,EMNLP,2017
"In the medical domain, the continuous stream of scientific research contains contradictory results supported by arguments and counter-arguments. As medical expertise occurs at different levels, part of the human agents have difficulties to face the huge amount of studies, but also to understand the reasons and pieces of evidences claimed by the proponents and the opponents of the debated topic. To better understand the supporting arguments for new findings related to current state of the art in the medical domain we need tools able to identify arguments in scientific papers. Our work here aims to fill the above technological gap.   Quite aware of the difficulty of this task, we embark to this road by relying on the well-known interleaving of domain knowledge with natural language processing. To formalise the existing medical knowledge, we rely on ontologies. To structure the argumentation model we use also the expressivity and reasoning capabilities of Description Logics. To perform argumentation mining we formalise various linguistic patterns in a rule-based language. We tested our solution against a corpus of scientific papers related to breast cancer. The run experiments show a F-measure between 0.71 and 0.86 for identifying conclusions of an argument and between 0.65 and 0.86 for identifying premises of an argument.",http://aclweb.org/anthology/D17-1140,D17-1,D17-1140,https://arxiv.org/abs/1607.08074,"('Eyal Shnarch', 'Ran Levy', 'Vikas Raykar', 'Noam Slonim')",9,GRASP: Rich Patterns for Argumentation Mining,EMNLP,2017
"When people converse about social or political topics, similar arguments are often paraphrased by different speakers, across many different conversations. Debate websites produce curated summaries of arguments on such topics; these summaries typically consist of lists of sentences that represent frequently paraphrased propositions, or labels capturing the essence of one particular aspect of an argument, e.g. Morality or Second Amendment. We call these frequently paraphrased propositions ARGUMENT FACETS. Like these curated sites, our goal is to induce and identify argument facets across multiple conversations, and produce summaries. However, we aim to do this automatically. We frame the problem as consisting of two steps: we first extract sentences that express an argument from raw social media dialogs, and then rank the extracted arguments in terms of their similarity to one another. Sets of similar arguments are used to represent argument facets. We show here that we can predict ARGUMENT FACET SIMILARITY with a correlation averaging 0.63 compared to a human topline averaging 0.68 over three debate topics, easily beating several reasonable baselines.",http://aclweb.org/anthology/D17-1141,D17-1,D17-1141,https://arxiv.org/abs/1709.01887,"('Khalid Al Khatib', 'Henning Wachsmuth', 'Matthias Hagen', 'Benno Stein')",9,Patterns of Argumentation Strategies across Topics,EMNLP,2017
"We study the helpful product reviews identification problem in this paper. We observe that the evidence-conclusion discourse relations, also known as arguments, often appear in product reviews, and we hypothesise that some argument-based features, e.g. the percentage of argumentative sentences, the evidences-conclusions ratios, are good indicators of helpful reviews. To validate this hypothesis, we manually annotate arguments in 110 hotel reviews, and investigate the effectiveness of several combinations of argument-based features. Experiments suggest that, when being used together with the argument-based features, the state-of-the-art baseline features can enjoy a performance boost (in terms of F1) of 11.01\% in average.",http://aclweb.org/anthology/D17-1142,D17-1,D17-1142,https://arxiv.org/abs/1707.07279,"('Haijing Liu', 'Yang Gao', 'Pin Lv', 'Mengxue Li', 'Shiqiang Geng', 'Minglan Li', 'Hao Wang')",9,Using Argument-based Features to Predict and Analyse Review Helpfulness,EMNLP,2017
"One of the major goals in automated argumentation mining is to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on the two tasks of extracting links between argument components, and classifying types of argument components. In order to solve this problem, we propose to use a joint model that is based on a Pointer Network architecture. A Pointer Network is appealing for this task for the following reasons: 1) It takes into account the sequential nature of argument components; 2) By construction, it enforces certain properties of the tree structure present in argument relations; 3) The hidden representations can be applied to auxiliary tasks. In order to extend the contribution of the original Pointer Network model, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, achieving far superior performance than a regular Pointer Network model. Our results show that optimizing for both tasks, and adding a fully-connected layer prior to recurrent neural network input, is crucial for high performance.",http://aclweb.org/anthology/D17-1143,D17-1,D17-1143,https://arxiv.org/abs/1612.08994,"('Peter Potash', 'Alexey Romanov', 'Anna Rumshisky')",9,Here's My Point: Joint Pointer Architecture for Argument Mining,EMNLP,2017
"The connections among natural language processing and argumentation theory are becoming stronger in the latest years, with a growing amount of works going in this direction, in different scenarios and applying heterogeneous techniques. In this paper, we present two datasets we built to cope with the combination of the Textual Entailment framework and bipolar abstract argumentation. In our approach, such datasets are used to automatically identify through a Textual Entailment system the relations among the arguments (i.e., attack, support), and then the resulting bipolar argumentation graphs are analyzed to compute the accepted arguments.",http://aclweb.org/anthology/D17-1144,D17-1,D17-1144,https://arxiv.org/abs/1405.0941,"('Oana Cocarascu', 'Francesca Toni')",9,Identifying attack and support argumentative relations using deep learning,EMNLP,2017
"In this work we focus on confidence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct. Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model, and verify or refine its input. Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores.",http://aclweb.org/anthology/D17-1145,D17-1,D17-1145,https://arxiv.org/abs/1805.04604,"('Matthias Sperber', 'Graham Neubig', 'Jan Niehues', 'Alex Waibel')",9,Neural Lattice-to-Sequence Models for Uncertain Inputs,EMNLP,2017
"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.",http://aclweb.org/anthology/D17-1146,D17-1,D17-1146,https://arxiv.org/abs/1706.03872,"('Yang Feng', 'Shiyue Zhang', 'Andi Zhang', 'Dong Wang', 'Andrew Abel')",9,Memory-augmented Neural Machine Translation,EMNLP,2017
"Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce dynamic data selection for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call gradual fine-tuning, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.",http://aclweb.org/anthology/D17-1147,D17-1,D17-1147,https://arxiv.org/abs/1708.00712,"('Marlies van der Wees', 'Arianna Bisazza', 'Christof Monz')",9,Dynamic Data Selection for Neural Machine Translation,EMNLP,2017
"In this paper, we introduce a hybrid search for attention-based neural machine translation (NMT). A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German->English news domain and English->Russian e-commerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3% BLEU absolute as compared to a strong NMT baseline.",http://aclweb.org/anthology/D17-1148,D17-1,D17-1148,https://arxiv.org/abs/1708.03271,"('Leonard Dahlmann', 'Evgeny Matusov', 'Pavel Petrushkov', 'Shahram Khadivi')",9,Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search,EMNLP,2017
"Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese to English translation show that the proposed model achieves significant improvements over the baseline on various test sets.",http://aclweb.org/anthology/D17-1149,D17-1,D17-1149,https://arxiv.org/abs/1708.01980,"('Xing Wang', 'Zhaopeng Tu', 'Deyi Xiong', 'Min Zhang')",9,Translating Phrases in Neural Machine Translation,EMNLP,2017
,http://aclweb.org/anthology/D17-1150,D17-1,D17-1150,,"('Baosong Yang', 'Derek F. Wong', 'Tong Xiao', 'Lidia S. Chao', 'Jingbo Zhu')",9,"
      Towards Bidirectional Hierarchical Representations for Attention-based
      Neural Machine Translation
    ",EMNLP,2017
"Neural machine translation (NMT) has been accelerated by deep learning neural networks over statistical-based approaches, due to the plethora and programmability of commodity heterogeneous computing architectures such as FPGAs and GPUs and the massive amount of training corpuses generated from news outlets, government agencies and social media. Training a learning classifier for neural networks entails tuning hyper-parameters that would yield the best performance. Unfortunately, the number of parameters for machine translation include discrete categories as well as continuous options, which makes for a combinatorial explosive problem. This research explores optimizing hyper-parameters when training deep learning neural networks for machine translation. Specifically, our work investigates training a language model with Marian NMT. Results compare NMT under various hyper-parameter settings across a variety of modern GPU architecture generations in single node and multi-node settings, revealing insights on which hyper-parameters matter most in terms of performance, such as words processed per second, convergence rates, and translation accuracy, and provides insights on how to best achieve high-performing NMT systems.",http://aclweb.org/anthology/D17-1151,D17-1,D17-1151,https://arxiv.org/abs/1805.02094,"('Denny Britz', 'Anna Goldie', 'Minh-Thang Luong', 'Quoc Le')",9,Massive Exploration of Neural Machine Translation Architectures,EMNLP,2017
"We propose a statistical model for natural language that begins by considering language as a monoid, then representing it in complex matrices with a compatible translation invariant probability measure. We interpret the probability measure as arising via the Born rule from a translation invariant matrix product state.",http://aclweb.org/anthology/D17-1152,D17-1,D17-1152,https://arxiv.org/abs/1711.01416,"('Derry Tanti Wijaya', 'Brendan Callahan', 'John Hewitt', 'Jie Gao', 'Xiao Ling', 'Marianna Apidianaki', 'Chris Callison-Burch')",9,Learning Translations via Matrix Completion,EMNLP,2017
,http://aclweb.org/anthology/D17-1153,D17-1,D17-1153,,"('Khanh Nguyen', 'Hal Daumé III', 'Jordan Boyd-Graber')",9,"
      Reinforcement Learning for Bandit Neural Machine Translation with
      Simulated Human Feedback
    ",EMNLP,2017
We propose and compare methods for gradient-based domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture - combining a state-of-the-art self-attentive model with compact domain adaptation - provides high quality personalized machine translation that is both space and time efficient.,http://aclweb.org/anthology/D17-1154,D17-1,D17-1154,https://arxiv.org/abs/1811.01990,"('Xiaowei Zhang', 'Wei Chen', 'Feng Wang', 'Shuang Xu', 'Bo Xu')",9,Towards Compact and Fast Neural Machine Translation Using a Combined Method,EMNLP,2017
"Neural machine translation (NMT) is a deep learning based approach for machine translation, which yields the state-of-the-art translation performance in scenarios where large-scale parallel corpora are available. Although the high-quality and domain-specific translation is crucial in the real world, domain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT performs poorly in such scenarios. Domain adaptation that leverages both out-of-domain parallel corpora as well as monolingual corpora for in-domain translation, is very important for domain-specific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT.",http://aclweb.org/anthology/D17-1155,D17-1,D17-1155,https://arxiv.org/abs/1806.00258,"('Rui Wang', 'Masao Utiyama', 'Lemao Liu', 'Kehai Chen', 'Eiichiro Sumita')",9,Instance Weighting for Neural Machine Translation Domain Adaptation,EMNLP,2017
"We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English->German and English->Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.",http://aclweb.org/anthology/D17-1156,D17-1,D17-1156,https://arxiv.org/abs/1707.09920,"('Antonio Valerio Miceli Barone', 'Barry Haddow', 'Ulrich Germann', 'Rico Sennrich')",9,Regularization techniques for fine-tuning in neural machine translation,EMNLP,2017
,http://aclweb.org/anthology/D17-1157,D17-1,D17-1157,,"('Yin-Wen Chang', 'Michael Collins')",9,"
      Source-Side Left-to-Right or Target-Side Left-to-Right? An Empirical
      Comparison of Two Phrase-Based Decoding Algorithms
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1158,D17-1,D17-1158,,"('Tobias Domhan', 'Felix Hieber')",9,"
      Using Target-side Monolingual Data for Neural Machine Translation through
      Multi-task Learning
    ",EMNLP,2017
"Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.",http://aclweb.org/anthology/D17-1159,D17-1,D17-1159,https://arxiv.org/abs/1703.04826,"('Diego Marcheggiani', 'Ivan Titov')",9,Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling,EMNLP,2017
"The goal of semantic parsing is to map natural language to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of deep learning technologies for semantic parsing is the lack of sufficient annotation training data. In this paper, we propose using sequence-to-sequence in a multi-task setup for semantic parsing with a focus on transfer learning. We explore three multi-task architectures for sequence-to-sequence modeling and compare their performance with an independently trained model. Our experiments show that the multi-task setup aids transfer learning from an auxiliary task with large labeled data to a target task with smaller labeled data. We see absolute accuracy gains ranging from 1.0% to 4.4% in our in- house data set, and we also see good gains ranging from 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.",http://aclweb.org/anthology/D17-1160,D17-1,D17-1160,https://arxiv.org/abs/1706.04326,"('Jayant Krishnamurthy', 'Pradeep Dasigi', 'Matt Gardner')",9,Neural Semantic Parsing with Type Constraints for Semi-Structured Tables,EMNLP,2017
"We present a framework that couples the syntax and semantics of natural language sentences in a generative model, in order to develop a semantic parser that jointly infers the syntactic, morphological, and semantic representations of a given sentence under the guidance of background knowledge. To generate a sentence in our framework, a semantic statement is first sampled from a prior, such as from a set of beliefs in a knowledge base. Given this semantic statement, a grammar probabilistically generates the output sentence. A joint semantic-syntactic parser is derived that returns the $k$-best semantic and syntactic parses for a given sentence. The semantic prior is flexible, and can be used to incorporate background knowledge during parsing, in ways unlike previous semantic parsing approaches. For example, semantic statements corresponding to beliefs in a knowledge base can be given higher prior probability, type-correct statements can be given somewhat lower probability, and beliefs outside the knowledge base can be given lower probability. The construction of our grammar invokes a novel application of hierarchical Dirichlet processes (HDPs), which in turn, requires a novel and efficient inference approach. We present experimental results showing, for a simple grammar, that our parser outperforms a state-of-the-art CCG semantic parser and scales to knowledge bases with millions of beliefs.",http://aclweb.org/anthology/D17-1161,D17-1,D17-1161,https://arxiv.org/abs/1606.06361,"('Shashank Srivastava', 'Igor Labutov', 'Tom Mitchell')",9,Joint Concept Learning and Semantic Parsing from Natural Language Explanations,EMNLP,2017
,http://aclweb.org/anthology/D17-1162,D17-1,D17-1162,,"('Marek Rei', 'Luana Bulat', 'Douwe Kiela', 'Ekaterina Shutova')",9,"
      Grasping the Finer Point: A Supervised Similarity Network for Metaphor
      Detection
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1163,D17-1,D17-1163,,"('Katherine Keith', 'Abram Handler', 'Michael Pinkham', 'Cara Magliozzi', 'Joshua McDuffie', ""Brendan O'Connor"")",9,"
      Identifying civilians killed by police with distantly supervised
      entity-event extraction
    ",EMNLP,2017
"Questions play a prominent role in social interactions, performing rhetorical functions that go beyond that of simple informational exchange. The surface form of a question can signal the intention and background of the person asking it, as well as the nature of their relation with the interlocutor. While the informational nature of questions has been extensively examined in the context of question-answering applications, their rhetorical aspects have been largely understudied.   In this work we introduce an unsupervised methodology for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role. By applying this framework to the setting of question sessions in the UK parliament, we show that the resulting typology encodes key aspects of the political discourse---such as the bifurcation in questioning behavior between government and opposition parties---and reveals new insights into the effects of a legislator's tenure and political career ambitions.",http://aclweb.org/anthology/D17-1164,D17-1,D17-1164,https://arxiv.org/abs/1708.02254,"('Justine Zhang', 'Arthur Spirling', 'Cristian Danescu-Niculescu-Mizil')",9,Asking too much? The rhetorical role of questions in political discourse,EMNLP,2017
"As political polarization in the United States continues to rise, the question of whether polarized individuals can fruitfully cooperate becomes pressing. Although diversity of individual perspectives typically leads to superior team performance on complex tasks, strong political perspectives have been associated with conflict, misinformation and a reluctance to engage with people and perspectives beyond one's echo chamber. It is unclear whether self-selected teams of politically diverse individuals will create higher or lower quality outcomes. In this paper, we explore the effect of team political composition on performance through analysis of millions of edits to Wikipedia's Political, Social Issues, and Science articles. We measure editors' political alignments by their contributions to conservative versus liberal articles. A survey of editors validates that those who primarily edit liberal articles identify more strongly with the Democratic party and those who edit conservative ones with the Republican party. Our analysis then reveals that polarized teams---those consisting of a balanced set of politically diverse editors---create articles of higher quality than politically homogeneous teams. The effect appears most strongly in Wikipedia's Political articles, but is also observed in Social Issues and even Science articles. Analysis of article ""talk pages"" reveals that politically polarized teams engage in longer, more constructive, competitive, and substantively focused but linguistically diverse debates than political moderates. More intense use of Wikipedia policies by politically diverse teams suggests institutional design principles to help unleash the power of politically polarized teams.",http://aclweb.org/anthology/D17-1165,D17-1,D17-1165,https://arxiv.org/abs/1712.06414,"('David Vilares', 'Yulan He')",9,Detecting Perspectives in Political Debates,EMNLP,2017
,http://aclweb.org/anthology/D17-1166,D17-1,D17-1166,,"('Sandesh Swamy', 'Alan Ritter', 'Marie-Catherine de Marneffe')",9,"
      ""i have a feeling trump will win.................."": Forecasting Winners
      and Losers from User Predictions on Twitter
    ",EMNLP,2017
"Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.",http://aclweb.org/anthology/D17-1167,D17-1,D17-1167,https://arxiv.org/abs/1708.05482,"('Lin Gui', 'Jiannan Hu', 'Yulan He', 'Ruifeng Xu', 'Lu Qin', 'Jiachen Du')",9,A Question Answering Approach for Emotion Cause Extraction,EMNLP,2017
"Although there has been substantial research in software analytics for effort estimation in traditional software projects, little work has been done for estimation in agile projects, especially estimating user stories or issues. Story points are the most common unit of measure used for estimating the effort involved in implementing a user story or resolving an issue. In this paper, we offer for the \emph{first} time a comprehensive dataset for story points-based estimation that contains 23,313 issues from 16 open source projects. We also propose a prediction model for estimating story points based on a novel combination of two powerful deep learning architectures: long short-term memory and recurrent highway network. Our prediction system is \emph{end-to-end} trainable from raw input data to prediction outcomes without any manual feature engineering. An empirical evaluation demonstrates that our approach consistently outperforms three common effort estimation baselines and two alternatives in both Mean Absolute Error and the Standardized Accuracy.",http://aclweb.org/anthology/D17-1168,D17-1,D17-1168,https://arxiv.org/abs/1609.00489,"('Snigdha Chaturvedi', 'Haoruo Peng', 'Dan Roth')",9,Story Comprehension for Predicting What Happens Next,EMNLP,2017
,http://aclweb.org/anthology/D17-1169,D17-1,D17-1169,,"('Bjarke Felbo', 'Alan Mislove', 'Anders Søgaard', 'Iyad Rahwan', 'Sune Lehmann')",9,"
      Using millions of emoji occurrences to learn any-domain representations
      for detecting sentiment, emotion and sarcasm
    ",EMNLP,2017
"We present opinion recommendation, a novel task of jointly predicting a custom review with a rating score that a certain user would give to a certain product or service, given existing reviews and rating scores to the product or service by other users, and the reviews that the user has given to other products and services. A characteristic of opinion recommendation is the reliance of multiple data sources for multi-task joint learning, which is the strength of neural models. We use a single neural network to model users and products, capturing their correlation and generating customised product representations using a deep memory network, from which customised ratings and reviews are constructed jointly. Results show that our opinion recommendation system gives ratings that are closer to real user ratings on Yelp.com data compared with Yelp's own ratings, and our methods give better results compared to several pipelines baselines using state-of-the-art sentiment rating and summarization systems.",http://aclweb.org/anthology/D17-1170,D17-1,D17-1170,https://arxiv.org/abs/1702.01517,"('Zhongqing Wang', 'Yue Zhang')",9,Opinion Recommendation Using A Neural Model,EMNLP,2017
"Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning generative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.",http://aclweb.org/anthology/D17-1171,D17-1,D17-1171,https://arxiv.org/abs/1708.01018,"('Jiong Cai', 'Yong Jiang', 'Kewei Tu')",9,CRF Autoencoder for Unsupervised Dependency Parsing,EMNLP,2017
,http://aclweb.org/anthology/D17-1172,D17-1,D17-1172,,"('Caio Corro', 'Joseph Le Roux', 'Mathieu Lacroix')",9,"
      Efficient Discontinuous Phrase-Structure Parsing via the Generalized
      Maximum Spanning Arborescence
    ",EMNLP,2017
"Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focused almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.",http://aclweb.org/anthology/D17-1173,D17-1,D17-1173,https://arxiv.org/abs/1704.07092,['Xiaoqing Zheng'],9,Incremental Graph-based Neural Dependency Parsing,EMNLP,2017
"This paper describes adaptations for EaFi, a parser for easy-first parsing of discontinuous constituents, to adapt it to multiple languages as well as make use of the unlabeled data that was provided as part of the SPMRL shared task 2014.",http://aclweb.org/anthology/D17-1174,D17-1,D17-1174,https://arxiv.org/abs/1409.3813,"('Miloš Stanojević', 'Raquel Garrido Alhama')",9,Neural Discontinuous Constituency Parsing,EMNLP,2017
"The addition of syntax-aware decoding in Neural Machine Translation (NMT) systems requires an effective tree-structured neural network, a syntax-aware attention model and a language generation model that is sensitive to sentence structure. We exploit a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakola (2017) to create an NMT model called Seq2DRNN that combines a sequential encoder with tree-structured decoding augmented with a syntax-aware attention model. Unlike previous approaches to syntax-based NMT which use dependency parsing models our method uses constituency parsing which we argue provides useful information for translation. In addition, we use the syntactic structure of the sentence to add new connections to the tree-structured decoder neural network (Seq2DRNN+SynC). We compare our NMT model with sequential and state of the art syntax-based NMT models and show that our model produces more fluent translations with better reordering. Since our model is capable of doing translation and constituency parsing at the same time we also compare our parsing accuracy against other neural parsing models.",http://aclweb.org/anthology/D17-1175,D17-1,D17-1175,https://arxiv.org/abs/1809.01854,"('Zhirui Zhang', 'Shujie Liu', 'Mu Li', 'Ming Zhou', 'Enhong Chen')",9,Stack-based Multi-layer Attention for Transition-based Dependency Parsing,EMNLP,2017
"We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence. We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.",http://aclweb.org/anthology/D17-1176,D17-1,D17-1176,https://arxiv.org/abs/1708.00801,"('Wenjuan Han', 'Yong Jiang', 'Kewei Tu')",9,Dependency Grammar Induction with Neural Lexicalization and Big Training Data,EMNLP,2017
,http://aclweb.org/anthology/D17-1177,D17-1,D17-1177,,"('Yong Jiang', 'Wenjuan Han', 'Kewei Tu')",9,"
      Combining Generative and Discriminative Approaches to Unsupervised
      Dependency Parsing via Dual Decomposition
    ",EMNLP,2017
"We describe SLING, a framework for parsing natural language into semantic frames. SLING supports general transition-based, neural-network parsing with bidirectional LSTM input encoding and a Transition Based Recurrent Unit (TBRU) for output decoding. The parsing model is trained end-to-end using only the text tokens as input. The transition system has been designed to output frame graphs directly without any intervening symbolic representation. The SLING framework includes an efficient and scalable frame store implementation as well as a neural network JIT compiler for fast inference during parsing. SLING is implemented in C++ and it is available for download on GitHub.",http://aclweb.org/anthology/D17-1178,D17-1,D17-1178,https://arxiv.org/abs/1710.07032,"('Mitchell Stern', 'Daniel Fried', 'Dan Klein')",9,Effective Inference for Generative Neural Parsing,EMNLP,2017
"Conditional Random Field (CRF) and recurrent neural models have achieved success in structured prediction. More recently, there is a marriage of CRF and recurrent neural models, so that we can gain from both non-linear dense features and globally normalized CRF objective. These recurrent neural CRF models mainly focus on encode node features in CRF undirected graphs. However, edge features prove important to CRF in structured prediction. In this work, we introduce a new recurrent neural CRF model, which learns non-linear edge features, and thus makes non-linear features encoded completely. We compare our model with different neural models in well-known structured prediction tasks. Experiments show that our model outperforms state-of-the-art methods in NP chunking, shallow parsing, Chinese word segmentation and POS tagging.",http://aclweb.org/anthology/D17-1179,D17-1,D17-1179,https://arxiv.org/abs/1611.04233,"('Xiao Zhang', 'Yong Jiang', 'Hao Peng', 'Kewei Tu', 'Dan Goldwasser')",9,Semi-supervised Structured Prediction with Neural CRF Autoencoder,EMNLP,2017
"We present a graph-based Tree Adjoining Grammar (TAG) parser that uses BiLSTMs, highway connections, and character-level CNNs. Our best end-to-end parser, which jointly performs supertagging, POS tagging, and parsing, outperforms the previously reported best results by more than 2.2 LAS and UAS points. The graph-based parsing architecture allows for global inference and rich feature representations for TAG parsing, alleviating the fundamental trade-off between transition-based and graph-based parsing systems. We also demonstrate that the proposed parser achieves state-of-the-art performance in the downstream tasks of Parsing Evaluation using Textual Entailments (PETE) and Unbounded Dependency Recovery. This provides further support for the claim that TAG is a viable formalism for problems that require rich structural analysis of sentences.",http://aclweb.org/anthology/D17-1180,D17-1,D17-1180,https://arxiv.org/abs/1804.06610,"('Jungo Kasai', 'Bob Frank', 'Tom McCoy', 'Owen Rambow', 'Alexis Nasr')",9,TAG Parsing with Neural Networks and Vector Representations of Supertags,EMNLP,2017
,http://aclweb.org/anthology/D17-1181,D17-1,D17-1181,,"('Heike Adel', 'Hinrich Schütze')",9,"
      Global Normalization of Convolutional Neural Networks for Joint Entity and
      Relation Classification
    ",EMNLP,2017
"Multi objective (MO) optimization is an emerging field which is increasingly being implemented in many industries globally. In this work, the MO optimization of the extraction process of bioactive compounds from the Gardenia Jasminoides Ellis fruit was solved. Three swarm-based algorithms have been applied in conjunction with normal-boundary intersection (NBI) method to solve this MO problem. The gravitational search algorithm (GSA) and the particle swarm optimization (PSO) technique were implemented in this work. In addition, a novel Hopfield-enhanced particle swarm optimization was developed and applied to the extraction problem. By measuring the levels of dominance, the optimality of the approximate Pareto frontiers produced by all the algorithms were gauged and compared. Besides, by measuring the levels of convergence of the frontier, some understanding regarding the structure of the objective space in terms of its relation to the level of frontier dominance is uncovered. Detail comparative studies were conducted on all the algorithms employed and developed in this work.",http://aclweb.org/anthology/D17-1182,D17-1,D17-1182,https://arxiv.org/abs/1611.06086,"('Meishan Zhang', 'Yue Zhang', 'Guohong Fu')",9,End-to-End Neural Relation Extraction with Global Optimization,EMNLP,2017
"Automatic construction of large knowledge graphs (KG) by mining web-scale text datasets has received considerable attention recently. Estimating accuracy of such automatically constructed KGs is a challenging problem due to their size and diversity. This important problem has largely been ignored in prior research we fill this gap and propose KGEval. KGEval binds facts of a KG using coupling constraints and crowdsources the facts that infer correctness of large parts of the KG. We demonstrate that the objective optimized by KGEval is submodular and NP-hard, allowing guarantees for our approximation algorithm. Through extensive experiments on real-world datasets, we demonstrate that KGEval is able to estimate KG accuracy more accurately compared to other competitive baselines, while requiring significantly lesser number of human evaluations.",http://aclweb.org/anthology/D17-1183,D17-1,D17-1183,https://arxiv.org/abs/1610.06912,"('Prakhar Ojha', 'Partha Talukdar')",9,KGEval: Accuracy Estimation of Automatically Constructed Knowledge Graphs,EMNLP,2017
"Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Latent variable models are well suited to deal with the high dimensionality and sparsity of typical knowledge graphs. In recent publications the embedding models were extended to also consider time evolutions, time patterns and subsymbolic representations. In this paper we map embedding models, which were developed purely as solutions to technical problems for modelling temporal knowledge graphs, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory, sensory memory, short-term memory, and working memory. We discuss learning, query answering, the path from sensory input to semantic decoding, and the relationship between episodic memory and semantic memory. We introduce a number of hypotheses on human memory that can be derived from the developed mathematical models.",http://aclweb.org/anthology/D17-1184,D17-1,D17-1184,https://arxiv.org/abs/1511.07972,"('Jay Pujara', 'Eriq Augustine', 'Lise Getoor')",9,Sparsity and Noise: Where Knowledge Graph Embeddings Fall Short,EMNLP,2017
"We construct the duality-symmetric actions for a large class of six-dimensional models describing hierarchies of non-Abelian scalar, vector and tensor fields related to each other by first-order (self-)duality equations that follow from these actions. In particular, this construction provides a Lorentz invariant action for non-Abelian self-dual tensor fields. The class of models includes the bosonic sectors of the 6d (1,0) superconformal models of interacting non-Abelian self-dual tensor, vector, and hypermultiplets.",http://aclweb.org/anthology/D17-1185,D17-1,D17-1185,https://arxiv.org/abs/1305.1304,"('Goran Glavaš', 'Simone Paolo Ponzetto')",9,Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations,EMNLP,2017
"Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which provide rich and useful information for relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with baselines. The source code of this paper can be obtained from https: //github.com/thunlp/PathNRE.",http://aclweb.org/anthology/D17-1186,D17-1,D17-1186,https://arxiv.org/abs/1609.07479,"('Wenyuan Zeng', 'Yankai Lin', 'Zhiyuan Liu', 'Maosong Sun')",9,Incorporating Relation Paths in Neural Relation Extraction,EMNLP,2017
"Adversarial training (AT) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data. We show how to use AT for the tasks of entity recognition and relation extraction. In particular, we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch).",http://aclweb.org/anthology/D17-1187,D17-1,D17-1187,https://arxiv.org/abs/1808.06876,"('Yi Wu', 'David Bamman', 'Stuart Russell')",9,Adversarial Training for Relation Extraction,EMNLP,2017
"The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross- sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.",http://aclweb.org/anthology/D17-1188,D17-1,D17-1188,https://arxiv.org/abs/1609.04873,"('Daniil Sorokin', 'Iryna Gurevych')",9,Context-Aware Representations for Knowledge Base Relation Extraction,EMNLP,2017
"Distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data.",http://aclweb.org/anthology/D17-1189,D17-1,D17-1189,https://arxiv.org/abs/1509.03739,"('Tianyu Liu', 'Kexiang Wang', 'Baobao Chang', 'Zhifang Sui')",9,A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction,EMNLP,2017
,http://aclweb.org/anthology/D17-1190,D17-1,D17-1190,,"('Prafulla Kumar Choubey', 'Ruihong Huang')",9,"
      A Sequential Model for Classifying Temporal Relations between
      Intra-Sentence Events
    ",EMNLP,2017
"Deep residual learning (ResNet) is a new method for training very deep neural networks using identity map-ping for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction.",http://aclweb.org/anthology/D17-1191,D17-1,D17-1191,https://arxiv.org/abs/1707.08866,"('YiYao Huang', 'William Yang Wang')",9,Deep Residual Learning for Weakly-Supervised Relation Extraction,EMNLP,2017
,http://aclweb.org/anthology/D17-1192,D17-1,D17-1192,,"('Qing Zhang', 'Houfeng Wang')",9,"
      Noise-Clustered Distant Supervision for Relation Extraction: A
      Nonparametric Bayesian Perspective
    ",EMNLP,2017
"Semantic Similarity is an important application which finds its use in many downstream NLP applications. Though the task is mathematically defined, semantic similarity's essence is to capture the notions of similarity impregnated in humans. Machines use some heuristics to calculate the similarity between words, but these are typically corpus dependent or are useful for specific domains. The difference between Semantic Similarity and Semantic Relatedness motivates the development of new algorithms. For a human, the word car and road are probably as related as car and bus. But this may not be the case for computational methods. Ontological methods are good at encoding Semantic Similarity and Vector Space models are better at encoding Semantic Relatedness. There is a dearth of methods which leverage ontologies to create better vector representations. The aim of this proposal is to explore in the direction of a hybrid method which combines statistical/vector space methods like Word2Vec and Ontological methods like WordNet to leverage the advantages provided by both.",http://aclweb.org/anthology/D17-1193,D17-1,D17-1193,https://arxiv.org/abs/1805.06503,"('Kata Gábor', 'Haifa Zargayouna', 'Isabelle Tellier', 'Davide Buscaldi', 'Thierry Charnois')",9,Exploring Vector Spaces for Semantic Relations,EMNLP,2017
,http://aclweb.org/anthology/D17-1194,D17-1,D17-1194,,"('Andrey Kutuzov', 'Erik Velldal', 'Lilja Øvrelid')",9,"
      Temporal dynamics of semantic relations in word embeddings: an application
      to predicting armed conflict participants
    ",EMNLP,2017
"Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, EntityNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.",http://aclweb.org/anthology/D17-1195,D17-1,D17-1195,https://arxiv.org/abs/1708.00781,"('Yangfeng Ji', 'Chenhao Tan', 'Sebastian Martschat', 'Yejin Choi', 'Noah A. Smith')",9,Dynamic Entity Representations in Neural Language Models,EMNLP,2017
"The field of quantum algorithms is vibrant. Still, there is currently a lack of programming languages for describing quantum computation on a practical scale, i.e., not just at the level of toy problems. We address this issue by introducing Quipper, a scalable, expressive, functional, higher-order quantum programming language. Quipper has been used to program a diverse set of non-trivial quantum algorithms, and can generate quantum gate representations using trillions of gates. It is geared towards a model of computation that uses a classical computer to control a quantum device, but is not dependent on any particular model of quantum hardware. Quipper has proven effective and easy to use, and opens the door towards using formal methods to analyze quantum algorithms.",http://aclweb.org/anthology/D17-1196,D17-1,D17-1196,https://arxiv.org/abs/1304.3390,"('Ivano Basile', 'Fabio Tamburini')",9,Towards Quantum Language Models,EMNLP,2017
"We introduce a new measure of distance between languages based on word embedding, called word embedding language divergence (WELD). WELD is defined as divergence between unified similarity distribution of words between languages. Using such a measure, we perform language comparison for fifty natural languages and twelve genetic languages. Our natural language dataset is a collection of sentence-aligned parallel corpora from bible translations for fifty languages spanning a variety of language families. Although we use parallel corpora, which guarantees having the same content in all languages, interestingly in many cases languages within the same family cluster together. In addition to natural languages, we perform language comparison for the coding regions in the genomes of 12 different organisms (4 plants, 6 animals, and two human subjects). Our result confirms a significant high-level difference in the genetic language model of humans/animals versus plants. The proposed method is a step toward defining a quantitative measure of similarity between languages, with applications in languages classification, genre identification, dialect identification, and evaluation of translations.",http://aclweb.org/anthology/D17-1197,D17-1,D17-1197,https://arxiv.org/abs/1604.08561,"('Zichao Yang', 'Phil Blunsom', 'Chris Dyer', 'Wang Ling')",9,Reference-Aware Language Models,EMNLP,2017
"In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec's algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.",http://aclweb.org/anthology/D17-1198,D17-1,D17-1198,https://arxiv.org/abs/1707.05266,"('Oren Melamud', 'Ido Dagan', 'Jacob Goldberger')",9,A Simple Language Model based on PMI Matrix Approximations,EMNLP,2017
"Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.",http://aclweb.org/anthology/D17-1199,D17-1,D17-1199,https://arxiv.org/abs/1707.06480,"('Zhenisbek Assylbekov', 'Rustem Takhanov', 'Bagdat Myrzakhmetov', 'Jonathan N. Washington')",9,Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones,EMNLP,2017
"Incorporating semantic information into the codecs during image compression can significantly reduce the repetitive computation of fundamental semantic analysis (such as object recognition) in client-side applications. The same practice also enable the compressed code to carry the image semantic information during storage and transmission. In this paper, we propose a concept called Deep Semantic Image Compression (DeepSIC) and put forward two novel architectures that aim to reconstruct the compressed image and generate corresponding semantic representations at the same time. The first architecture performs semantic analysis in the encoding process by reserving a portion of the bits from the compressed code to store the semantic representations. The second performs semantic analysis in the decoding step with the feature maps that are embedded in the compressed code. In both architectures, the feature maps are shared by the compression and the semantic analytics modules. To validate our approaches, we conduct experiments on the publicly available benchmarking datasets and achieve promising results. We also provide a thorough analysis of the advantages and disadvantages of the proposed technique.",http://aclweb.org/anthology/D17-1200,D17-1,D17-1200,https://arxiv.org/abs/1801.09468,"('Lea Frermann', 'György Szarvas')",9,Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels,EMNLP,2017
,http://aclweb.org/anthology/D17-1201,D17-1,D17-1201,,"('Shen Li', 'Zhe Zhao', 'Tao Liu', 'Renfen Hu', 'Xiaoyong Du')",9,"
      Initializing Convolutional Filters with Semantic Features for Text
      Classification
    ",EMNLP,2017
"We define and study the link prediction problem in bipartite networks, specializing general link prediction algorithms to the bipartite case. In a graph, a link prediction function of two vertices denotes the similarity or proximity of the vertices. Common link prediction functions for general graphs are defined using paths of length two between two nodes. Since in a bipartite graph adjacency vertices can only be connected by paths of odd lengths, these functions do not apply to bipartite graphs. Instead, a certain class of graph kernels (spectral transformation kernels) can be generalized to bipartite graphs when the positive-semidefinite kernel constraint is relaxed. This generalization is realized by the odd component of the underlying spectral transformation. This construction leads to several new link prediction pseudokernels such as the matrix hyperbolic sine, which we examine for rating graphs, authorship graphs, folksonomies, document--feature networks and other types of bipartite networks.",http://aclweb.org/anthology/D17-1202,D17-1,D17-1202,https://arxiv.org/abs/1006.5367,"('Giannis Nikolentzos', 'Polykarpos Meladianos', 'Francois Rousseau', 'Yannis Stavrakas', 'Michalis Vazirgiannis')",9,Shortest-Path Graph Kernels for Document Similarity,EMNLP,2017
"We present a probabilistic model that uses both prosodic and lexical cues for the automatic segmentation of speech into topically coherent units. We propose two methods for combining lexical and prosodic information using hidden Markov models and decision trees. Lexical information is obtained from a speech recognizer, and prosodic features are extracted automatically from speech waveforms. We evaluate our approach on the Broadcast News corpus, using the DARPA-TDT evaluation metrics. Results show that the prosodic model alone is competitive with word-based segmentation methods. Furthermore, we achieve a significant reduction in error by combining the prosodic and word-based knowledge sources.",http://aclweb.org/anthology/D17-1203,D17-1,D17-1203,https://arxiv.org/abs/cs/0105037,"('Weiwei Yang', 'Jordan Boyd-Graber', 'Philip Resnik')",9,Adapting Topic Models using Lexical Associations with Tree Priors,EMNLP,2017
,http://aclweb.org/anthology/D17-1204,D17-1,D17-1204,,"('Natalie Parde', 'Rodney Nielsen')",9,"
      Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation
      for Crowdsourced Data
    ",EMNLP,2017
"The aim of this paper is to propose an approach based on the concept of annotation for supporting design communication. In this paper, we describe a co-operative design case study where we analyse some annotation practices, mainly focused on design minutes recorded during project reviews. We point out specific requirements concerning annotation needs. Based on these requirements, we propose an annotation model, inspired from the Speech Act Theory (SAT) to support communication in a 3D digital environment. We define two types of annotations in the engineering design context, locutionary and illocutionary annotations. The annotations we describe in this paper are materialised by a set of digital artefacts, which have a semantic dimension allowing express/record elements of technical justifications, traces of contradictory debates, etc. In this paper, we first clarify the semantic annotation concept, and we define general properties of annotations in the engineering design context, and the role of annotations in different design project situations. After the description of the case study, where we observe and analyse annotations usage during the design reviews and minute making, the last section is dedicated to present our approach. We then describe the SAT concept, and define the concept of annotation acts. We conclude with a description of basic annotation functionalities that are actually implemented in a software, based on our approach.",http://aclweb.org/anthology/D17-1205,D17-1,D17-1205,https://arxiv.org/abs/0711.2486,"('Chenguang Wang', 'Alan Akbik', 'laura chiticariu', 'Yunyao Li', 'Fei Xia', 'Anbang Xu')",9,CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles,EMNLP,2017
"Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.",http://aclweb.org/anthology/D17-1206,D17-1,D17-1206,https://arxiv.org/abs/1611.01587,"('Kazuma Hashimoto', 'caiming xiong', 'Yoshimasa Tsuruoka', 'Richard Socher')",9,A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,EMNLP,2017
,http://aclweb.org/anthology/D17-1207,D17-1,D17-1207,,"('Meng Zhang', 'Yang Liu', 'Huanbo Luan', 'Maosong Sun')",9,"
      Earth Mover's Distance Minimization for Unsupervised Bilingual Lexicon
      Induction
    ",EMNLP,2017
"Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On Japanese-English we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.",http://aclweb.org/anthology/D17-1208,D17-1,D17-1208,https://arxiv.org/abs/1704.03279,"('Felix Stahlberg', 'Bill Byrne')",9,Unfolding and Shrinking Neural Machine Translation Ensembles,EMNLP,2017
"We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.",http://aclweb.org/anthology/D17-1209,D17-1,D17-1209,https://arxiv.org/abs/1704.04675,"('Joost Bastings', 'Ivan Titov', 'Wilker Aziz', 'Diego Marcheggiani', 'Khalil Simaan')",9,Graph Convolutional Encoders for Syntax-aware Neural Machine Translation,EMNLP,2017
"Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.",http://aclweb.org/anthology/D17-1210,D17-1,D17-1210,https://arxiv.org/abs/1702.02429,"('Jiatao Gu', 'Kyunghyun Cho', 'Victor O.K. Li')",9,Trainable Greedy Decoding for Neural Machine Translation,EMNLP,2017
,http://aclweb.org/anthology/D17-1211,D17-1,D17-1211,,"('Fan Yang', 'Arjun Mukherjee', 'Eduard Dragut')",9,"
      Satirical News Detection and Analysis using Attention Mechanism and
      Linguistic Features
    ",EMNLP,2017
"\emph{Verifiability} is one of the core editing principles in Wikipedia, editors being encouraged to provide citations for the added content. For a Wikipedia article, determining the \emph{citation span} of a citation, i.e. what content is covered by a citation, is important as it helps decide for which content citations are still missing.   We are the first to address the problem of determining the \emph{citation span} in Wikipedia articles. We approach this problem by classifying which textual fragments in an article are covered by a citation. We propose a sequence classification approach where for a paragraph and a citation, we determine the citation span at a fine-grained level.   We provide a thorough experimental evaluation and compare our approach against baselines adopted from the scientific domain, where we show improvement for all evaluation metrics.",http://aclweb.org/anthology/D17-1212,D17-1,D17-1212,https://arxiv.org/abs/1707.07278,"('Besnik Fetahu', 'Katja Markert', 'Avishek Anand')",9,Fine Grained Citation Span for References in Wikipedia,EMNLP,2017
"We propose a dynamic map of knowledge generated from Wikipedia pages and the Web URLs contained therein. GalaxySearch provides answers to the questions we don't know how to ask, by constructing a semantic network of the most relevant pages in Wikipedia related to a search term. This search graph is constructed based on the Wikipedia bidirectional link structure, the most recent edits on the pages, the importance of the page, and the article quality; search results are then ranked by the centrality of their network position. GalaxySearch provides the results in three related ways: (1) WikiSearch - identifying the most prominent Wikipedia pages and Weblinks for a chosen topic, (2) WikiMap - creating a visual temporal map of the changes in the semantic network generated by the search results over the lifetime of the returned Wikipedia articles, and (3) WikiPulse - finding the most recent and most relevant changes and updates about a topic.",http://aclweb.org/anthology/D17-1213,D17-1,D17-1213,https://arxiv.org/abs/1204.3375,"('Diyi Yang', 'Aaron Halfaker', 'Robert Kraut', 'Eduard Hovy')",9,Identifying Semantic Edit Intentions from Revisions in Wikipedia,EMNLP,2017
"In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, result- ing in a sub-optimal representation. To ad- dress this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.",http://aclweb.org/anthology/D17-1214,D17-1,D17-1214,https://arxiv.org/abs/1809.10324,"('Daniel Hewlett', 'Llion Jones', 'Alexandre Lacoste', 'izzeddin gur')",9,Accurate Supervised and Semi-Supervised Machine Reading for Long Documents,EMNLP,2017
"Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of $75\%$ F1 score to $36\%$; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to $7\%$. We hope our insights will motivate the development of new models that understand language more precisely.",http://aclweb.org/anthology/D17-1215,D17-1,D17-1215,https://arxiv.org/abs/1707.07328,"('Robin Jia', 'Percy Liang')",9,Adversarial Examples for Evaluating Reading Comprehension Systems,EMNLP,2017
"Commonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset~\cite{levesque2011winograd}. In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.",http://aclweb.org/anthology/D17-1216,D17-1,D17-1216,https://arxiv.org/abs/1806.02847,"('Hongyu Lin', 'Le Sun', 'Xianpei Han')",9,Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension,EMNLP,2017
"Sentiment classification involves quantifying the affective reaction of a human to a document, media item or an event. Although researchers have investigated several methods to reliably infer sentiment from lexical, speech and body language cues, training a model with a small set of labeled datasets is still a challenge. For instance, in expanding sentiment analysis to new languages and cultures, it may not always be possible to obtain comprehensive labeled datasets. In this paper, we investigate the application of semi-supervised and transfer learning methods to improve performances on low resource sentiment classification tasks. We experiment with extracting dense feature representations, pre-training and manifold regularization in enhancing the performance of sentiment classification systems. Our goal is a coherent implementation of these methods and we evaluate the gains achieved by these methods in matched setting involving training and testing on a single corpus setting as well as two cross corpora settings. In both the cases, our experiments demonstrate that the proposed methods can significantly enhance the model performance against a purely supervised approach, particularly in cases involving a handful of training data.",http://aclweb.org/anthology/D17-1217,D17-1,D17-1217,https://arxiv.org/abs/1806.02863,"('Yichun Yin', 'Yangqiu Song', 'Ming Zhang')",9,Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension,EMNLP,2017
"Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent perception of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.",http://aclweb.org/anthology/D17-1218,D17-1,D17-1218,https://arxiv.org/abs/1704.07203,"('Johannes Daxenberger', 'Steffen Eger', 'Ivan Habernal', 'Christian Stab', 'Iryna Gurevych')",9,What is the Essence of a Claim? Cross-Domain Claim Identification,EMNLP,2017
,http://aclweb.org/anthology/D17-1219,D17-1,D17-1219,,"('Xinya Du', 'Claire Cardie')",9,"
      Identifying Where to Focus in Reading Comprehension for Neural Question
      Generation
    ",EMNLP,2017
"Comprehending lyrics, as found in songs and poems, can pose a challenge to human and machine readers alike. This motivates the need for systems that can understand the ambiguity and jargon found in such creative texts, and provide commentary to aid readers in reaching the correct interpretation. We introduce the task of automated lyric annotation (ALA). Like text simplification, a goal of ALA is to rephrase the original text in a more easily understandable manner. However, in ALA the system must often include additional information to clarify niche terminology and abstract concepts. To stimulate research on this task, we release a large collection of crowdsourced annotations for song lyrics. We analyze the performance of translation and retrieval models on this task, measuring performance with both automated and human evaluation. We find that each model captures a unique type of information important to the task.",http://aclweb.org/anthology/D17-1220,D17-1,D17-1220,https://arxiv.org/abs/1708.03492,"('Lucas Sterckx', 'Jason Naradowsky', 'Bill Byrne', 'Thomas Demeester', 'Chris Develder')",9,Break it Down for Me: A Study in Automated Lyric Annotation,EMNLP,2017
,http://aclweb.org/anthology/D17-1221,D17-1,D17-1221,,"('Piji Li', 'Wai Lam', 'Lidong Bing', 'Weiwei Guo', 'Hang Li')",9,"
      Cascaded Attention based Unsupervised Information Distillation for
      Compressive Summarization
    ",EMNLP,2017
We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN).   Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality.   Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables.   Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states.   Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods.,http://aclweb.org/anthology/D17-1222,D17-1,D17-1222,https://arxiv.org/abs/1708.00625,"('Piji Li', 'Wai Lam', 'Lidong Bing', 'Zihao Wang')",9,Deep Recurrent Generative Decoder for Abstractive Text Summarization,EMNLP,2017
"Sequence to sequence (Seq2Seq) learning has recently been used for abstractive and extractive summarization. In current study, Seq2Seq models have been used for eBay product description summarization. We propose a novel Document-Context based Seq2Seq models using RNNs for abstractive and extractive summarizations. Intuitively, this is similar to humans reading the title, abstract or any other contextual information before reading the document. This gives humans a high-level idea of what the document is about. We use this idea and propose that Seq2Seq models should be started with contextual information at the first time-step of the input to obtain better summaries. In this manner, the output summaries are more document centric, than being generic, overcoming one of the major hurdles of using generative models. We generate document-context from user-behavior and seller provided information. We train and evaluate our models on human-extracted-golden-summaries. The document-contextual Seq2Seq models outperform standard Seq2Seq models. Moreover, generating human extracted summaries is prohibitively expensive to scale, we therefore propose a semi-supervised technique for extracting approximate summaries and using it for training Seq2Seq models at scale. Semi-supervised models are evaluated against human extracted summaries and are found to be of similar efficacy. We provide side by side comparison for abstractive and extractive summarizers (contextual and non-contextual) on same evaluation dataset. Overall, we provide methodologies to use and evaluate the proposed techniques for large document summarization. Furthermore, we found these techniques to be highly effective, which is not the case with existing techniques.",http://aclweb.org/anthology/D17-1223,D17-1,D17-1223,https://arxiv.org/abs/1807.08000,"('Masaru Isonuma', 'Toru Fujino', 'Junichiro Mori', 'Yutaka Matsuo', 'Ichiro Sakata')",9,Extractive Summarization Using Multi-Task Learning with Document Classification,EMNLP,2017
"This paper proposes relational program synthesis, a new problem that concerns synthesizing one or more programs that collectively satisfy a relational specification. As a dual of relational program verification, relational program synthesis is an important problem that has many practical applications, such as automated program inversion and automatic generation of comparators. However, this relational synthesis problem introduces new challenges over its non-relational counterpart due to the combinatorially larger search space. As a first step towards solving this problem, this paper presents a synthesis technique that combines the counterexample-guided inductive synthesis framework with a novel inductive synthesis algorithm that is based on relational version space learning. We have implemented the proposed technique in a framework called Relish, which can be instantiated to different application domains by providing a suitable domain-specific language and the relevant relational specification. We have used the Relish framework to build relational synthesizers to automatically generate string encoders/decoders as well as comparators, and we evaluate our tool on several benchmarks taken from prior work and online forums. Our experimental results show that the proposed technique can solve almost all of these benchmarks and that it significantly outperforms EUSolver, a generic synthesis framework that won the general track of the most recent SyGuS competition.",http://aclweb.org/anthology/D17-1224,D17-1,D17-1224,https://arxiv.org/abs/1809.02283,"('Jianmin Zhang', 'Xiaojun Wan')",9,Towards Automatic Construction of News Overview Articles by News Synthesis,EMNLP,2017
"Various treebanks have been released for dependency parsing. Despite that treebanks may belong to different languages or have different annotation schemes, they contain syntactic knowledge that is potential to benefit each other. This paper presents an universal framework for exploiting these multi-typed treebanks to improve parsing with deep multi-task learning. We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. Multiple treebanks are trained jointly and interacted with multi-level parameter sharing. Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models.",http://aclweb.org/anthology/D17-1225,D17-1,D17-1225,https://arxiv.org/abs/1606.01161,"('Kai Zhao', 'Liang Huang')",9,Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank,EMNLP,2017
,http://aclweb.org/anthology/D17-1226,D17-1,D17-1226,,"('Prafulla Kumar Choubey', 'Ruihong Huang')",9,"
      Event Coreference Resolution by Iteratively Unfolding Inter-dependencies
      among Events
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1227,D17-1,D17-1227,,"('Liang Huang', 'Kai Zhao', 'Mingbo Ma')",9,"
      When to Finish? Optimal Beam Search for Neural Text Generation (modulo
      beam size)
    ",EMNLP,2017
"We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selective-sampling. We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to restrict style and topic without degrading output quality in conversational tasks.",http://aclweb.org/anthology/D17-1228,D17-1,D17-1228,https://arxiv.org/abs/1709.03010,"('Di Wang', 'Nebojsa Jojic', 'Chris Brockett', 'Eric Nyberg')",9,Steering Output Style and Topic in Neural Response Generation,EMNLP,2017
"Recognising dialogue acts (DA) is important for many natural language processing tasks such as dialogue generation and intention recognition. In this paper, we propose a dual-attention hierarchical recurrent neural network for dialogue act classification. Our model is partially inspired by the observation that conversational utterances are normally associated with both a dialogue act and a topic, where the former captures the social act and the latter describes the subject matter. However, such a dependency between dialogue acts and topics has not been utilised by most existing systems for DA classification. With a novel dual task-specific attention mechanism, our model is able, for utterances, to capture information about both dialogue acts and topics, as well as information about the interactions between them. We evaluate the performance of our model on two publicly available datasets, i.e., Switchboard and DailyDialog. Experimental results show that by modelling topic as an auxiliary task, our model can significantly improve DA classification.",http://aclweb.org/anthology/D17-1229,D17-1,D17-1229,https://arxiv.org/abs/1810.09154,"('Quan Hung Tran', 'Ingrid Zukerman', 'Gholamreza Haffari')",9,Preserving Distributional Information in Dialogue Act Classification,EMNLP,2017
"In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues.   In addition to adversarial training we describe a model for adversarial {\em evaluation} that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.",http://aclweb.org/anthology/D17-1230,D17-1,D17-1230,https://arxiv.org/abs/1701.06547,"('Jiwei Li', 'Will Monroe', 'Tianlin Shi', 'Sėbastien Jean', 'Alan Ritter', 'Dan Jurafsky')",9,Adversarial Learning for Neural Dialogue Generation,EMNLP,2017
"Recent works have proposed neural models for dialog act classification in spoken dialogs. However, they have not explored the role and the usefulness of acoustic information. We propose a neural model that processes both lexical and acoustic features for classification. Our results on two benchmark datasets reveal that acoustic features are helpful in improving the overall accuracy. Finally, a deeper analysis shows that acoustic features are valuable in three cases: when a dialog act has sufficient data, when lexical information is limited and when strong lexical cues are not present.",http://aclweb.org/anthology/D17-1231,D17-1,D17-1231,https://arxiv.org/abs/1803.00831,"('Yang Liu', 'Kun Han', 'Zhao Tan', 'Yun Lei')",9,Using Context Information for Dialog Act Classification in DNN Framework,EMNLP,2017
"For the task of recognizing dialogue acts, we are applying the Transformation-Based Learning (TBL) machine learning algorithm. To circumvent a sparse data problem, we extract values of well-motivated features of utterances, such as speaker direction, punctuation marks, and a new feature, called dialogue act cues, which we find to be more effective than cue phrases and word n-grams in practice. We present strategies for constructing a set of dialogue act cues automatically by minimizing the entropy of the distribution of dialogue acts in a training corpus, filtering out irrelevant dialogue act cues, and clustering semantically-related words. In addition, to address limitations of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures. These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task.",http://aclweb.org/anthology/D17-1232,D17-1,D17-1232,https://arxiv.org/abs/cmp-lg/9806006,"('Yohan Jo', 'Michael Yoder', 'Hyeju Jang', 'Carolyn Rose')",9,Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences,EMNLP,2017
,http://aclweb.org/anthology/D17-1233,D17-1,D17-1233,,"('Lili Yao', 'Yaoyuan Zhang', 'Yansong Feng', 'Dongyan Zhao', 'Rui Yan')",9,"
      Towards Implicit Content-Introducing for Generative Short-Text
      Conversation Systems
    ",EMNLP,2017
"Dialogue policy transfer enables us to build dialogue policies in a target domain with little data by leveraging knowledge from a source domain with plenty of data. Dialogue sentences are usually represented by speech-acts and domain slots, and the dialogue policy transfer is usually achieved by assigning a slot mapping matrix based on human heuristics. However, existing dialogue policy transfer methods cannot transfer across dialogue domains with different speech-acts, for example, between systems built by different companies. Also, they depend on either common slots or slot entropy, which are not available when the source and target slots are totally disjoint and no database is available to calculate the slot entropy. To solve this problem, we propose a Policy tRansfer across dOMaIns and SpEech-acts (PROMISE) model, which is able to transfer dialogue policies across domains with different speech-acts and disjoint slots. The PROMISE model can learn to align different speech-acts and slots simultaneously, and it does not require common slots or the calculation of the slot entropy. Experiments on both real-world dialogue data and simulations demonstrate that PROMISE model can effectively transfer dialogue policies across domains with different speech-acts and disjoint slots.",http://aclweb.org/anthology/D17-1234,D17-1,D17-1234,https://arxiv.org/abs/1804.07691,"('Cheng Chang', 'Runzhe Yang', 'Lu Chen', 'Xiang Zhou', 'Kai Yu')",9,Affordable On-line Dialogue Policy Learning,EMNLP,2017
,http://aclweb.org/anthology/D17-1235,D17-1,D17-1235,,"('Yuanlong Shao', 'Stephan Gouws', 'Denny Britz', 'Anna Goldie', 'Brian Strope', 'Ray Kurzweil')",9,"
      Generating High-Quality and Informative Conversation Responses with
      Sequence-to-Sequence Models
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1236,D17-1,D17-1236,,"('Arash Eshghi', 'Igor Shalyminov', 'Oliver Lemon')",9,"
      Bootstrapping incremental dialogue systems from minimal data: the
      generalisation power of dialogue grammars
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1237,D17-1,D17-1237,,"('Baolin Peng', 'Xiujun Li', 'Lihong Li', 'Jianfeng Gao', 'Asli Celikyilmaz', 'Sungjin Lee', 'Kam-Fai Wong')",9,"
      Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep
      Reinforcement Learning
    ",EMNLP,2017
"The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.",http://aclweb.org/anthology/D17-1238,D17-1,D17-1238,https://arxiv.org/abs/1707.06875,"('Jekaterina Novikova', 'Ondřej Dušek', 'Amanda Cercas Curry', 'Verena Rieser')",9,Why We Need New Evaluation Metrics for NLG,EMNLP,2017
"Challenges for physical solitaire puzzle games are typically designed in advance by humans and limited in number. Alternately, some games incorporate stochastic setup rules, where the human solver randomly sets up the game board before solving the challenge, which can greatly increase the number of possible challenges. However, these setup rules can often generate unsolvable or uninteresting challenges. To better understand these setup processes, we apply a taxonomy for procedural content generation algorithms to solitaire puzzle games. In particular, for the game Fujisan, we examine how different stochastic challenge generation algorithms attempt to minimize undesirable challenges, and we report their affect on ease of physical setup, challenge solvability, and challenge difficulty. We find that algorithms can be simple for the solver yet generate solvable and difficult challenges, by constraining randomness through embedding sub-elements of the puzzle mechanics into the physical pieces of the game.",http://aclweb.org/anthology/D17-1239,D17-1,D17-1239,https://arxiv.org/abs/1810.01926,"('Sam Wiseman', 'Stuart Shieber', 'Alexander Rush')",9,Challenges in Data-to-Document Generation,EMNLP,2017
,http://aclweb.org/anthology/D17-1240,D17-1,D17-1240,,"('Jasabanta Patro', 'Bidisha Samanta', 'Saurabh Singh', 'Abhipsa Basu', 'Prithwish Mukherjee', 'Monojit Choudhury', 'Animesh Mukherjee')",9,"
      All that is English may be Hindi: Enhancing language identification
      through automatic ranking of the likeliness of word borrowing in social
      media
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1241,D17-1,D17-1241,,"('Tao Ding', 'Warren K. Bickel', 'Shimei Pan')",9,"
      Multi-View Unsupervised User Feature Embedding for Social Media-based
      Substance Use Prediction
    ",EMNLP,2017
"We consider words coding exchange of three intervals with permutation (3,2,1), here called 3iet words. Recently, a characterization of substitution invariant 3iet words was provided. We study the opposite question: what are the morphisms fixing a 3iet word? We reveal a narrow connection of such morphisms and morphisms fixing Sturmian words using the new notion of amicability.",http://aclweb.org/anthology/D17-1242,D17-1,D17-1242,https://arxiv.org/abs/0811.2147,"('Aparna Garimella', 'Carmen Banea', 'Rada Mihalcea')",9,Demographic-aware word associations,EMNLP,2017
,http://aclweb.org/anthology/D17-1243,D17-1,D17-1243,,"('Hao Cheng', 'Hao Fang', 'Mari Ostendorf')",9,"
      A Factored Neural Network Model for Characterizing Online Discussions in
      Vector Space
    ",EMNLP,2017
"Current theories from biosocial (e.g.: the role of neurotransmitters in behavioral features), ecological (e.g.: cultural, political, and institutional conditions), and interpersonal (e.g.: attachment) perspectives have grounded interpersonal and romantic relationships in normative social experiences. However, these theories have not been developed to the point of providing a solid theoretical understanding of the dynamics present in interpersonal and romantic relationships, and integrative theories are still lacking. In this paper, mathematical models are use to investigate the dynamics of interpersonal and romantic relationships, which are examined via ordinary and stochastic differential equations, in order to provide insight into the behaviors of love. The analysis starts with a deterministic model and progresses to nonlinear stochastic models capturing the stochastic rates and factors (e.g.: ecological factors, such as historical, cultural and community conditions) that affect proximal experiences and shape the patterns of relationship. Numerical examples are given to illustrate various dynamics of interpersonal and romantic behaviors (with emphasis placed on sustained oscillations, and transitions between locally stable equilibria) that are observable in stochastic models (closely related to real interpersonal dynamics), but absent in deterministic models.",http://aclweb.org/anthology/D17-1244,D17-1,D17-1244,https://arxiv.org/abs/0911.0013,"('Farzana Rashid', 'Eduardo Blanco')",9,Dimensions of Interpersonal Relationships: Corpus and Experiments,EMNLP,2017
"Internet users generate content at unprecedented rates. Building intelligent systems capable of discriminating useful content within this ocean of information is thus becoming a urgent need. In this paper, we aim to predict the usefulness of Amazon reviews, and to do this we exploit features coming from an off-the-shelf argumentation mining system. We argue that the usefulness of a review, in fact, is strictly related to its argumentative content, whereas the use of an already trained system avoids the costly need of relabeling a novel dataset. Results obtained on a large publicly available corpus support this hypothesis.",http://aclweb.org/anthology/D17-1245,D17-1,D17-1245,https://arxiv.org/abs/1809.08145,"('Mihai Dusmanu', 'Elena Cabrio', 'Serena Villata')",9,"Argument Mining on Twitter: Arguments, Facts and Sources",EMNLP,2017
"The most commonly used Japanese alphabets are Kanji, Hiragana and Katakana. The Kanji alphabet includes pictographs or ideographic characters that were adopted from the Chinese alphabet. Hiragana is used to spell words of Japanese origin, while Katakana is used to spell words of western or other foreign origin.   Two methods are commonly used to input Japanese to the computer. One, the 'kana input method' that uses a keyboard having 46 Japanese iroha (or kana) letter keys. The other method is 'Roma-ji input method', where the Japanese letters are composed of English input from a standard QWERTY keyboard. Both the methods have their advantages and disadvantages.   This article analyses two inventions on inputting Japanese language through a computer keyboard. One invention uses a standard English keyboard to input Japanese characters, the other invention uses a standard mobile phone key board to input the Japanese characters.",http://aclweb.org/anthology/D17-1246,D17-1,D17-1246,https://arxiv.org/abs/1310.3115,"('Tatsuya Aoki', 'Ryohei Sasano', 'Hiroya Takamura', 'Manabu Okumura')",9,Distinguishing Japanese Non-standard Usages from Standard Ones,EMNLP,2017
"Through a particular choice of a predicate (e.g., ""x violated y""), a writer can subtly connote a range of implied sentiments and presupposed facts about the entities x and y: (1) writer's perspective: projecting x as an ""antagonist""and y as a ""victim"", (2) entities' perspective: y probably dislikes x, (3) effect: something bad happened to y, (4) value: y is something valuable, and (5) mental state: y is distressed by the event. We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations. First, we investigate the feasibility of obtaining connotative labels through crowdsourcing experiments. We then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations. Empirical results confirm that connotation frames can be induced from various data sources that reflect how people use language and give rise to the connotative meanings. We conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media.",http://aclweb.org/anthology/D17-1247,D17-1,D17-1247,https://arxiv.org/abs/1506.02739,"('Maarten Sap', 'Marcella Cindy Prasettio', 'Ari Holtzman', 'Hannah Rashkin', 'Yejin Choi')",9,Connotation Frames of Power and Agency in Modern Films,EMNLP,2017
"This article is a sequel to our earlier work [25]. The main objective of our research is to explore the potential of supervised machine learning in face-induced social computing and cognition, riding on the momentum of much heralded successes of face processing, analysis and recognition on the tasks of biometric-based identification. We present a case study of automated statistical inference on sociopsychological perceptions of female faces controlled for race, attractiveness, age and nationality. Our empirical evidences point to the possibility of training machine learning algorithms, using example face images characterized by internet users, to predict perceptions of personality traits and demeanors.",http://aclweb.org/anthology/D17-1248,D17-1,D17-1248,https://arxiv.org/abs/1612.04158,"('Daniel Preoţiuc-Pietro', 'Sharath Chandra Guntuku', 'Lyle Ungar')",9,Controlling Human Perception of Basic User Traits,EMNLP,2017
"The world's digital transformation has influenced not only the way we do business, but also the way we perform daily activities. In fact, the past Presidential elections in the United States as well as those in Great Britain (Brexit) and in Colombia (peace agreement referendum) are proof that social media play an important part in modern politics. In fact, this digital political field is filled by political movements and political candidates looking for popular support (number of followers), regular citizens' messages discussing social issues (trending topics flooding social media), or even political propaganda in favor or against politicians or political movements (advertisement). One of the issues with social media in this era is the presence of automatic accounts (bots) that artificially fill accounts with fake followers, create false trending topics, and share fake news or simply flood the net with propaganda. All this artificial information may influence people and sometimes may even censor people's real opinions undermining their freedom of speech. In this paper, we propose a methodology to track elections and a set of tools used to collect and analyze election data. In particular, this paper discusses our experiences during the Presidential Elections in Ecuador held in 2017. In fact, we show how all candidates prepared an online campaign in social media (Twitter) and how the political campaign altered a common follower rate subscription. We discuss that the high presence of followers during the period between the first and second round of elections may be altered by automatic accounts. Finally, we use bot detection systems and gathered more than 30,000 political motivated bots. In our data analysis, we show that these bots were mainly used for propaganda purposes in favor or against a particular candidate.",http://aclweb.org/anthology/D17-1249,D17-1,D17-1249,https://arxiv.org/abs/1807.06147,"('Clément Gautrais', 'Peggy Cellier', 'René Quiniou', 'Alexandre Termier')",9,Topic Signatures in Political Campaign Speeches,EMNLP,2017
"One goal of online social recommendation systems is to harness the wisdom of crowds in order to identify high quality content. Yet the sequential voting mechanisms that are commonly used by these systems are at odds with existing theoretical and empirical literature on optimal aggregation. This literature suggests that sequential voting will promote herding---the tendency for individuals to copy the decisions of others around them---and hence lead to suboptimal content recommendation. Is there a problem with our practice, or a problem with our theory? Previous attempts at answering this question have been limited by a lack of objective measurements of content quality. Quality is typically defined endogenously as the popularity of content in absence of social influence. The flaw of this metric is its presupposition that the preferences of the crowd are aligned with underlying quality. Domains in which content quality can be defined exogenously and measured objectively are thus needed in order to better assess the design choices of social recommendation systems. In this work, we look to the domain of education, where content quality can be measured via how well students are able to learn from the material presented to them. Through a behavioral experiment involving a simulated massive open online course (MOOC) run on Amazon Mechanical Turk, we show that sequential voting systems can surface better content than systems that elicit independent votes.",http://aclweb.org/anthology/D17-1250,D17-1,D17-1250,https://arxiv.org/abs/1603.04466,"('H. Andrew Schwartz', 'Masoud Rouhizadeh', 'Michael Bishop', 'Philip Tetlock', 'Barbara Mellers', 'Lyle Ungar')",9,Assessing Objective Recommendation Quality through Political Forecasting,EMNLP,2017
,http://aclweb.org/anthology/D17-1251,D17-1,D17-1251,,"('Masumi Shirakawa', 'Takahiro Hara', 'Takuya Maekawa')",9,"
      Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on
      Microblogs Using Set Cover Problem
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1252,D17-1,D17-1252,,"('Haoruo Peng', 'Ming-Wei Chang', 'Wen-tau Yih')",9,"
      Maximum Margin Reward Networks for Learning from Explicit and Implicit
      Supervision
    ",EMNLP,2017
"Methodological contributions: This paper introduces a family of kernels for analyzing (anatomical) trees endowed with vector valued measurements made along the tree. While state-of-the-art graph and tree kernels use combinatorial tree/graph structure with discrete node and edge labels, the kernels presented in this paper can include geometric information such as branch shape, branch radius or other vector valued properties. In addition to being flexible in their ability to model different types of attributes, the presented kernels are computationally efficient and some of them can easily be computed for large datasets (N of the order 10.000) of trees with 30-600 branches. Combining the kernels with standard machine learning tools enables us to analyze the relation between disease and anatomical tree structure and geometry. Experimental results: The kernels are used to compare airway trees segmented from low-dose CT, endowed with branch shape descriptors and airway wall area percentage measurements made along the tree. Using kernelized hypothesis testing we show that the geometric airway trees are significantly differently distributed in patients with Chronic Obstructive Pulmonary Disease (COPD) than in healthy individuals. The geometric tree kernels also give a significant increase in the classification accuracy of COPD from geometric tree structure endowed with airway wall thickness measurements in comparison with state-of-the-art methods, giving further insight into the relationship between airway wall thickness and COPD. Software: Software for computing kernels and statistical tests is available at http://image.diku.dk/aasa/software.php.",http://aclweb.org/anthology/D17-1253,D17-1,D17-1253,https://arxiv.org/abs/1303.7390,"('Henning Wachsmuth', 'Giovanni Da San Martino', 'Dora Kiesel', 'Benno Stein')",9,The Impact of Modeling Overall Argumentation with Tree Kernels,EMNLP,2017
"We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes. The model is learned by using a convolutional neural network as an encoder to map an input sentence into a continuous vector, and using a long short-term memory recurrent neural network as a decoder. Several tasks are considered, including sentence reconstruction and future sentence prediction. Further, a hierarchical encoder-decoder model is proposed to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.",http://aclweb.org/anthology/D17-1254,D17-1,D17-1254,https://arxiv.org/abs/1611.07897,"('Zhe Gan', 'Yunchen Pu', 'Ricardo Henao', 'Chunyuan Li', 'Xiaodong He', 'Lawrence Carin')",9,Learning Generic Sentence Representations Using Convolutional Neural Networks,EMNLP,2017
,http://aclweb.org/anthology/D17-1255,D17-1,D17-1255,,"('Hadi Amiri', 'Timothy Miller', 'Guergana Savova')",9,"
      Repeat before Forgetting: Spaced Repetition for Efficient and Effective
      Training of Neural Networks
    ",EMNLP,2017
"We study cross-lingual sequence tagging with little or no labeled data in the target language. Adversarial training has previously been shown to be effective for training cross-lingual sentence classifiers. However, it is not clear if language-agnostic representations enforced by an adversarial language discriminator will also enable effective transfer for token-level prediction tasks. Therefore, we experiment with different types of adversarial training on two tasks: dependency parsing and sentence compression. We show that adversarial training consistently leads to improved cross-lingual performance on each task compared to a conventionally trained baseline.",http://aclweb.org/anthology/D17-1256,D17-1,D17-1256,https://arxiv.org/abs/1808.04736,"('Tao Gui', 'Qi Zhang', 'Haoran Huang', 'Minlong Peng', 'Xuanjing Huang')",9,Part-of-Speech Tagging for Twitter with Adversarial Neural Networks,EMNLP,2017
,http://aclweb.org/anthology/D17-1257,D17-1,D17-1257,,"('Bofang Li', 'Tao Liu', 'Zhe Zhao', 'Buzhou Tang', 'Aleksandr Drozd', 'Anna Rogers', 'Xiaoyong Du')",9,"
      Investigating Different Syntactic Context Types and Context
      Representations for Learning Word Embeddings
    ",EMNLP,2017
"In recent years, more research has been devoted to studying the subtask of the complete shallow discourse parsing, such as indentifying discourse connective and arguments of connective. There is a need to design a full discourse parser to pull these subtasks together. So we develop a discourse parser turning the free text into discourse relations. The parser includes connective identifier, arguments identifier, sense classifier and non-explicit identifier, which connects with each other in pipeline. Each component applies the maximum entropy model with abundant lexical and syntax features extracted from the Penn Discourse Tree-bank. The head-based representation of the PDTB is adopted in the arguments identifier, which turns the problem of indentifying the arguments of discourse connective into finding the head and end of the arguments. In the non-explicit identifier, the contextual type features like words which have high frequency and can reflect the discourse relation are introduced to improve the performance of non-explicit identifier. Compared with other methods, experimental results achieve the considerable performance.",http://aclweb.org/anthology/D17-1258,D17-1,D17-1258,https://arxiv.org/abs/1710.11334,"('Chloé Braud', 'Ophélie Lacroix', 'Anders Søgaard')",9,Does syntax help discourse segmentation? Not so much,EMNLP,2017
"Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).",http://aclweb.org/anthology/D17-1259,D17-1,D17-1259,https://arxiv.org/abs/1706.05125,"('Mike Lewis', 'Denis Yarats', 'Yann Dauphin', 'Devi Parikh', 'Dhruv Batra')",9,Deal or No Deal? End-to-End Learning of Negotiation Dialogues,EMNLP,2017
"In statistical dialogue management, the dialogue manager learns a policy that maps a belief state to an action for the system to perform. Efficient exploration is key to successful policy optimisation. Current deep reinforcement learning methods are very promising but rely on epsilon-greedy exploration, thus subjecting the user to a random choice of action during learning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) estimate uncertainties and are sample efficient, leading to better user experience, but on the expense of a greater computational complexity. This paper examines approaches to extract uncertainty estimates from deep Q-networks (DQN) in the context of dialogue management. We perform an extensive benchmark of deep Bayesian methods to extract uncertainty estimates, namely Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and alpha-divergences, combining it with DQN algorithm.",http://aclweb.org/anthology/D17-1260,D17-1,D17-1260,https://arxiv.org/abs/1711.11486,"('Lu Chen', 'Xiang Zhou', 'Cheng Chang', 'Runzhe Yang', 'Kai Yu')",9,Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning,EMNLP,2017
"Debate and deliberation play essential roles in politics and government, but most models presume that debates are won mainly via superior style or agenda control. Ideally, however, debates would be won on the merits, as a function of which side has the stronger arguments. We propose a predictive model of debate that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118 Oxford-style debates, our model's combination of content (as latent topics) and style (as linguistic features) allows us to predict audience-adjudicated winners with 74% accuracy, significantly outperforming linguistic features alone (66%). Our model finds that winning sides employ stronger arguments, and allows us to identify the linguistic features associated with strong or weak arguments.",http://aclweb.org/anthology/D17-1261,D17-1,D17-1261,https://arxiv.org/abs/1705.05040,"('Peter Potash', 'Anna Rumshisky')",9,Towards Debate Automation: a Recurrent Model for Predicting Debate Winners,EMNLP,2017
,http://aclweb.org/anthology/D17-1262,D17-1,D17-1262,,"('Qingsong Ma', 'Yvette Graham', 'Timothy Baldwin', 'Qun Liu')",9,"
      Further Investigation into Reference Bias in Monolingual Evaluation of
      Machine Translation
    ",EMNLP,2017
"Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system's capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach.",http://aclweb.org/anthology/D17-1263,D17-1,D17-1263,https://arxiv.org/abs/1704.07431,"('Pierre Isabelle', 'Colin Cherry', 'George Foster')",9,A Challenge Set Approach to Evaluating Machine Translation,EMNLP,2017
We present a method for learning bilingual translation dictionaries between English and Bantu languages. We show that exploiting the grammatical structure common to Bantu languages enables bilingual dictionary induction for languages where training data is unavailable.,http://aclweb.org/anthology/D17-1264,D17-1,D17-1264,https://arxiv.org/abs/1811.07080,"('Ndapandula Nakashole', 'Raphael Flauger')",9,Knowledge Distillation for Bilingual Dictionary Induction,EMNLP,2017
,http://aclweb.org/anthology/D17-1265,D17-1,D17-1265,,['Rachel Bawden'],9,"
      Machine Translation, it's a question of style, innit? The case of English
      tag questions
    ",EMNLP,2017
"In this paper, we attempt to classify tweets into root categories of the Amazon browse node hierarchy using a set of tweets with browse node ID labels, a much larger set of tweets without labels, and a set of Amazon reviews. Examining twitter data presents unique challenges in that the samples are short (under 140 characters) and often contain misspellings or abbreviations that are trivial for a human to decipher but difficult for a computer to parse. A variety of query and document expansion techniques are implemented in an effort to improve information retrieval to modest success.",http://aclweb.org/anthology/D17-1266,D17-1,D17-1266,https://arxiv.org/abs/1511.08299,"('Nima Pourdamghani', 'Kevin Knight')",9,Deciphering Related Languages,EMNLP,2017
"Out-Of-Vocabulary (OOV) words can pose serious challenges for machine translation (MT) tasks, and in particular, for Low-Resource Languages (LRLs). This paper adapts variants of seq2seq models to perform transduction of such words from Hindi to Bhojpuri (an LRL instance), learning from a set of cognate pairs built upon a bilingual dictionary of Hindi-Bhojpuri words. We demonstrate that our models can effectively be used for languages that have a limited amount of parallel corpora, by working at the character-level to grasp phonetic and orthographic similarities across multiple types of word adaptions, whether synchronic or diachronic, loan words or cognates. We provide a comprehensive overview over the training aspects of character-level NMT systems adapted to this task, combined with a detailed analysis of their respective error cases. Using our method, we achieve an improvement by over 6 BLEU on the Hindi-to-Bhojpuri translation task. Further, we show that such transductions generalize well to other languages by applying it successfully to Hindi-Bangla cognate pairs. Our work can be seen as an important step in the process of: (i) resolving the OOV words problem arising in MT tasks, (ii) creating effective parallel corpora for resource-constrained languages, and (iii) leveraging the enhanced semantic knowledge captured by word-level embeddings onto character-level tasks.",http://aclweb.org/anthology/D17-1267,D17-1,D17-1267,https://arxiv.org/abs/1811.08816,"('Adam St Arnaud', 'David Beck', 'Grzegorz Kondrak')",9,Identifying Cognate Sets Across Dictionaries of Related Languages,EMNLP,2017
"Although linguistic typology has a long history, computational approaches have only recently gained popularity. The use of distributed representations in computational linguistics has also become increasingly popular. A recent development is to learn distributed representations of language, such that typologically similar languages are spatially close to one another. Although empirical successes have been shown for such language representations, they have not been subjected to much typological probing. In this paper, we first look at whether this type of language representations are empirically useful for model transfer between Uralic languages in deep neural networks. We then investigate which typological features are encoded in these representations by attempting to predict features in the World Atlas of Language Structures, at various stages of fine-tuning of the representations. We focus on Uralic languages, and find that some typological traits can be automatically inferred with accuracies well above a strong baseline.",http://aclweb.org/anthology/D17-1268,D17-1,D17-1268,https://arxiv.org/abs/1711.05468,"('Chaitanya Malaviya', 'Graham Neubig', 'Patrick Littell')",9,Learning Language Representations for Typology Prediction,EMNLP,2017
"Named Entity Recognition is always important when dealing with major Natural Language Processing tasks such as information extraction, question-answering, machine translation, document summarization etc so in this paper we put forward a survey of Named Entities in Indian Languages with particular reference to Assamese. There are various rule-based and machine learning approaches available for Named Entity Recognition. At the very first of the paper we give an idea of the available approaches for Named Entity Recognition and then we discuss about the related research in this field. Assamese like other Indian languages is agglutinative and suffers from lack of appropriate resources as Named Entity Recognition requires large data sets, gazetteer list, dictionary etc and some useful feature like capitalization as found in English cannot be found in Assamese. Apart from this we also describe some of the issues faced in Assamese while doing Named Entity Recognition.",http://aclweb.org/anthology/D17-1269,D17-1,D17-1269,https://arxiv.org/abs/1407.2918,"('Stephen Mayhew', 'Chen-Tse Tsai', 'Dan Roth')",9,Cheap Translation for Cross-Lingual Named Entity Recognition,EMNLP,2017
,http://aclweb.org/anthology/D17-1270,D17-1,D17-1270,,"('Ivan Vulić', 'Nikola Mrkšić', 'Anna Korhonen')",9,"
      Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector
      Space Specialisation
    ",EMNLP,2017
"As a kind of meta-data feature, annotations have been formally introduced into Java since Java 5. Since the introduction, annotations have been widely used by the Java community for different purposes, such as compiler guidance and runtime processing. Despite the ever-growing use, there is still limited empirical evidence about how developers use annotations in practice and the impact of annotation use on software quality. To fill this gap, we perform the first large-scale empirical study about Java annotation uses on 1,094 open-source projects hosted on GitHub. Our study answers some fundamental questions about Java annotation use. First, we answer the question ""annotate what?"" and find that annotations are typically used to annotate 4 aspects of program elements. Second, we answer the question ""how annotations evolve?"" and identify 6 different annotation change types, their frequencies, and their characteristics. Third, we answer the question ""who uses annotations?"" and establish the relationships between annotation uses and code ownership and developer experience. In addition, we also use regression models to explore the correlation between annotation uses and code quality, and we find that annotations do have an impact on making code less error-prone.",http://aclweb.org/anthology/D17-1271,D17-1,D17-1271,https://arxiv.org/abs/1805.01965,"('Annemarie Friedrich', 'Damyana Gateva')",9,Classification of telicity using cross-linguistic annotation projection,EMNLP,2017
,http://aclweb.org/anthology/D17-1272,D17-1,D17-1272,,"('Carolin Lawrence', 'Artem Sokolov', 'Stefan Riezler')",9,"
      Counterfactual Learning from Bandit Feedback under Deterministic Logging :
      A Case Study in Statistical Machine Translation
    ",EMNLP,2017
"Coordinate relation refers to the relation between instances of a concept and the relation between the directly hyponyms of a concept. In this paper, we focus on the task of extracting terms which are coordinate with a user given seed term in Chinese, and grouping the terms which belong to different concepts if the seed term has several meanings. We propose a semi-supervised method that integrates manually defined linguistic patterns and automatically learned semi-structural patterns to extract coordinate terms in Chinese from web search results. In addition, terms are grouped into different concepts based on their co-occurring terms and contexts. We further calculate the saliency scores of extracted terms and rank them accordingly. Experimental results demonstrate that our proposed method generates results with high quality and wide coverage.",http://aclweb.org/anthology/D17-1273,D17-1,D17-1273,https://arxiv.org/abs/1507.02145,"('Chengyu Wang', 'Yan Fan', 'Xiaofeng He', 'Aoying Zhou')",9,Learning Fine-grained Relations from Chinese User Generated Categories,EMNLP,2017
,http://aclweb.org/anthology/D17-1274,D17-1,D17-1274,,"('Lifu Huang', 'Avirup Sil', 'Heng Ji', 'Radu Florian')",9,"
      Improving Slot Filling Performance with Attentive Neural Networks on
      Dependency Structures
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1275,D17-1,D17-1275,,"('Greg Durrett', 'Jonathan K. Kummerfeld', 'Taylor Berg-Kirkpatrick', 'Rebecca Portnoff', 'Sadia Afroz', 'Damon McCoy', 'Kirill Levchenko', 'Vern Paxson')",9,"
      Identifying Products in Online Cybercrime Marketplaces: A Dataset for
      Fine-grained Domain Adaptation
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1276,D17-1,D17-1276,,"('Aldrian Obaja Muis', 'Wei Lu')",9,"
      Labeling Gaps Between Words: Recognizing Overlapping Mentions with Mention
      Separators
    ",EMNLP,2017
"We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or state-of-the-art accuracy at moderate computational costs.",http://aclweb.org/anthology/D17-1277,D17-1,D17-1277,https://arxiv.org/abs/1704.04920,"('Octavian-Eugen Ganea', 'Thomas Hofmann')",9,Deep Joint Entity Disambiguation with Local Neural Attention,EMNLP,2017
"Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. Our preliminary study reveals nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from this problem. While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. To avoid generating fake facts in a summary, we leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text. The dual-attention sequence-to-sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions. Experiments on the Gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80%. Notably, the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text.",http://aclweb.org/anthology/D17-1278,D17-1,D17-1278,https://arxiv.org/abs/1711.04434,"('Kiril Gashteovski', 'Rainer Gemulla', 'Luciano Del Corro')",9,MinIE: Minimizing Facts in Open Information Extraction,EMNLP,2017
"This paper addresses the problem of extracting keyphrases from scientific articles and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce semi-supervised methods to a neural tagging model, which builds on recent advances in named entity recognition. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task.",http://aclweb.org/anthology/D17-1279,D17-1,D17-1279,https://arxiv.org/abs/1708.06075,"('Yi Luan', 'Mari Ostendorf', 'Hannaneh Hajishirzi')",9,Scientific Information Extraction with Semi-supervised Neural Tagging,EMNLP,2017
"Most existing methods for biomedical entity recognition task rely on explicit feature engineering where many features either are specific to a particular task or depends on output of other existing NLP tools. Neural architectures have been shown across various domains that efforts for explicit feature design can be reduced. In this work we propose an unified framework using bi-directional long short term memory network (BLSTM) for named entity recognition (NER) tasks in biomedical and clinical domains. Three important characteristics of the framework are as follows - (1) model learns contextual as well as morphological features using two different BLSTM in hierarchy, (2) model uses first order linear conditional random field (CRF) in its output layer in cascade of BLSTM to infer label or tag sequence, (3) model does not use any domain specific features or dictionary, i.e., in another words, same set of features are used in the three NER tasks, namely, disease name recognition (Disease NER), drug name recognition (Drug NER) and clinical entity recognition (Clinical NER). We compare performance of the proposed model with existing state-of-the-art models on the standard benchmark datasets of the three tasks. We show empirically that the proposed framework outperforms all existing models. Further our analysis of CRF layer and word-embedding obtained using character based embedding show their importance.",http://aclweb.org/anthology/D17-1280,D17-1,D17-1280,https://arxiv.org/abs/1708.03447,"('Siliang Tang', 'Ning Zhang', 'Jinjiang Zhang', 'Fei Wu', 'Yueting Zhuang')",9,NITE: A Neural Inductive Teaching Framework for Domain Specific NER,EMNLP,2017
,http://aclweb.org/anthology/D17-1281,D17-1,D17-1281,,"('Aditya Sharma', 'Zarana Parekh', 'Partha Talukdar')",9,"
      Speeding up Reinforcement Learning-based Information Extraction Training
      using Asynchronous Methods
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1282,D17-1,D17-1282,,"('Peng-Hsuan Li', 'Ruo-Ping Dong', 'Yu-Siang Wang', 'Ju-Chieh Chou', 'Wei-Yun Ma')",9,"
      Leveraging Linguistic Structures for Named Entity Recognition with
      Bidirectional Recursive Neural Networks
    ",EMNLP,2017
"Today when many practitioners run basic NLP on the entire web and large-volume traffic, faster methods are paramount to saving time and energy costs. Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining per-token vector representations serving as input to labeling tasks such as NER (often followed by prediction in a linear-chain CRF). Though expressive and accurate, these models fail to fully exploit GPU parallelism, limiting their computational efficiency. This paper proposes a faster alternative to Bi-LSTMs for NER: Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of parallelism, ID-CNNs permit fixed-depth convolutions to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x test-time speedups while retaining accuracy comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8x faster test time speeds.",http://aclweb.org/anthology/D17-1283,D17-1,D17-1283,https://arxiv.org/abs/1702.02098,"('Emma Strubell', 'Patrick Verga', 'David Belanger', 'Andrew McCallum')",9,Fast and Accurate Entity Recognition with Iterated Dilated Convolutions,EMNLP,2017
"Textual information is considered as significant supplement to knowledge representation learning (KRL). There are two main challenges for constructing knowledge representations from plain texts: (1) How to take full advantages of sequential contexts of entities in plain texts for KRL. (2) How to dynamically select those informative sentences of the corresponding entities for KRL. In this paper, we propose the Sequential Text-embodied Knowledge Representation Learning to build knowledge representations from multiple sentences. Given each reference sentence of an entity, we first utilize recurrent neural network with pooling or long short-term memory network to encode the semantic information of the sentence with respect to the entity. Then we further design an attention model to measure the informativeness of each sentence, and build text-based representations of entities. We evaluate our method on two tasks, including triple classification and link prediction. Experimental results demonstrate that our method outperforms other baselines on both tasks, which indicates that our method is capable of selecting informative sentences and encoding the textual information well into knowledge representations.",http://aclweb.org/anthology/D17-1284,D17-1,D17-1284,https://arxiv.org/abs/1609.07075,"('Nitish Gupta', 'Sameer Singh', 'Dan Roth')",9,"Entity Linking via Joint Encoding of Types, Descriptions, and Context",EMNLP,2017
"Factoid question answering (QA) has recently benefited from the development of deep learning (DL) systems. Neural network models outperform traditional approaches in domains where large datasets exist, such as SQuAD (ca. 100,000 questions) for Wikipedia articles. However, these systems have not yet been applied to QA in more specific domains, such as biomedicine, because datasets are generally too small to train a DL system from scratch. For example, the BioASQ dataset for biomedical QA comprises less then 900 factoid (single answer) and list (multiple answers) QA instances. In this work, we adapt a neural QA system trained on a large open-domain dataset (SQuAD, source) to a biomedical dataset (BioASQ, target) by employing various transfer learning techniques. Our network architecture is based on a state-of-the-art QA system, extended with biomedical word embeddings and a novel mechanism to answer list questions. In contrast to existing biomedical QA systems, our system does not rely on domain-specific ontologies, parsers or entity taggers, which are expensive to create. Despite this fact, our systems achieve state-of-the-art results on factoid questions and competitive results on list questions.",http://aclweb.org/anthology/D17-1285,D17-1,D17-1285,https://arxiv.org/abs/1706.03610,"('Hua He', 'Kris Ganjam', 'Navendu Jain', 'Jessica Lundin', 'Ryen White', 'Jimmy Lin')",9,An Insight Extraction System on BioMedical Literature with Deep Neural Networks,EMNLP,2017
"We propose a novel approach to learn word embeddings based on an extended version of the distributional hypothesis. Our model derives word embedding vectors using the etymological composition of words, rather than the context in which they appear. It has the strength of not requiring a large text corpus, but instead it requires reliable access to etymological roots of words, making it specially fit for languages with logographic writing systems. The model consists on three steps: (1) building an etymological graph, which is a bipartite network of words and etymological roots, (2) obtaining the biadjacency matrix of the etymological graph and reducing its dimensionality, (3) using columns/rows of the resulting matrices as embedding vectors. We test our model in the Chinese and Sino-Korean vocabularies. Our graphs are formed by a set of 117,000 Chinese words, and a set of 135,000 Sino-Korean words. In both cases we show that our model performs well in the task of synonym discovery.",http://aclweb.org/anthology/D17-1286,D17-1,D17-1286,https://arxiv.org/abs/1709.10445,"('Vivi Nastase', 'Carlo Strapparava')",9,Word Etymology as Native Language Interference,EMNLP,2017
,http://aclweb.org/anthology/D17-1287,D17-1,D17-1287,,"('Joshua Eisenberg', 'Mark Finlayson')",9,"
      A Simpler and More Generalizable Story Detector using Verb and Character
      Features
    ",EMNLP,2017
"Developing a Bangla OCR requires bunch of algorithm and methods. There were many effort went on for developing a Bangla OCR. But all of them failed to provide an error free Bangla OCR. Each of them has some lacking. We discussed about the problem scope of currently existing Bangla OCR's. In this paper, we present the basic steps required for developing a Bangla OCR and a complete workflow for development of a Bangla OCR with mentioning all the possible algorithms required.",http://aclweb.org/anthology/D17-1288,D17-1,D17-1288,https://arxiv.org/abs/1204.1198,"('Sarah Schulz', 'Jonas Kuhn')",9,Multi-modular domain-tailored OCR post-correction,EMNLP,2017
"The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.",http://aclweb.org/anthology/D17-1289,D17-1,D17-1289,https://arxiv.org/abs/1707.09168,"('Bingfeng Luo', 'Yansong Feng', 'Jianbo Xu', 'Xiang Zhang', 'Dongyan Zhao')",9,Learning to Predict Charges for Criminal Cases with Legal Basis,EMNLP,2017
"One of the important factors that make a search engine fast and accurate is a concise and duplicate free index. In order to remove duplicate and near-duplicate documents from the index, a search engine needs a swift and reliable duplicate and near-duplicate text document detection system. Traditional approaches to this problem, such as brute force comparisons or simple hash-based algorithms are not suitable as they are not scalable and are not capable of detecting near-duplicate documents effectively. In this paper, a new signature-based approach to text similarity detection is introduced which is fast, scalable, reliable and needs less storage space. The proposed method is examined on popular text document data-sets such as CiteseerX, Enron, Gold Set of Near-duplicate News Articles and etc. The results are promising and comparable with the best cutting-edge algorithms, considering the accuracy and performance. The proposed method is based on the idea of using reference texts to generate signatures for text documents. The novelty of this paper is the use of genetic algorithms to generate better reference texts.",http://aclweb.org/anthology/D17-1290,D17-1,D17-1290,https://arxiv.org/abs/1810.03102,"('Alexandra Schofield', 'Laure Thompson', 'David Mimno')",9,Quantifying the Effects of Text Duplication on Semantic Models,EMNLP,2017
"Outlier detection aims to identify unusual data instances that deviate from expected patterns. The outlier detection is particularly challenging when outliers are context dependent and when they are defined by unusual combinations of multiple outcome variable values. In this paper, we develop and study a new conditional outlier detection approach for multivariate outcome spaces that works by (1) transforming the conditional detection to the outlier detection problem in a new (unconditional) space and (2) defining outlier scores by analyzing the data in the new space. Our approach relies on the classifier chain decomposition of the multi-dimensional classification problem that lets us transform the output space into a probability vector, one probability for each dimension of the output space. Outlier scores applied to these transformed vectors are then used to detect the outliers. Experiments on multiple multi-dimensional classification problems with the different outlier injection rates show that our methodology is robust and able to successfully identify outliers when outliers are either sparse (manifested in one or very few dimensions) or dense (affecting multiple dimensions).",http://aclweb.org/anthology/D17-1291,D17-1,D17-1291,https://arxiv.org/abs/1505.04097,"('Honglei Zhuang', 'Chi Wang', 'Fangbo Tao', 'Lance Kaplan', 'Jiawei Han')",9,Identifying Semantically Deviating Outlier Documents,EMNLP,2017
"Explaining underlying causes or effects about events is a challenging but valuable task. We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with textual data and (2) constructing a connecting chain between them to generate an explanation. To detect causal features from text, we propose a novel method based on the Granger causality of time series between features extracted from text such as N-grams, topics, sentiments, and their composition. The generation of the sequence of causal entities requires a commonsense causative knowledge base with efficient reasoning. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.",http://aclweb.org/anthology/D17-1292,D17-1,D17-1292,https://arxiv.org/abs/1707.08852,"('Dongyeop Kang', 'Varun Gangal', 'Ang Lu', 'Zheng Chen', 'Eduard Hovy')",9,Detecting and Explaining Causes From Text For a Time Series Event,EMNLP,2017
,http://aclweb.org/anthology/D17-1293,D17-1,D17-1293,,"('Zhuoxuan Jiang', 'Shanshan Feng', 'Gao Cong', 'Chunyan Miao', 'Xiaoming Li')",9,"
      A Novel Cascade Model for Learning Latent Similarity from Heterogeneous
      Sequential Data of MOOC
    ",EMNLP,2017
"A variety of tools have been introduced recently that are designed to help people protect their privacy on the Internet. These tools perform many different functions in-cluding encrypting and/or anonymizing communications, preventing the use of persistent identifiers such as cookies, automatically fetching and analyzing web site privacy policies, and displaying privacy-related information to users. This paper discusses the set of privacy tools that aim specifically at facilitating notice and choice about Web site data practices. While these tools may also have components that perform other functions such as encryption, or they may be able to work in conjunction with other privacy tools, the primary pur-pose of these tools is to help make users aware of web site privacy practices and to make it easier for users to make informed choices about when to provide data to web sites. Examples of such tools include the Platform for Privacy Preferences (P3P) and various infomediary services.",http://aclweb.org/anthology/D17-1294,D17-1,D17-1294,https://arxiv.org/abs/cs/0001011,"('Kanthashree Mysore Sathyendra', 'Shomir Wilson', 'Florian Schaub', 'Sebastian Zimmeck', 'Norman Sadeh')",9,Identifying the Provision of Choices in Privacy Policy Text,EMNLP,2017
"The purpose of this paper is to present some functionalities of the HyperPro System. HyperPro is a hypertext tool which allows to develop Constraint Logic Programming (CLP) together with their documentation. The text editing part is not new and is based on the free software Thot. A HyperPro program is a Thot document written in a report style. The tool is designed for CLP but it can be adapted to other programming paradigms as well. Thot offers navigation and editing facilities and synchronized static document views. HyperPro has new functionalities such as document exportations, dynamic views (projections), indexes and version management. Projection is a mechanism for extracting and exporting relevant pieces of code program or of document according to specific criteria. Indexes are useful to find the references and occurrences of a relation in a document, i.e., where its predicate definition is found and where a relation is used in other programs or document versions and, to translate hyper-texts links into paper references. It still lack importation facilities.",http://aclweb.org/anthology/D17-1295,D17-1,D17-1295,https://arxiv.org/abs/cs/0111046,"('Tanya Goyal', 'Sachin Kelkar', 'Manas Agarwal', 'Jeenu Grover')",9,An Empirical Analysis of Edit Importance between Document Versions,EMNLP,2017
"This paper presents a model for disfluency detection in spontaneous speech transcripts called LSTM Noisy Channel Model. The model uses a Noisy Channel Model (NCM) to generate n-best candidate disfluency analyses and a Long Short-Term Memory (LSTM) language model to score the underlying fluent sentences of each analysis. The LSTM language model scores, along with other features, are used in a MaxEnt reranker to identify the most plausible analysis. We show that using an LSTM language model in the reranking process of noisy channel disfluency model improves the state-of-the-art in disfluency detection.",http://aclweb.org/anthology/D17-1296,D17-1,D17-1296,https://arxiv.org/abs/1808.09091,"('Shaolei Wang', 'Wanxiang Che', 'Yue Zhang', 'Meishan Zhang', 'Ting Liu')",9,Transition-Based Disfluency Detection using LSTMs,EMNLP,2017
"We propose a neural encoder-decoder model with reinforcement learning (NRL) for grammatical error correction (GEC). Unlike conventional maximum likelihood estimation (MLE), the model directly optimizes towards an objective that considers a sentence-level, task-specific evaluation metric, avoiding the exposure bias issue in MLE. We demonstrate that NRL outperforms MLE both in human and automated evaluation metrics, achieving the state-of-the-art on a fluency-oriented GEC corpus.",http://aclweb.org/anthology/D17-1297,D17-1,D17-1297,https://arxiv.org/abs/1707.00299,"('Helen Yannakoudakis', 'Marek Rei', 'Øistein E. Andersen', 'Zheng Yuan')",9,Neural Sequence-Labelling Models for Grammatical Error Correction,EMNLP,2017
"In a controlled experiment of sequence-to-sequence approaches for the task of sentence correction, we find that character-based models are generally more effective than word-based models and models that encode subword information via convolutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches. Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.",http://aclweb.org/anthology/D17-1298,D17-1,D17-1298,https://arxiv.org/abs/1707.09067,"('Allen Schmaltz', 'Yoon Kim', 'Alexander Rush', 'Stuart Shieber')",9,Adapting Sequence Models for Sentence Correction,EMNLP,2017
,http://aclweb.org/anthology/D17-1299,D17-1,D17-1299,,"('Xing Niu', 'Marianna Martindale', 'Marine Carpuat')",9,"
      A Study of Style in Machine Translation: Controlling the Formality of
      Machine Translation Output
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1300,D17-1,D17-1300,,['Jacob Devlin'],9,"
      Sharp Models on Dull Hardware: Fast and Accurate Neural Machine
      Translation Decoding on the CPU
    ",EMNLP,2017
"For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models' ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50% accuracy on our coreference test set and 53.5% for coherence/cohesion (compared to a non-contextual baseline of 50%). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5% for coreference and 57% for coherence/cohesion), highlighting the importance of target-side context.",http://aclweb.org/anthology/D17-1301,D17-1,D17-1301,https://arxiv.org/abs/1711.00513,"('Longyue Wang', 'Zhaopeng Tu', 'Andy Way', 'Qun Liu')",9,Exploiting Cross-Sentence Context for Neural Machine Translation,EMNLP,2017
"A major challenge in Entity Linking (EL) is making effective use of contextual information to disambiguate mentions to Wikipedia that might refer to different entities in different contexts. The problem exacerbates with cross-lingual EL which involves linking mentions written in non-English documents to entries in the English Wikipedia: to compare textual clues across languages we need to compute similarity between textual fragments across languages. In this paper, we propose a neural EL model that trains fine-grained similarities and dissimilarities between the query and candidate document from multiple perspectives, combined with convolution and tensor networks. Further, we show that this English-trained system can be applied, in zero-shot learning, to other languages by making surprisingly effective use of multi-lingual embeddings. The proposed system has strong empirical evidence yielding state-of-the-art results in English as well as cross-lingual: Spanish and Chinese TAC 2015 datasets.",http://aclweb.org/anthology/D17-1302,D17-1,D17-1302,https://arxiv.org/abs/1712.01813,"('Joo-Kyung Kim', 'Young-Bum Kim', 'Ruhi Sarikaya', 'Eric Fosler-Lussier')",9,Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources,EMNLP,2017
"In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of image search and image understanding. Our model learns a common representation for images and their descriptions in two different languages (which need not be parallel) by considering the image as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance.",http://aclweb.org/anthology/D17-1303,D17-1,D17-1303,https://arxiv.org/abs/1707.07601,"('Spandana Gella', 'Rico Sennrich', 'Frank Keller', 'Mirella Lapata')",9,Image Pivoting for Learning Multilingual Multimodal Representations,EMNLP,2017
"We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.",http://aclweb.org/anthology/D17-1304,D17-1,D17-1304,https://arxiv.org/abs/1607.00578,"('Kehai Chen', 'Rui Wang', 'Masao Utiyama', 'Lemao Liu', 'Akihiro Tamura', 'Eiichiro Sumita', 'Tiejun Zhao')",9,Neural Machine Translation with Source Dependency Representation,EMNLP,2017
"Capturing semantic relations between sentences, such as entailment, is a long-standing challenge for computational semantics. Logic-based models analyse entailment in terms of possible worlds (interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether models can perform better if, in addition to P and H, there is also an image (corresponding to the relevant ""world"" or ""situation""). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare ""blind"" and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing ""grounding"" in an optimal fashion.",http://aclweb.org/anthology/D17-1305,D17-1,D17-1305,https://arxiv.org/abs/1806.05645,"('Dan Han', 'Pascual Martínez-Gómez', 'Koji Mineshima')",9,Visual Denotations for Recognizing Textual Entailment,EMNLP,2017
"Over the last few years, deep learning has revolutionized the field of machine learning by dramatically improving the state-of-the-art in various domains. However, as the size of supervised artificial neural networks grows, typically so does the need for larger labeled datasets. Recently, crowdsourcing has established itself as an efficient and cost-effective solution for labeling large sets of data in a scalable manner, but it often requires aggregating labels from multiple noisy contributors with different levels of expertise. In this paper, we address the problem of learning deep neural networks from crowds. We begin by describing an EM algorithm for jointly learning the parameters of the network and the reliabilities of the annotators. Then, a novel general-purpose crowd layer is proposed, which allows us to train deep neural networks end-to-end, directly from the noisy labels of multiple annotators, using only backpropagation. We empirically show that the proposed approach is able to internally capture the reliability and biases of different annotators and achieve new state-of-the-art results for various crowdsourced datasets across different settings, namely classification, regression and sequence labeling.",http://aclweb.org/anthology/D17-1306,D17-1,D17-1306,https://arxiv.org/abs/1709.01779,"('Nitika Mathur', 'Timothy Baldwin', 'Trevor Cohn')",9,Sequence Effects in Crowdsourced Annotations,EMNLP,2017
,http://aclweb.org/anthology/D17-1307,D17-1,D17-1307,,"('Ferhan Ture', 'Oliver Jojic')",9,No Need to Pay Attention: Simple Recurrent Neural Networks Work!,EMNLP,2017
"By means of the quark mass density- and temperature- dependent model, it is found that the negative charge and the higher strangeness fraction are in favor of the stability of strange quark matter at finite temperature. A critical baryon number density n_Bc has been found. The strange quark matter is unstable when n_B < n_Bc. The charge and strangeness fraction dependence of stability for strange quark matter is addressed.",http://aclweb.org/anthology/D17-1308,D17-1,D17-1308,https://arxiv.org/abs/nucl-th/0203007,"('David Mimno', 'Laure Thompson')",9,The strange geometry of skip-gram with negative sampling,EMNLP,2017
"Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.",http://aclweb.org/anthology/D17-1309,D17-1,D17-1309,https://arxiv.org/abs/1510.00726,"('Jan A. Botha', 'Emily Pitler', 'Ji Ma', 'Anton Bakalov', 'Alex Salcianu', 'David Weiss', 'Ryan McDonald', 'Slav Petrov')",9,Natural Language Processing with Small Feed-Forward Networks,EMNLP,2017
"The outstanding pattern recognition performance of deep learning brings new vitality to the synthetic aperture radar (SAR) automatic target recognition (ATR). However, there is a limitation in current deep learning based ATR solution that each learning process only handle one SAR image, namely learning the static scattering information, while missing the space-varying information. It is obvious that multi-aspect joint recognition introduced space-varying scattering information should improve the classification accuracy and robustness. In this paper, a novel multi-aspect-aware method is proposed to achieve this idea through the bidirectional Long Short-Term Memory (LSTM) recurrent neural networks based space-varying scattering information learning. Specifically, we first select different aspect images to generate the multi-aspect space-varying image sequences. Then, the Gabor filter and three-patch local binary pattern (TPLBP) are progressively implemented to extract a comprehensive spatial features, followed by dimensionality reduction with the Multi-layer Perceptron (MLP) network. Finally, we design a bidirectional LSTM recurrent neural network to learn the multi-aspect features with further integrating the softmax classifier to achieve target recognition. Experimental results demonstrate that the proposed method can achieve 99.9% accuracy for 10-class recognition. Besides, its anti-noise and anti-confusion performance are also better than the conventional deep learning based methods.",http://aclweb.org/anthology/D17-1310,D17-1,D17-1310,https://arxiv.org/abs/1707.09875,"('Xin Li', 'Wai Lam')",9,Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction,EMNLP,2017
"We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a ""syntax"" with functional analogues to qualitative properties of natural language.",http://aclweb.org/anthology/D17-1311,D17-1,D17-1311,https://arxiv.org/abs/1707.08139,"('Jacob Andreas', 'Dan Klein')",9,Analogs of Linguistic Structure in Deep Representations,EMNLP,2017
,http://aclweb.org/anthology/D17-1312,D17-1,D17-1312,,"('Wei Yang', 'Wei Lu', 'Vincent Zheng')",9,"
      A Simple Regularization-based Algorithm for Learning Cross-Domain Word
      Embeddings
    ",EMNLP,2017
"Recent efforts in bioinformatics have achieved tremendous progress in the machine reading of biomedical literature, and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways. However, batch machine reading of literature at today's scale (PubMed alone indexes over 1 million papers per year) is unfeasible due to both cost and processing overhead. In this work, we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible. We introduce a family of algorithms for focused reading, including an intuitive, strong baseline, and a second approach which uses a reinforcement learning (RL) framework that learns when to explore (widen the search) or exploit (narrow it). We demonstrate that the RL approach is capable of answering more queries than the baseline, while being more efficient, i.e., reading fewer documents.",http://aclweb.org/anthology/D17-1313,D17-1,D17-1313,https://arxiv.org/abs/1709.00149,"('Enrique Noriega-Atala', 'Marco A. Valenzuela-Escárcega', 'Clayton Morrison', 'Mihai Surdeanu')",9,Learning what to read: Focused machine reading,EMNLP,2017
"Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training. This also applies to text learning or text classification. As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem. This problem is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.",http://aclweb.org/anthology/D17-1314,D17-1,D17-1314,https://arxiv.org/abs/1709.08716,"('Lei Shu', 'Hu Xu', 'Bing Liu')",9,DOC: Deep Open Classification of Text Documents,EMNLP,2017
"Portmanteaus are a word formation phenomenon where two words are combined to form a new word. We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model. This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.",http://aclweb.org/anthology/D17-1315,D17-1,D17-1315,https://arxiv.org/abs/1707.01176,"('Varun Gangal', 'Harsh Jhamtani', 'Graham Neubig', 'Eduard Hovy', 'Eric Nyberg')",9,Charmanteau: Character Embedding Models For Portmanteau Creation,EMNLP,2017
,http://aclweb.org/anthology/D17-1316,D17-1,D17-1316,,"('E. Dario Gutierrez', 'Guillermo Cecchi', 'Cheryl Corcoran', 'Philip Corlett')",9,"
      Using Automated Metaphor Identification to Aid in Detection and Prediction
      of First-Episode Schizophrenia
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1317,D17-1,D17-1317,,"('Hannah Rashkin', 'Eunsol Choi', 'Jin Yea Jang', 'Svitlana Volkova', 'Yejin Choi')",9,"
      Truth of Varying Shades: Analyzing Language in Fake News and Political
      Fact-Checking
    ",EMNLP,2017
"In the United States electoral system, a candidate is elected indirectly by winning a majority of electoral votes cast by individual states, the election usually being decided by the votes cast by a small number of ""swing states"" where the two candidates historically have roughly equal probabilities of winning. The effective value of a swing state in deciding the election is determined not only by the number of its electoral votes but by the frequency of its appearance in the set of winning partitions of the electoral college. Since the electoral vote values of swing states are not identical, the presence or absence of a state in a winning partition is generally correlated with the frequency of appearance of other states and, hence, their effective values. We quantify the effective value of states by an {\sl electoral susceptibility}, $\chi_j$, the variation of the winning probability with the ""cost"" of changing the probability of winning state $j$. We study $\chi_j$ for realistic data accumulated for the 2012 U.S. presidential election and for a simple model with a Zipf's law type distribution of electoral votes. In the latter model we show that the susceptibility for small states is largest in ""one-sided"" electoral contests and smallest in close contests. We draw an analogy to models of entropically driven interactions in poly-disperse colloidal solutions.",http://aclweb.org/anthology/D17-1318,D17-1,D17-1318,https://arxiv.org/abs/1211.0656,"('Stefano Menini', 'Federico Nanni', 'Simone Paolo Ponzetto', 'Sara Tonelli')",9,Topic-Based Agreement and Disagreement in US Electoral Manifestos,EMNLP,2017
,http://aclweb.org/anthology/D17-1319,D17-1,D17-1319,,"('Hainan Xu', 'Philipp Koehn')",9,"
      Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled
      Parallel Corpora
    ",EMNLP,2017
,http://aclweb.org/anthology/D17-1320,D17-1,D17-1320,,"('Tobias Falke', 'Iryna Gurevych')",9,"
      Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of
      Concept Maps
    ",EMNLP,2017
"A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision!   In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional.   In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",http://aclweb.org/anthology/D17-1321,D17-1,D17-1321,https://arxiv.org/abs/1706.08502,"('Satwik Kottur', 'José Moura', 'Stefan Lee', 'Dhruv Batra')",9,Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog,EMNLP,2017
"Users suffering from mental health conditions often turn to online resources for support, including specialized online support communities or general communities such as Twitter and Reddit. In this work, we present a neural framework for supporting and studying users in both types of communities. We propose methods for identifying posts in support communities that may indicate a risk of self-harm, and demonstrate that our approach outperforms strong previously proposed methods for identifying such posts. Self-harm is closely related to depression, which makes identifying depressed users on general forums a crucial related task. We introduce a large-scale general forum dataset (""RSDD"") consisting of users with self-reported depression diagnoses matched with control users. We show how our method can be applied to effectively identify depressed users from their use of language alone. We demonstrate that our method outperforms strong baselines on this general forum dataset.",http://aclweb.org/anthology/D17-1322,D17-1,D17-1322,https://arxiv.org/abs/1709.01848,"('Andrew Yates', 'Arman Cohan', 'Nazli Goharian')",9,Depression and Self-Harm Risk Assessment in Online Forums,EMNLP,2017
,http://aclweb.org/anthology/D17-1323,D17-1,D17-1323,,"('Jieyu Zhao', 'Tianlu Wang', 'Mark Yatskar', 'Vicente Ordonez', 'Kai-Wei Chang')",9,"
      Men Also Like Shopping: Reducing Gender Bias Amplification using
      Corpus-level Constraints
    ",EMNLP,2017
