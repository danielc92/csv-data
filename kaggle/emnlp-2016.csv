abstract,acl_url,anthology,anthology_id,arxiv_url,authors,month,title,venue,year
,http://aclweb.org/anthology/D16-1000,D16-1,D16-1000,,"('Jian Su', 'Kevin Duh', 'Xavier Carreras')",11,"
      Proceedings of the 2016 Conference on Empirical Methods in Natural
      Language Processing
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1001,D16-1,D16-1001,,"('James Cross', 'Liang Huang')",11,"
      Span-Based Constituency Parsing with a Structure-Label System and Provably
      Optimal Dynamic Oracles
    ",EMNLP,2016
"Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the corresponding minimization, and point out interesting linguistic side-effects of this operation.",http://aclweb.org/anthology/D16-1002,D16-1,D16-1002,https://arxiv.org/abs/cmp-lg/9407003,"('Pascual Martínez-Gómez', 'Yusuke Miyao')",11,Rule Extraction for Tree-to-Tree Transducers by Cost Minimization,EMNLP,2016
"We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus.",http://aclweb.org/anthology/D16-1003,D16-1,D16-1003,https://arxiv.org/abs/1610.03946,"('Jessica Ficler', 'Yoav Goldberg')",11,A Neural Network for Coordination Boundary Prediction,EMNLP,2016
,http://aclweb.org/anthology/D16-1004,D16-1,D16-1004,,"('Hiroshi Noji', 'Yusuke Miyao', 'Mark Johnson')",11,"
      Using Left-corner Parsing to Encode Universal Structural Constraints in
      Grammar Induction
    ",EMNLP,2016
"The tremendous amount of user generated data through social networking sites led to the gaining popularity of automatic text classification in the field of computational linguistics over the past decade. Within this domain, one problem that has drawn the attention of many researchers is automatic humor detection in texts. In depth semantic understanding of the text is required to detect humor which makes the problem difficult to automate. With increase in the number of social media users, many multilingual speakers often interchange between languages while posting on social media which is called code-mixing. It introduces some challenges in the field of linguistic analysis of social media content (Barman et al., 2014), like spelling variations and non-grammatical structures in a sentence. Past researches include detecting puns in texts (Kao et al., 2016) and humor in one-lines (Mihalcea et al., 2010) in a single language, but with the tremendous amount of code-mixed data available online, there is a need to develop techniques which detects humor in code-mixed tweets. In this paper, we analyze the task of humor detection in texts and describe a freely available corpus containing English-Hindi code-mixed tweets annotated with humorous(H) or non-humorous(N) tags. We also tagged the words in the tweets with Language tags (English/Hindi/Others). Moreover, we describe the experiments carried out on the corpus and provide a baseline classification system which distinguishes between humorous and non-humorous texts.",http://aclweb.org/anthology/D16-1005,D16-1,D16-1005,https://arxiv.org/abs/1806.05513,"('Ruihong Huang', 'Ignacio Cases', 'Dan Jurafsky', 'Cleo Condoravdi', 'Ellen Riloff')",11,"Distinguishing Past, On-going, and Future Events: The EventStatus Corpus",EMNLP,2016
"We introduce Graphene, an Open IE system whose goal is to generate accurate, meaningful and complete propositions that may facilitate a variety of downstream semantic applications. For this purpose, we transform syntactically complex input sentences into clean, compact structures in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them in order to maintain their semantic relationship. In that way, we preserve the context of the relational tuples extracted from a source sentence, generating a novel lightweight semantic representation for Open IE that enhances the expressiveness of the extracted propositions.",http://aclweb.org/anthology/D16-1006,D16-1,D16-1006,https://arxiv.org/abs/1808.09463,"('Nikita Bhutani', 'H V Jagadish', 'Dragomir Radev')",11,Nested Propositions in Open Information Extraction,EMNLP,2016
,http://aclweb.org/anthology/D16-1007,D16-1,D16-1007,,"('Yunlun Yang', 'Yunhai Tong', 'Shulei Ma', 'Zhi-Hong Deng')",11,"
      A Position Encoding Convolutional Neural Network Based on Dependency Tree
      for Relation Classification
    ",EMNLP,2016
"This paper focuses on the study of recognizing discontiguous entities. Motivated by a previous work, we propose to use a novel hypergraph representation to jointly encode discontiguous entities of unbounded length, which can overlap with one another. To compare with existing approaches, we first formally introduce the notion of model ambiguity, which defines the difficulty level of interpreting the outputs of a model, and then formally analyze the theoretical advantages of our model over previous existing approaches based on linear-chain CRFs. Our empirical results also show that our model is able to achieve significantly better results when evaluated on standard data with many discontiguous entities.",http://aclweb.org/anthology/D16-1008,D16-1,D16-1008,https://arxiv.org/abs/1810.08579,"('Aldrian Obaja Muis', 'Wei Lu')",11,Learning to Recognize Discontiguous Entities,EMNLP,2016
"When humans read text, they fixate some words and skip others. However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g.,~using surprisal). In this paper, we propose a novel approach that models both skipping and reading, using an unsupervised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning. Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading.",http://aclweb.org/anthology/D16-1009,D16-1,D16-1009,https://arxiv.org/abs/1608.05604,"('Michael Hahn', 'Frank Keller')",11,Modeling Human Reading with Neural Attention,EMNLP,2016
,http://aclweb.org/anthology/D16-1010,D16-1,D16-1010,,"('Libby Barak', 'Adele E. Goldberg', 'Suzanne Stevenson')",11,"
      Comparing Computational Cognitive Models of Generalization in a Language
      Acquisition Task
    ",EMNLP,2016
"Deviations from rational decision-making due to limited computational resources have been studied in the field of bounded rationality, originally proposed by Herbert Simon. There have been a number of different approaches to model bounded rationality ranging from optimality principles to heuristics. Here we take an information-theoretic approach to bounded rationality, where information-processing costs are measured by the relative entropy between a posterior decision strategy and a given fixed prior strategy. In the case of multiple environments, it can be shown that there is an optimal prior rendering the bounded rationality problem equivalent to the rate distortion problem for lossy compression in information theory. Accordingly, the optimal prior and posterior strategies can be computed by the well-known Blahut-Arimoto algorithm which requires the computation of partition sums over all possible outcomes and cannot be applied straightforwardly to continuous problems. Here we derive a sampling-based alternative update rule for the adaptation of prior behaviors of decision-makers and we show convergence to the optimal prior predicted by rate distortion theory. Importantly, the update rule avoids typical infeasible operations such as the computation of partition sums. We show in simulations a proof of concept for discrete action and environment domains. This approach is not only interesting as a generic computational method, but might also provide a more realistic model of human decision-making processes occurring on a fast and a slow time scale.",http://aclweb.org/anthology/D16-1011,D16-1,D16-1011,https://arxiv.org/abs/1511.01710,"('Tao Lei', 'Regina Barzilay', 'Tommi Jaakkola')",11,Rationalizing Neural Predictions,EMNLP,2016
"We consider the task of identifying attitudes towards a given set of entities from text. Conventionally, this task is decomposed into two separate subtasks: target detection that identifies whether each entity is mentioned in the text, either explicitly or implicitly, and polarity classification that classifies the exact sentiment towards an identified entity (the target) into positive, negative, or neutral.   Instead, we show that attitude identification can be solved with an end-to-end machine learning architecture, in which the two subtasks are interleaved by a deep memory network. In this way, signals produced in target detection provide clues for polarity classification, and reversely, the predicted polarity provides feedback to the identification of targets. Moreover, the treatments for the set of targets also influence each other -- the learned representations may share the same semantics for some targets but vary for others. The proposed deep memory network, the AttNet, outperforms methods that do not consider the interactions between the subtasks or those among the targets, including conventional machine learning methods and the state-of-the-art deep learning models.",http://aclweb.org/anthology/D16-1012,D16-1,D16-1012,https://arxiv.org/abs/1701.04189,"('Pengfei Liu', 'Xipeng Qiu', 'Xuanjing Huang')",11,Deep Multi-Task Learning with Shared Memory for Text Classification,EMNLP,2016
"We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children's Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.",http://aclweb.org/anthology/D16-1013,D16-1,D16-1013,https://arxiv.org/abs/1606.02270,"('Adam Trischler', 'Zheng Ye', 'Xingdi Yuan', 'Philip Bachman', 'Alessandro Sordoni', 'Kaheer Suleman')",11,Natural Language Comprehension with the EpiReader,EMNLP,2016
"A common model for question answering (QA) is that a good answer is one that is closely related to the question, where relatedness is often determined using general-purpose lexical models such as word embeddings. We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which may be determined through task-specific embeddings. With causality as a use case, we implement this insight in three steps. First, we generate causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns. Second, we train dedicated embeddings over this data, by using task-specific contexts, i.e., the context of a cause is its effect. Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embeddings. We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo! Answers. We show that explicitly modeling causality improves performance in both tasks. In the QA task our best model achieves 37.3% P@1, significantly outperforming a strong baseline by 7.7% (relative).",http://aclweb.org/anthology/D16-1014,D16-1,D16-1014,https://arxiv.org/abs/1609.08097,"('Rebecca Sharp', 'Mihai Surdeanu', 'Peter Jansen', 'Peter Clark', 'Michael Hammond')",11,Creating Causal Embeddings for Question Answering with Minimal Supervision,EMNLP,2016
"Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a neural network based relation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Experiments on the WebQuestions question answering dataset show that our method achieves an F_1 of 53.3%, a substantial improvement over the state-of-the-art.",http://aclweb.org/anthology/D16-1015,D16-1,D16-1015,https://arxiv.org/abs/1603.00957,"('Semih Yavuz', 'Izzeddin Gur', 'Yu Su', 'Mudhakar Srivatsa', 'Xifeng Yan')",11,Improving Semantic Parsing via Answer Type Inference,EMNLP,2016
"Situated question answering is the problem of answering questions about an environment such as an image or diagram. This problem requires jointly interpreting a question and an environment using background knowledge to select the correct answer. We present Parsing to Probabilistic Programs (P3), a novel situated question answering model that can use background knowledge and global features of the question/environment interpretation while retaining efficient approximate inference. Our key insight is to treat semantic parses as probabilistic programs that execute nondeterministically and whose possible executions represent environmental uncertainty. We evaluate our approach on a new, publicly-released data set of 5000 science diagram questions, outperforming several competitive classical and neural baselines.",http://aclweb.org/anthology/D16-1016,D16-1,D16-1016,https://arxiv.org/abs/1606.07046,"('Jayant Krishnamurthy', 'Oyvind Tafjord', 'Aniruddha Kembhavi')",11,Semantic Parsing to Probabilistic Programs for Situated Question Answering,EMNLP,2016
"It is approved that artificial neural networks can be considerable effective in anticipating and analyzing flows in which traditional methods and statics are not able to solve. in this article, by using two-layer feedforward network with tan-sigmoid transmission function in input and output layers, we can anticipate participation rate of public in kohgiloye and boyerahmad province in future presidential election of islamic republic of iran with 91% accuracy. the assessment standards of participation such as confusion matrix and roc diagrams have been approved our claims.",http://aclweb.org/anthology/D16-1017,D16-1,D16-1017,https://arxiv.org/abs/1309.2183,"('Ottokar Tilk', 'Vera Demberg', 'Asad Sayeed', 'Dietrich Klakow', 'Stefan Thater')",11,Event participant modelling with neural networks,EMNLP,2016
"Word embeddings play a significant role in many modern NLP systems. Since learning one representation per word is problematic for polysemous words and homonymous words, researchers propose to use one embedding per word sense. Their approaches mainly train word sense embeddings on a corpus. In this paper, we propose to use word sense definitions to learn one embedding per word sense. Experimental results on word similarity tasks and a word sense disambiguation task show that word sense embeddings produced by our approach are of high quality.",http://aclweb.org/anthology/D16-1018,D16-1,D16-1018,https://arxiv.org/abs/1606.04835,"('Lin Qiu', 'Kewei Tu', 'Yong Yu')",11,Context-Dependent Sense Embedding,EMNLP,2016
"Representation learning of knowledge graphs encodes entities and relation types into a continuous low-dimensional vector space, learns embeddings of entities and relation types. Most existing methods only concentrate on knowledge triples, ignoring logic rules which contain rich background knowledge. Although there has been some work aiming at leveraging both knowledge triples and logic rules, they ignore the transitivity and antisymmetry of logic rules. In this paper, we propose a novel approach to learn knowledge representations with entities and ordered relations in knowledges and logic rules. The key idea is to integrate knowledge triples and logic rules, and approximately order the relation types in logic rules to utilize the transitivity and antisymmetry of logic rules. All entries of the embeddings of relation types are constrained to be non-negative. We translate the general constrained optimization problem into an unconstrained optimization problem to solve the non-negative matrix factorization. Experimental results show that our model significantly outperforms other baselines on knowledge graph completion task. It indicates that our model is capable of capturing the transitivity and antisymmetry information, which is significant when learning embeddings of knowledge graphs.",http://aclweb.org/anthology/D16-1019,D16-1,D16-1019,https://arxiv.org/abs/1702.07543,"('Shu Guo', 'Quan Wang', 'Lihong Wang', 'Bin Wang', 'Li Guo')",11,Jointly Embedding Knowledge Graphs and Logical Rules,EMNLP,2016
,http://aclweb.org/anthology/D16-1020,D16-1,D16-1020,,"('Chloé Braud', 'Pascal Denis')",11,"
      Learning Connective-based Word Representations for Implicit Discourse
      Relation Identification
    ",EMNLP,2016
"We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.",http://aclweb.org/anthology/D16-1021,D16-1,D16-1021,https://arxiv.org/abs/1605.08900,"('Duyu Tang', 'Bing Qin', 'Ting Liu')",11,Aspect Level Sentiment Classification with Deep Memory Network,EMNLP,2016
,http://aclweb.org/anthology/D16-1022,D16-1,D16-1022,,"('Lei Shu', 'Bing Liu', 'Hu Xu', 'Annice Kim')",11,"
      Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and
      Aspects in Opinion Targets
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1023,D16-1,D16-1023,,"('Jianfei Yu', 'Jing Jiang')",11,"
      Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain
      Sentiment Classification
    ",EMNLP,2016
"With the development of the Internet, natural language processing (NLP), in which sentiment analysis is an important task, became vital in information processing.Sentiment analysis includes aspect sentiment classification. Aspect sentiment can provide complete and in-depth results with increased attention on aspect-level. Different context words in a sentence influence the sentiment polarity of a sentence variably, and polarity varies based on the different aspects in a sentence. Take the sentence, 'I bought a new camera. The picture quality is amazing but the battery life is too short.'as an example. If the aspect is picture quality, then the expected sentiment polarity is 'positive', if the battery life aspect is considered, then the sentiment polarity should be 'negative'; therefore, aspect is important to consider when we explore aspect sentiment in the sentence. Recurrent neural network (RNN) is regarded as a good model to deal with natural language processing, and RNNs has get good performance on aspect sentiment classification including Target-Dependent LSTM (TD-LSTM) ,Target-Connection LSTM (TC-LSTM) (Tang, 2015a, b), AE-LSTM, AT-LSTM, AEAT-LSTM (Wang et al., 2016).There are also extensive literatures on sentiment classification utilizing convolutional neural network, but there is little literature on aspect sentiment classification using convolutional neural network. In our paper, we develop attention-based input layers in which aspect information is considered by input layer. We then incorporate attention-based input layers into convolutional neural network (CNN) to introduce context words information. In our experiment, incorporating aspect information into CNN improves the latter's aspect sentiment classification performance without using syntactic parser or external sentiment lexicons in a benchmark dataset from Twitter but get better performance compared with other models.",http://aclweb.org/anthology/D16-1024,D16-1,D16-1024,https://arxiv.org/abs/1807.01704,"('Xinjie Zhou', 'Xiaojun Wan', 'Jianguo Xiao')",11,Attention-based LSTM Network for Cross-Lingual Sentiment Classification,EMNLP,2016
"Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models -- such as the reordering of verbs -- while pointing out other aspects that remain to be improved.",http://aclweb.org/anthology/D16-1025,D16-1,D16-1025,https://arxiv.org/abs/1608.04631,"('Luisa Bentivogli', 'Arianna Bisazza', 'Mauro Cettolo', 'Marcello Federico')",11,Neural versus Phrase-Based Machine Translation Quality: a Case Study,EMNLP,2016
"There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.",http://aclweb.org/anthology/D16-1026,D16-1,D16-1026,https://arxiv.org/abs/1702.03525,"('Orhan Firat', 'Baskaran Sankaran', 'Yaser Al-Onaizan', 'Fatos T. Yarman Vural', 'Kyunghyun Cho')",11,Zero-Resource Translation with Multi-Lingual Neural Machine Translation,EMNLP,2016
"Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.",http://aclweb.org/anthology/D16-1027,D16-1,D16-1027,https://arxiv.org/abs/1702.02429,"('Mingxuan Wang', 'Zhengdong Lu', 'Hang Li', 'Qun Liu')",11,Memory-enhanced Decoder for Neural Machine Translation,EMNLP,2016
"We propose a method of moments (MoM) algorithm for training large-scale implicit generative models. Moment estimation in this setting encounters two problems: it is often difficult to define the millions of moments needed to learn the model parameters, and it is hard to determine which properties are useful when specifying moments. To address the first issue, we introduce a moment network, and define the moments as the network's hidden units and the gradient of the network's output with the respect to its parameters. To tackle the second problem, we use asymptotic theory to highlight desiderata for moments -- namely they should minimize the asymptotic variance of estimated model parameters -- and introduce an objective to learn better moments. The sequence of objectives created by this Method of Learned Moments (MoLM) can train high-quality neural image samplers. On CIFAR-10, we demonstrate that MoLM-trained generators achieve significantly higher Inception Scores and lower Frechet Inception Distances than those trained with gradient penalty-regularized and spectrally-normalized adversarial objectives. These generators also achieve nearly perfect Multi-Scale Structural Similarity Scores on CelebA, and can create high-quality samples of 128x128 images.",http://aclweb.org/anthology/D16-1028,D16-1,D16-1028,https://arxiv.org/abs/1806.11006,"('Zita Marinho', 'André F. T. Martins', 'Shay B. Cohen', 'Noah A. Smith')",11,Semi-Supervised Learning of Sequence Models with Method of Moments,EMNLP,2016
,http://aclweb.org/anthology/D16-1029,D16-1,D16-1029,,"('Shyam Upadhyay', 'Ming-Wei Chang', 'Kai-Wei Chang', 'Wen-tau Yih')",11,"
      Learning from Explicit and Implicit Supervision Jointly For Algebra Word
      Problems
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1030,D16-1,D16-1030,,"('Jeniya Tabassum', 'Alan Ritter', 'Wei Xu')",11,"
      TweeTime : A Minimally Supervised Method for Recognizing and Normalizing
      Time Expressions in Twitter
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1031,D16-1,D16-1031,,"('Yishu Miao', 'Phil Blunsom')",11,"
      Language as a Latent Variable: Discrete Generative Models for Sentence
      Compression
    ",EMNLP,2016
"Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on measuring individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) in narrow domains.   In this paper, we describe domain-independent neural models of discourse coherence that are capable of measuring multiple aspects of coherence in existing sentences and can maintain coherence while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latent-variable Markovian generative model that captures the latent discourse dependencies between sentences in a text.   Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts.",http://aclweb.org/anthology/D16-1032,D16-1,D16-1032,https://arxiv.org/abs/1606.01545,"('Chloé Kiddon', 'Luke Zettlemoyer', 'Yejin Choi')",11,Globally Coherent Text Generation with Neural Checklist Models,EMNLP,2016
,http://aclweb.org/anthology/D16-1033,D16-1,D16-1033,,"('Kristina Toutanova', 'Chris Brockett', 'Ke M. Tran', 'Saleema Amershi')",11,"
      A Dataset and Evaluation Metrics for Abstractive Compression of Sentences
      and Short Paragraphs
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1034,D16-1,D16-1034,,"('Dominique Brunato', 'Andrea Cimino', ""Felice Dell'Orletta"", 'Giulia Venturi')",11,"
      PaCCSS-IT: A Parallel Corpus of Complex-Simple Sentences for Automatic
      Text Simplification
    ",EMNLP,2016
"This paper describes the Georgia Tech team's approach to the CoNLL-2016 supplementary evaluation on discourse relation sense classification. We use long short-term memories (LSTM) to induce distributed representations of each argument, and then combine these representations with surface features in a neural network. The architecture of the neural network is determined by Bayesian hyperparameter search.",http://aclweb.org/anthology/D16-1035,D16-1,D16-1035,https://arxiv.org/abs/1606.04503,"('Qi Li', 'Tianshi Li', 'Baobao Chang')",11,Discourse Parsing with Attention-based Hierarchical Neural Networks,EMNLP,2016
"Conversational agents are exploding in popularity. However, much work remains in the area of social conversation as well as free-form conversation over a broad range of domains and topics. To advance the state of the art in conversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar university competition where sixteen selected university teams were challenged to build conversational agents, known as socialbots, to converse coherently and engagingly with humans on popular topics such as Sports, Politics, Entertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers the academic community a unique opportunity to perform research with a live system used by millions of users. The competition provided university teams with real user conversational data at scale, along with the user-provided ratings and feedback augmented with annotations by the Alexa team. This enabled teams to effectively iterate and make improvements throughout the competition while being evaluated in real-time through live user interactions. To build their socialbots, university teams combined state-of-the-art techniques with novel strategies in the areas of Natural Language Understanding, Context Modeling, Dialog Management, Response Generation, and Knowledge Acquisition. To support the efforts of participating teams, the Alexa Prize team made significant scientific and engineering investments to build and improve Conversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice User Experience, and tools for traffic management and scalability. This paper outlines the advances created by the university teams as well as the Alexa Prize team to achieve the common goal of solving the problem of Conversational AI.",http://aclweb.org/anthology/D16-1036,D16-1,D16-1036,https://arxiv.org/abs/1801.03604,"('Xiangyang Zhou', 'Daxiang Dong', 'Hua Wu', 'Shiqi Zhao', 'Dianhai Yu', 'Hao Tian', 'Xuan Liu', 'Rui Yan')",11,Multi-view Response Selection for Human-Computer Conversation,EMNLP,2016
"Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce neural discourse relation models to approximate the prior and posterior distributions of the latent variable, and employ these approximated distributions to optimize a reparameterized variational lower bound. This allows VarNDRR to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VarNDRR can achieve comparable results against stateof- the-art baselines without using any manual features.",http://aclweb.org/anthology/D16-1037,D16-1,D16-1037,https://arxiv.org/abs/1603.03876,"('Biao Zhang', 'Deyi Xiong', 'jinsong su', 'Qun Liu', 'Rongrong Ji', 'Hong Duan', 'Min Zhang')",11,Variational Neural Discourse Relation Recognizer,EMNLP,2016
"Audio Event Detection is an important task for content analysis of multimedia data. Most of the current works on detection of audio events is driven through supervised learning approaches. We propose a weakly supervised learning framework which can make use of the tremendous amount of web multimedia data with significantly reduced annotation effort and expense. Specifically, we use several multiple instance learning algorithms to show that audio event detection through weak labels is feasible. We also propose a novel scalable multiple instance learning algorithm and show that its competitive with other multiple instance learning algorithms for audio event detection tasks.",http://aclweb.org/anthology/D16-1038,D16-1,D16-1038,https://arxiv.org/abs/1606.03664,"('Haoruo Peng', 'Yangqiu Song', 'Dan Roth')",11,Event Detection and Co-reference with Minimal Supervision,EMNLP,2016
,http://aclweb.org/anthology/D16-1039,D16-1,D16-1039,,"('Tuan Luu Anh', 'Yi Tay', 'Siu Cheung Hui', 'See Kiong Ng')",11,"
      Learning Term Embeddings for Taxonomic Relation Identification Using
      Dynamic Weighting Neural Network
    ",EMNLP,2016
"Given a set of documents from a specific domain (e.g., medical research journals), how do we automatically build a Knowledge Graph (KG) for that domain? Automatic identification of relations and their schemas, i.e., type signature of arguments of relations (e.g., undergo(Patient, Surgery)), is an important first step towards this goal. We refer to this problem as Relation Schema Induction (RSI). In this paper, we propose Schema Induction using Coupled Tensor Factorization (SICTF), a novel tensor factorization method for relation schema induction. SICTF factorizes Open Information Extraction (OpenIE) triples extracted from a domain corpus along with additional side information in a principled way to induce relation schemas. To the best of our knowledge, this is the first application of tensor factorization for the RSI problem. Through extensive experiments on multiple real-world datasets, we find that SICTF is not only more accurate than state-of-the-art baselines, but also significantly faster (about 14x faster).",http://aclweb.org/anthology/D16-1040,D16-1,D16-1040,https://arxiv.org/abs/1605.04227,"('Madhav Nimishakavi', 'Uday Singh Saini', 'Partha Talukdar')",11,Relation Schema Induction using Tensor Factorization with Side Information,EMNLP,2016
"Modeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as entailment, coreference, relation extraction, and question answering. Supervised learning from labeled hypernym sources, such as WordNet, limits the coverage of these models, which can be addressed by learning hypernyms from unlabeled text. Existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy. This paper introduces distributional inclusion vector embedding (DIVE), a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings which preserve the inclusion property of word contexts in a low-dimensional and interpretable space. In experimental evaluations more comprehensive than any previous literature of which we are aware-evaluating on 11 datasets using multiple existing as well as newly proposed scoring functions-we find that our method provides up to double the precision of previous unsupervised embeddings, and the highest average performance, using a much more compact word representation, and yielding many new state-of-the-art results.",http://aclweb.org/anthology/D16-1041,D16-1,D16-1041,https://arxiv.org/abs/1710.00880,"('Luis Espinosa Anke', 'Jose Camacho-Collados', 'Claudio Delli Bovi', 'Horacio Saggion')",11,Supervised Distributional Hypernym Discovery via Domain Adaptation,EMNLP,2016
"Latent tree learning models represent sentences by composing their words according to an induced parse tree, all based on a downstream task. These models often outperform baselines which use (externally provided) syntax trees to drive the composition order. This work contributes (a) a new latent tree learning model based on shift-reduce parsing, with competitive downstream performance and non-trivial induced trees, and (b) an analysis of the trees learned by our shift-reduce model and by a chart-based model.",http://aclweb.org/anthology/D16-1042,D16-1,D16-1042,https://arxiv.org/abs/1806.00840,['Tomáš Brychcín'],11,Latent Tree Language Model,EMNLP,2016
,http://aclweb.org/anthology/D16-1043,D16-1,D16-1043,,"('Douwe Kiela', 'Anita Lilla Verő', 'Stephen Clark')",11,"
      Comparing Data Sources and Architectures for Deep Visual Representation
      Learning in Semantics
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1044,D16-1,D16-1044,,"('Akira Fukui', 'Dong Huk Park', 'Daylen Yang', 'Anna Rohrbach', 'Trevor Darrell', 'Marcus Rohrbach')",11,"
      Multimodal Compact Bilinear Pooling for Visual Question Answering and
      Visual Grounding
    ",EMNLP,2016
"We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a new structured prediction algorithm that generalizes the Collins Structured Perceptron (CSP). Unlike CSP, the update rule of SWVP explicitly exploits the internal structure of the predicted labels. We prove the convergence of SWVP for linearly separable training sets, provide mistake and generalization bounds, and show that in the general case these bounds are tighter than those of the CSP special case. In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results.",http://aclweb.org/anthology/D16-1045,D16-1,D16-1045,https://arxiv.org/abs/1602.03040,"('Rotem Dror', 'Roi Reichart')",11,The Structured Weighted Violations Perceptron Algorithm,EMNLP,2016
"Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.",http://aclweb.org/anthology/D16-1046,D16-1,D16-1046,https://arxiv.org/abs/1603.06111,"('Lili Mou', 'Zhao Meng', 'Rui Yan', 'Ge Li', 'Yan Xu', 'Lu Zhang', 'Zhi Jin')",11,How Transferable are Neural Networks in NLP Applications?,EMNLP,2016
"Word embeddings allow natural language processing systems to share statistical information across related words. These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features. Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging.",http://aclweb.org/anthology/D16-1047,D16-1,D16-1047,https://arxiv.org/abs/1608.01056,"('Parminder Bhatia', 'Robert Guthrie', 'Jacob Eisenstein')",11,Morphological Priors for Probabilistic Neural Word Embeddings,EMNLP,2016
,http://aclweb.org/anthology/D16-1048,D16-1,D16-1048,,"('Wenbin Jiang', 'Wen Zhang', 'Jinan Xu', 'Rangjia Cai')",11,"
      Automatic Cross-Lingual Similarization of Dependency Grammars for
      Tree-based Machine Translation
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1049,D16-1,D16-1049,,"('Naoki Otani', 'Toshiaki Nakazawa', 'Daisuke Kawahara', 'Sadao Kurohashi')",11,"
      IRT-based Aggregation Model of Crowdsourced Pairwise Comparison for
      Evaluating Machine Translations
    ",EMNLP,2016
"Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and English- German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.",http://aclweb.org/anthology/D16-1050,D16-1,D16-1050,https://arxiv.org/abs/1605.07869,"('Biao Zhang', 'Deyi Xiong', 'jinsong su', 'Hong Duan', 'Min Zhang')",11,Variational Neural Machine Translation,EMNLP,2016
"This paper presents a novel approach towards Indic handwritten word recognition using zone-wise information. Because of complex nature due to compound characters, modifiers, overlapping and touching, etc., character segmentation and recognition is a tedious job in Indic scripts (e.g. Devanagari, Bangla, Gurumukhi, and other similar scripts). To avoid character segmentation in such scripts, HMM-based sequence modeling has been used earlier in holistic way. This paper proposes an efficient word recognition framework by segmenting the handwritten word images horizontally into three zones (upper, middle and lower) and recognize the corresponding zones. The main aim of this zone segmentation approach is to reduce the number of distinct component classes compared to the total number of classes in Indic scripts. As a result, use of this zone segmentation approach enhances the recognition performance of the system. The components in middle zone where characters are mostly touching are recognized using HMM. After the recognition of middle zone, HMM based Viterbi forced alignment is applied to mark the left and right boundaries of the characters. Next, the residue components, if any, in upper and lower zones in their respective boundary are combined to achieve the final word level recognition. Water reservoir feature has been integrated in this framework to improve the zone segmentation and character alignment defects while segmentation. A novel sliding window-based feature, called Pyramid Histogram of Oriented Gradient (PHOG) is proposed for middle zone recognition. An exhaustive experiment is performed on two Indic scripts namely, Bangla and Devanagari for the performance evaluation. From the experiment, it has been noted that proposed zone-wise recognition improves accuracy with respect to the traditional way of Indic word recognition.",http://aclweb.org/anthology/D16-1051,D16-1,D16-1051,https://arxiv.org/abs/1708.00227,"('Andrei Simion', 'Michael Collins', 'Cliff Stein')",11,Towards a Convex HMM Surrogate for Word Alignment,EMNLP,2016
"Intelligence Quotient (IQ) Test is a set of standardized questions designed to evaluate human intelligence. Verbal comprehension questions appear very frequently in IQ tests, which measure human's verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by artificial intelligence technologies, especially the deep learning technologies that are recently developed and successfully applied in a number of fields. However, we found that the task was quite challenging, and simply applying existing technologies (e.g., word embedding) could not achieve a good performance, mainly due to the multiple senses of words and the complex relations among words. To tackle these challenges, we propose a novel framework consisting of three components. First, we build a classifier to recognize the specific type of a verbal question (e.g., analogy, classification, synonym, or antonym). Second, we obtain distributed representations of words and relations by leveraging a novel word embedding method that considers the multi-sense nature of words and the relational knowledge among words (or their senses) contained in dictionaries. Third, for each type of questions, we propose a specific solver based on the obtained distributed word representations and relation representations. Experimental results have shown that the proposed framework can not only outperform existing methods for solving verbal comprehension questions but also exceed the average performance of the Amazon Mechanical Turk workers involved in the study. The results indicate that with appropriate uses of the deep learning technologies we might be a further step closer to the human intelligence.",http://aclweb.org/anthology/D16-1052,D16-1,D16-1052,https://arxiv.org/abs/1505.07909,"('Huazheng Wang', 'Fei Tian', 'Bin Gao', 'Chengjieren Zhu', 'Jiang Bian', 'Tie-Yan Liu')",11,Solving Verbal Questions in IQ Test by Knowledge-Powered Word Embedding,EMNLP,2016
"New long read sequencing technologies, like PacBio SMRT and Oxford NanoPore, can produce sequencing reads up to 50,000 bp long but with an error rate of at least 15%. Reducing the error rate is necessary for subsequent utilisation of the reads in, e.g., de novo genome assembly. The error correction problem has been tackled either by aligning the long reads against each other or by a hybrid approach that uses the more accurate short reads produced by second generation sequencing technologies to correct the long reads. We present an error correction method that uses long reads only. The method consists of two phases: first we use an iterative alignment-free correction method based on de Bruijn graphs with increasing length of k-mers, and second, the corrected reads are further polished using long-distance dependencies that are found using multiple alignments. According to our experiments the proposed method is the most accurate one relying on long reads only for read sets with high coverage. Furthermore, when the coverage of the read set is at least 75x, the throughput of the new method is at least 20% higher. LoRMA is freely available at http://www.cs.helsinki.fi/u/lmsalmel/LoRMA/.",http://aclweb.org/anthology/D16-1053,D16-1,D16-1053,https://arxiv.org/abs/1604.02233,"('Jianpeng Cheng', 'Li Dong', 'Mirella Lapata')",11,Long Short-Term Memory-Networks for Machine Reading,EMNLP,2016
"While question answering (QA) with neural network, i.e. neural QA, has achieved promising results in recent years, lacking of large scale real-word QA dataset is still a challenge for developing and evaluating neural QA system. To alleviate this problem, we propose a large scale human annotated real-world QA dataset WebQA with more than 42k questions and 556k evidences. As existing neural QA methods resolve QA either as sequence generation or classification/ranking problem, they face challenges of expensive softmax computation, unseen answers handling or separate candidate answer generation component. In this work, we cast neural QA as a sequence labeling problem and propose an end-to-end sequence labeling model, which overcomes all the above challenges. Experimental results on WebQA show that our model outperforms the baselines significantly with an F1 score of 74.69% with word-based input, and the performance drops only 3.72 F1 points with more challenging character-based input.",http://aclweb.org/anthology/D16-1054,D16-1,D16-1054,https://arxiv.org/abs/1607.06275,"('Yu Su', 'Huan Sun', 'Brian Sadler', 'Mudhakar Srivatsa', 'Izzeddin Gur', 'Zenghui Yan', 'Xifeng Yan')",11,On Generating Characteristic-rich Question Sets for QA Evaluation,EMNLP,2016
"In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, context-based, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong baseline (p<0.05): translating all text into English, then training a classifier based only on English (original or translated) text.",http://aclweb.org/anthology/D16-1055,D16-1,D16-1055,https://arxiv.org/abs/1609.08210,"('Ferhan Ture', 'Elizabeth Boschee')",11,Learning to Translate for Multilingual Question Answering,EMNLP,2016
"We study the problem of identifying individuals based on their characteristic gaze patterns during reading of arbitrary text. The motivation for this problem is an unobtrusive biometric setting in which a user is observed during access to a document, but no specific challenge protocol requiring the user's time and attention is carried out. Existing models of individual differences in gaze control during reading are either based on simple aggregate features of eye movements, or rely on parametric density models to describe, for instance, saccade amplitudes or word fixation durations. We develop flexible semiparametric models of eye movements during reading in which densities are inferred under a Gaussian process prior centered at a parametric distribution family that is expected to approximate the true distribution well. An empirical study on reading data from 251 individuals shows significant improvements over the state of the art.",http://aclweb.org/anthology/D16-1056,D16-1,D16-1056,https://arxiv.org/abs/1607.05271,"('Ahmed Abdelwahab', 'Reinhold Kliegl', 'Niels Landwehr')",11,A Semiparametric Model for Bayesian Reader Identification,EMNLP,2016
"A word's sentiment depends on the domain in which it is used. Computational social science research thus requires sentiment lexicons that are specific to the domains being studied. We combine domain-specific word embeddings with a label propagation framework to induce accurate domain-specific sentiment lexicons using small sets of seed words, achieving state-of-the-art performance competitive with approaches that rely on hand-curated resources. Using our framework we perform two large-scale empirical studies to quantify the extent to which sentiment varies across time and between communities. We induce and release historical sentiment lexicons for 150 years of English and community-specific sentiment lexicons for 250 online communities from the social media forum Reddit. The historical lexicons show that more than 5% of sentiment-bearing (non-neutral) English words completely switched polarity during the last 150 years, and the community-specific lexicons highlight how sentiment varies drastically between different communities.",http://aclweb.org/anthology/D16-1057,D16-1,D16-1057,https://arxiv.org/abs/1606.02820,"('William L. Hamilton', 'Kevin Clark', 'Jure Leskovec', 'Dan Jurafsky')",11,Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora,EMNLP,2016
"With the development of the Internet, natural language processing (NLP), in which sentiment analysis is an important task, became vital in information processing.Sentiment analysis includes aspect sentiment classification. Aspect sentiment can provide complete and in-depth results with increased attention on aspect-level. Different context words in a sentence influence the sentiment polarity of a sentence variably, and polarity varies based on the different aspects in a sentence. Take the sentence, 'I bought a new camera. The picture quality is amazing but the battery life is too short.'as an example. If the aspect is picture quality, then the expected sentiment polarity is 'positive', if the battery life aspect is considered, then the sentiment polarity should be 'negative'; therefore, aspect is important to consider when we explore aspect sentiment in the sentence. Recurrent neural network (RNN) is regarded as a good model to deal with natural language processing, and RNNs has get good performance on aspect sentiment classification including Target-Dependent LSTM (TD-LSTM) ,Target-Connection LSTM (TC-LSTM) (Tang, 2015a, b), AE-LSTM, AT-LSTM, AEAT-LSTM (Wang et al., 2016).There are also extensive literatures on sentiment classification utilizing convolutional neural network, but there is little literature on aspect sentiment classification using convolutional neural network. In our paper, we develop attention-based input layers in which aspect information is considered by input layer. We then incorporate attention-based input layers into convolutional neural network (CNN) to introduce context words information. In our experiment, incorporating aspect information into CNN improves the latter's aspect sentiment classification performance without using syntactic parser or external sentiment lexicons in a benchmark dataset from Twitter but get better performance compared with other models.",http://aclweb.org/anthology/D16-1058,D16-1,D16-1058,https://arxiv.org/abs/1807.01704,"('Yequan Wang', 'Minlie Huang', 'xiaoyan zhu', 'Li Zhao')",11,Attention-based LSTM for Aspect-level Sentiment Classification,EMNLP,2016
"In aspect-based sentiment analysis, extracting aspect terms along with the opinions being expressed from user-generated content is one of the most important subtasks. Previous studies have shown that exploiting connections between aspect and opinion terms is promising for this task. In this paper, we propose a novel joint model that integrates recursive neural networks and conditional random fields into a unified framework for explicit aspect and opinion terms co-extraction. The proposed model learns high-level discriminative features and double propagate information between aspect and opinion terms, simultaneously. Moreover, it is flexible to incorporate hand-crafted features into the proposed model to further boost its information extraction performance. Experimental results on the SemEval Challenge 2014 dataset show the superiority of our proposed model over several baseline methods as well as the winning systems of the challenge.",http://aclweb.org/anthology/D16-1059,D16-1,D16-1059,https://arxiv.org/abs/1603.06679,"('Wenya Wang', 'Sinno Jialin Pan', 'Daniel Dahlmeier', 'Xiaokui Xiao')",11,Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis,EMNLP,2016
"Subjectivity detection is the task of identifying objective and subjective sentences. Objective sentences are those which do not exhibit any sentiment. So, it is desired for a sentiment analysis engine to find and separate the objective sentences for further analysis, e.g., polarity detection. In subjective sentences, opinions can often be expressed on one or multiple topics. Aspect extraction is a subtask of sentiment analysis that consists in identifying opinion targets in opinionated text, i.e., in detecting the specific aspects of a product or service the opinion holder is either praising or complaining about.",http://aclweb.org/anthology/D16-1060,D16-1,D16-1060,https://arxiv.org/abs/1710.06536,"('Abhishek Laddha', 'Arjun Mukherjee')",11,Extracting Aspect Specific Opinion Expressions,EMNLP,2016
"Emotion can be expressed in many ways that can be seen such as facial expression and gestures, speech and by written text. Emotion Detection in text documents is essentially a content - based classification problem involving concepts from the domains of Natural Language Processing as well as Machine Learning. In this paper emotion recognition based on textual data and the techniques used in emotion detection are discussed.",http://aclweb.org/anthology/D16-1061,D16-1,D16-1061,https://arxiv.org/abs/1205.4944,"('Deyu ZHOU', 'Xuan Zhang', 'Yin Zhou', 'Quan Zhao', 'Xin Geng')",11,Emotion Distribution Learning from Texts,EMNLP,2016
"Evaluation of NLP methods requires testing against a previously vetted gold-standard test set and reporting standard metrics (accuracy/precision/recall/F1). The current assumption is that all items in a given test set are equal with regards to difficulty and discriminating power. We propose Item Response Theory (IRT) from psychometrics as an alternative means for gold-standard test-set generation and NLP system evaluation. IRT is able to describe characteristics of individual items - their difficulty and discriminating power - and can account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we demonstrate IRT by generating a gold-standard test set for Recognizing Textual Entailment. By collecting a large number of human responses and fitting our IRT model, we show that our IRT model compares NLP systems with the performance in a human population and is able to provide more insight into system performance than standard evaluation metrics. We show that a high accuracy score does not always imply a high IRT score, which depends on the item characteristics and the response pattern.",http://aclweb.org/anthology/D16-1062,D16-1,D16-1062,https://arxiv.org/abs/1605.08889,"('John Lalor', 'Hao Wu', 'hong yu')",11,Building an Evaluation Scale using Item Response Theory,EMNLP,2016
"Embedding words in a vector space has gained a lot of attention in recent years. While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework WordRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the- arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage.",http://aclweb.org/anthology/D16-1063,D16-1,D16-1063,https://arxiv.org/abs/1506.02761,"('Shihao Ji', 'Hyokun Yun', 'Pinar Yanardag', 'Shin Matsushima', 'S. V. N. Vishwanathan')",11,WordRank: Learning Word Embeddings via Robust Ranking,EMNLP,2016
"We evaluate 8 different word embedding models on their usefulness for predicting the neural activation patterns associated with concrete nouns. The models we consider include an experiential model, based on crowd-sourced association data, several popular neural and distributional models, and a model that reflects the syntactic context of words (based on dependency parses). Our goal is to assess the cognitive plausibility of these various embedding models, and understand how we can further improve our methods for interpreting brain imaging data.   We show that neural word embedding models exhibit superior performance on the tasks we consider, beating experiential word representation model. The syntactically informed model gives the overall best performance when predicting brain activation patterns from word embeddings; whereas the GloVe distributional method gives the overall best performance when predicting in the reverse direction (words vectors from brain images). Interestingly, however, the error patterns of these different models are markedly different. This may support the idea that the brain uses different systems for processing different kinds of words. Moreover, we suggest that taking the relative strengths of different embedding models into account will lead to better models of the brain activity associated with words.",http://aclweb.org/anthology/D16-1064,D16-1,D16-1064,https://arxiv.org/abs/1711.09285,"('Yu-Ping Ruan', 'Zhen-Hua Ling', 'Yu Hu')",11,Exploring Semantic Representation in Brain Activity Using Word Embeddings,EMNLP,2016
"Abstract meaning representations (AMRs) are broad-coverage sentence-level semantic representations. AMRs represent sentences as rooted labeled directed acyclic graphs. AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences. We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts, relations and alignments. As exact inference requires marginalizing over alignments and is infeasible, we use the variational auto-encoding framework and a continuous relaxation of the discrete alignments. We show that joint modeling is preferable to using a pipeline of align and parse. The parser achieves the best reported results on the standard benchmark (74.4% on LDC2016E25).",http://aclweb.org/anthology/D16-1065,D16-1,D16-1065,https://arxiv.org/abs/1805.05286,"('Junsheng Zhou', 'Feiyu Xu', 'Hans Uszkoreit', 'Weiguang QU', 'Ran Li', 'Yanhui Gu')",11,AMR Parsing with an Incremental Joint Model,EMNLP,2016
"We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.",http://aclweb.org/anthology/D16-1066,D16-1,D16-1066,https://arxiv.org/abs/1609.00425,"('Ethan Fast', 'Eric Horvitz')",11,Identifying Dogmatism in Social Media: Signals and Models,EMNLP,2016
"The MBTI personality test and a personal facebook network were used in order to gain some insights on the relationship of social network centrality and path length measures and different personality types. Although the personality classification data were scarce, there were some intuitive quantitative results supporting anecdotal statements, based on empirical observations, about the expected social behavior of personality types.",http://aclweb.org/anthology/D16-1067,D16-1,D16-1067,https://arxiv.org/abs/1406.3663,"('Dong Zhou', 'Séamus Lawless', 'Xuan Wu', 'Wenyu Zhao', 'Jianxun Liu')",11,Enhanced Personalized Search using Social Data,EMNLP,2016
"Easy-first parsing relies on subtree re-ranking to build the complete parse tree. Whereas the intermediate state of parsing processing are represented by various subtrees, whose internal structural information is the key lead for later parsing action decisions, we explore a better representation for such subtrees. In detail, this work introduces a bottom-up subtree encoder based on the child-sum tree-LSTM. Starting from an easy-first dependency parser without other handcraft features, we show that the effective subtree encoder does promote the parsing process, and is able to make a greedy search easy-first parser achieve promising results on benchmark treebanks compared to state-of-the-art baselines.",http://aclweb.org/anthology/D16-1068,D16-1,D16-1068,https://arxiv.org/abs/1811.03511,"('Ilan Tchernowitz', 'Liron Yedidsion', 'Roi Reichart')",11,Effective Greedy Inference for Graph-based Non-Projective Dependency Parsing,EMNLP,2016
,http://aclweb.org/anthology/D16-1069,D16-1,D16-1069,,"('Qi Zhang', 'Jin Qian', 'Ya Guo', 'Yaqian Zhou', 'Xuanjing Huang')",11,"
      Generating Abbreviations for Chinese Named Entities Using Recurrent Neural
      Network with Dynamic Dictionary
    ",EMNLP,2016
"Argument mining is a core technology for automating argument search in large document collections. Despite its usefulness for this task, most current approaches to argument mining are designed for use only with specific text types and fall short when applied to heterogeneous texts. In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts. We source annotations for over 25,000 instances covering eight controversial topics. The results of cross-topic experiments show that our attention-based neural network generalizes best to unseen topics and outperforms vanilla BiLSTM models by 6% in accuracy and 11% in F-score.",http://aclweb.org/anthology/D16-1070,D16-1,D16-1070,https://arxiv.org/abs/1802.05758,"('Hongshen Chen', 'Yue Zhang', 'Qun Liu')",11,Neural Network for Heterogeneous Annotations,EMNLP,2016
"We provide a comprehensive analysis of the interactions between pre-trained word embeddings, character models and POS tags in a transition-based dependency parser. While previous studies have shown POS information to be less important in the presence of character models, we show that in fact there are complex interactions between all three techniques. In isolation each produces large improvements over a baseline system using randomly initialised word embeddings only, but combining them quickly leads to diminishing returns. We categorise words by frequency, POS tag and language in order to systematically investigate how each of the techniques affects parsing quality. For many word categories, applying any two of the three techniques is almost as good as the full combined system. Character models tend to be more important for low-frequency open-class words, especially in morphologically rich languages, while POS tags can help disambiguate high-frequency function words. We also show that large character embedding sizes help even for languages with small character sets, especially in morphologically rich languages.",http://aclweb.org/anthology/D16-1071,D16-1,D16-1071,https://arxiv.org/abs/1808.09060,"('Sebastian Ebert', 'Thomas Müller', 'Hinrich Schütze')",11,LAMB: A Good Shepherd of Morphologically Rich Languages,EMNLP,2016
,http://aclweb.org/anthology/D16-1072,D16-1,D16-1072,,"('Zhenghua Li', 'Jiayuan Chao', 'Min Zhang', 'Jiwen Yang')",11,"
      Fast Coupled Sequence Labeling on Heterogeneous Annotations via
      Context-aware Pruning
    ",EMNLP,2016
"Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.",http://aclweb.org/anthology/D16-1073,D16-1,D16-1073,https://arxiv.org/abs/1808.09111,"('Yong Jiang', 'Wenjuan Han', 'Kewei Tu')",11,Unsupervised Neural Dependency Parsing,EMNLP,2016
"Coherence plays a critical role in producing a high-quality summary from a document. In recent years, neural extractive summarization is becoming increasingly attractive. However, most of them ignore the coherence of summaries when extracting sentences. As an effort towards extracting coherent summaries, we propose a neural coherence model to capture the cross-sentence semantic and syntactic coherence patterns. The proposed neural coherence model obviates the need for feature engineering and can be trained in an end-to-end fashion using unlabeled data. Empirical results show that the proposed neural coherence model can efficiently capture the cross-sentence coherence patterns. Using the combined output of the neural coherence model and ROUGE package as the reward, we design a reinforcement learning method to train a proposed neural extractive summarizer which is named Reinforced Neural Extractive Summarization (RNES) model. The RNES model learns to optimize coherence and informative importance of the summary simultaneously. Experimental results show that the proposed RNES outperforms existing baselines and achieves state-of-the-art performance in term of ROUGE on CNN/Daily Mail dataset. The qualitative evaluation indicates that summaries produced by RNES are more coherent and readable.",http://aclweb.org/anthology/D16-1074,D16-1,D16-1074,https://arxiv.org/abs/1804.07036,"('Daraksha Parveen', 'Mohsen Mesgar', 'Michael Strube')",11,Generating Coherent Summaries of Scientific Articles Using Coherence Patterns,EMNLP,2016
"Graph streams, which refer to the graph with edges being updated sequentially in a form of a stream, have wide applications such as cyber security, social networks and transportation networks. This paper studies the problem of summarizing graph streams. Specifically, given a graph stream G, directed or undirected, the objective is to summarize G as S with much smaller (sublinear) space, linear construction time and constant maintenance cost for each edge update, such that S allows many queries over G to be approximately conducted efficiently. Due to the sheer volume and highly dynamic nature of graph streams, summarizing them remains a notoriously hard, if not impossible, problem. The widely used practice of summarizing data streams is to treat each element independently by e.g., hash- or sampling-based method, without keeping track of the connections between elements in a data stream, which gives these summaries limited power in supporting complicated queries over graph streams. This paper discusses a fundamentally different philosophy for summarizing graph streams. We present gLava, a probabilistic graph model that, instead of treating an edge (a stream element) as the operating unit, uses the finer grained node in an element. This will naturally form a new graph sketch where edges capture the connections inside elements, and nodes maintain relationships across elements. We discuss a wide range of supported graph queries and establish theoretical error bounds for basic queries.",http://aclweb.org/anthology/D16-1075,D16-1,D16-1075,https://arxiv.org/abs/1510.02219,"('Tao Ge', 'Lei Cui', 'Baobao Chang', 'Sujian Li', 'Ming Zhou', 'Zhifang Sui')",11,News Stream Summarization using Burst Information Networks,EMNLP,2016
"This article provides an interesting exploration of character-level convolutional neural network solving Chinese corpus text classification problem. We constructed a large-scale Chinese language dataset, and the result shows that character-level convolutional neural network works better on Chinese corpus than its corresponding pinyin format dataset. This is the first time that character-level convolutional neural network applied to text classification problem.",http://aclweb.org/anthology/D16-1076,D16-1,D16-1076,https://arxiv.org/abs/1611.04358,"('Ye Zhang', 'Iain Marshall', 'Byron C. Wallace')",11,Rationale-Augmented Convolutional Neural Networks for Text Classification,EMNLP,2016
,http://aclweb.org/anthology/D16-1077,D16-1,D16-1077,,"('Yu-Yang Huang', 'Shou-De Lin')",11,"
      Transferring User Interests Across Websites with Unstructured Text for
      Cold-Start Recommendation
    ",EMNLP,2016
"Negation scope has been annotated in several English and Chinese corpora, and highly accurate models for this task in these languages have been learned from these annotations. Unfortunately, annotations are not available in other languages. Could a model that detects negation scope be applied to a language that it hasn't been trained on? We develop neural models that learn from cross-lingual word embeddings or universal dependencies in English, and test them on Chinese, showing that they work surprisingly well. We find that modelling syntax is helpful even in monolingual settings and that cross-lingual word embeddings help relatively little, and we analyse cases that are still difficult for this task.",http://aclweb.org/anthology/D16-1078,D16-1,D16-1078,https://arxiv.org/abs/1810.02156,"('Zhong Qian', 'Peifeng Li', 'Qiaoming Zhu', 'Guodong Zhou', 'Zhunchen Luo', 'Wei Luo')",11,Speculation and Negation Scope Detection via Convolutional Neural Networks,EMNLP,2016
"Natural language generation (NLG) is a critical component in spoken dialogue system, which can be divided into two phases: (1) sentence planning: deciding the overall sentence structure, (2) surface realization: determining specific word forms and flattening the sentence structure into a string. With the rise of deep learning, most modern NLG models are based on a sequence-to-sequence (seq2seq) model, which basically contains an encoder-decoder structure; these NLG models generate sentences from scratch by jointly optimizing sentence planning and surface realization. However, such simple encoder-decoder architecture usually fail to generate complex and long sentences, because the decoder has difficulty learning all grammar and diction knowledge well. This paper introduces an NLG model with a hierarchical attentional decoder, where the hierarchy focuses on leveraging linguistic knowledge in a specific order. The experiments show that the proposed method significantly outperforms the traditional seq2seq model with a smaller model size, and the design of the hierarchical attentional decoder can be applied to various NLG systems. Furthermore, different generation strategies based on linguistic patterns are investigated and analyzed in order to guide future NLG research work.",http://aclweb.org/anthology/D16-1079,D16-1,D16-1079,https://arxiv.org/abs/1809.07629,"('Peng Qian', 'Xipeng Qiu', 'Xuanjing Huang')",11,Analyzing Linguistic Knowledge in Sequential Model of Sentence,EMNLP,2016
"Keyphrases provide a simple way of describing a document, giving the reader some clues about its contents. Keyphrases can be useful in a various applications such as retrieval engines, browsing interfaces, thesaurus construction, text mining etc.. There are also other tasks for which keyphrases are useful, as we discuss in this paper. This paper describes a neural network based approach to keyphrase extraction from scientific articles. Our results show that the proposed method performs better than some state-of-the art keyphrase extraction approaches.",http://aclweb.org/anthology/D16-1080,D16-1,D16-1080,https://arxiv.org/abs/1004.3274,"('Qi Zhang', 'Yang Wang', 'Yeyun Gong', 'Xuanjing Huang')",11,Keyphrase Extraction Using Deep Recurrent Neural Networks on Twitter,EMNLP,2016
"Handwriting of Chinese has long been an important skill in East Asia. However, automatic generation of handwritten Chinese characters poses a great challenge due to the large number of characters. Various machine learning techniques have been used to recognize Chinese characters, but few works have studied the handwritten Chinese character generation problem, especially with unpaired training data. In this work, we formulate the Chinese handwritten character generation as a problem that learns a mapping from an existing printed font to a personalized handwritten style. We further propose DenseNet CycleGAN to generate Chinese handwritten characters. Our method is applied not only to commonly used Chinese characters but also to calligraphy work with aesthetic values. Furthermore, we propose content accuracy and style discrepancy as the evaluation metrics to assess the quality of the handwritten characters generated. We then use our proposed metrics to evaluate the generated characters from CASIA dataset as well as our newly introduced Lanting calligraphy dataset.",http://aclweb.org/anthology/D16-1081,D16-1,D16-1081,https://arxiv.org/abs/1801.08624,"('Chuanqi Tan', 'Furu Wei', 'Li Dong', 'Weifeng Lv', 'Ming Zhou')",11,Solving and Generating Chinese Character Riddles,EMNLP,2016
"Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In clinical domain one major application of sequence labeling involves extraction of medical entities such as medication, indication, and side-effects from Electronic Health Record narratives. Sequence labeling in this domain, presents its own set of challenges and objectives. In this work we experimented with various CRF based structured learning models with Recurrent Neural Networks. We extend the previously studied LSTM-CRF models with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methodologies for structured prediction in order to improve the exact phrase detection of various medical entities.",http://aclweb.org/anthology/D16-1082,D16-1,D16-1082,https://arxiv.org/abs/1608.00612,"('Abhyuday Jagannatha', 'hong yu')",11,Structured prediction models for RNN based sequence labeling in clinical text,EMNLP,2016
"With the rapid adoption of Internet as an easy way to communicate, the amount of unsolicited e-mails, known as spam e-mails, has been growing rapidly. The major problem of spam e-mails is the loss of productivity and a drain on IT resources. Today, we receive spam more rapidly than the legitimate e-mails. Initially, spam e-mails contained only textual messages which were easily detected by the text-based spam filters. To evade such detection, spammers came up with a new sophisticated technique called image spam. Image spam consists in embedding the advertisement text in images rather than in the body of the e-mail, yet the image contents are not detected by most spam filters. In this paper, we examine the motivations and the challenges in image spam filtering research, and we review the recent trends in combating image spam e-mails. The review indicates that spamming is a business model and spammers are becoming more sophisticated in their approach to adapt to all challenges, and hence, defeating the conventional spam filtering technologies. Therefore, image spam detection techniques should be scalable and adaptable to meet the future tactics of the spammers.",http://aclweb.org/anthology/D16-1083,D16-1,D16-1083,https://arxiv.org/abs/1212.1763,"('Xuepeng Wang', 'Kang Liu', 'Shizhu He', 'Jun Zhao')",11,Learning to Represent Review with Tensor Decomposition for Spam Detection,EMNLP,2016
"Stance detection is the task of classifying the attitude expressed in a text towards a target such as Hillary Clinton to be ""positive"", negative"" or ""neutral"". Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently. Performance is improved further when the conditional model is augmented with bidirectional encoding. We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target. When such weak supervision is added, our approach achieves state-of-the-art results.",http://aclweb.org/anthology/D16-1084,D16-1,D16-1084,https://arxiv.org/abs/1606.05464,"('Isabelle Augenstein', 'Tim Rocktäschel', 'Andreas Vlachos', 'Kalina Bontcheva')",11,Stance Detection with Bidirectional Conditional Encoding,EMNLP,2016
"We propose a multi-label multi-task framework based on a convolutional recurrent neural network to unify detection of isolated and overlapping audio events. The framework leverages the power of convolutional recurrent neural network architectures; convolutional layers learn effective features over which higher recurrent layers perform sequential modelling. Furthermore, the output layer is designed to handle arbitrary degrees of event overlap. At each time step in the recurrent output sequence, an output triple is dedicated to each event category of interest to jointly model event occurrence and temporal boundaries. That is, the network jointly determines whether an event of this category occurs, and when it occurs, by estimating onset and offset positions at each recurrent time step. We then introduce three sequential losses for network training: multi-label classification loss, distance estimation loss, and confidence loss. We demonstrate good generalization on two datasets: ITC-Irst for isolated audio event detection, and TUT-SED-Synthetic-2016 for overlapping audio event detection.",http://aclweb.org/anthology/D16-1085,D16-1,D16-1085,https://arxiv.org/abs/1811.01092,"('Thien Huu Nguyen', 'Ralph Grishman')",11,Modeling Skip-Grams for Event Detection with Convolutional Neural Networks,EMNLP,2016
"Neural Machine Translation (NMT) has been widely used in recent years with significant improvements for many language pairs. Although state-of-the-art NMT systems are generating progressively better translations, idiom translation remains one of the open challenges in this field. Idioms, a category of multiword expressions, are an interesting language phenomenon where the overall meaning of the expression cannot be composed from the meanings of its parts. A first important challenge is the lack of dedicated data sets for learning and evaluating idiom translation. In this paper we address this problem by creating the first large-scale data set for idiom translation. Our data set is automatically extracted from a widely used German-English translation corpus and includes, for each language direction, a targeted evaluation set where all sentences contain idioms and a regular training corpus where sentences including idioms are marked. We release this data set and use it to perform preliminary NMT experiments as the first step towards better idiom translation.",http://aclweb.org/anthology/D16-1086,D16-1,D16-1086,https://arxiv.org/abs/1802.04681,"('Tobias Falke', 'Gabriel Stanovsky', 'Iryna Gurevych', 'Ido Dagan')",11,Porting an Open Information Extraction System from English to German,EMNLP,2016
"In named entity recognition, we often don't have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly. In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-specific NE model. That is, the novelty in the task setup is that we assume not just domain mismatch, but also label mismatch.",http://aclweb.org/anthology/D16-1087,D16-1,D16-1087,https://arxiv.org/abs/1610.09914,"('Lizhen Qu', 'Gabriela Ferraro', 'Liyuan Zhou', 'Weiwei Hou', 'Timothy Baldwin')",11,Named Entity Recognition for Novel Types by Transfer Learning,EMNLP,2016
"Multi-particle azimuthal cumulants, often used to study collective flow in high-energy heavy-ion collisions, have recently been applied in small collision systems such as $pp$ and $p$+A to extract the second-order azimuthal harmonic flow $v_2$. Recent observation of four-, six- and eight-particle cumulants with ""correct sign"" $c_2\{4\}<0, c_2\{6\}>0, c_2\{8\}<0$ and approximate equality of the inferred single-particle harmonic flow, $v_2\{4\}\approx v_2\{6\}\approx v_2\{8\}$, have been used as strong evidence for a collective emission of all soft particles produced in the collisions. We show that these relations in principle could be violated due to the non-Gaussianity in the event-by-event fluctuation of flow and/or non-flow. Furthermore, we show, using $pp$ events generated with the PYTHIA model, that $c_2\{2k\}$ obtained with standard cumulant method are dominated by non-flow from dijets. An alternative cumulant method based on two or more $\eta$-separated subevents is proposed to suppress the dijet contribution. The new method is shown to be able to recover a flow signal as low as 4\% imposed on the PYTHIA events, independently of how the event activity class is defined. Therefore the subevent cumulant method offers a more robust way of studying collectivity based on the existence of long-range azimuthal correlations between multiple distinct $\eta$ ranges. The prospect of using the subevent cumulants to study collective flow in A+A collisions, in particular its longitudinal dynamics, is discussed.",http://aclweb.org/anthology/D16-1088,D16-1,D16-1088,https://arxiv.org/abs/1701.03830,"('Allison Badgett', 'Ruihong Huang')",11,Extracting Subevents via an Effective Two-phase Approach,EMNLP,2016
"Embeddings in machine learning are low-dimensional representations of complex input patterns, with the property that simple geometric operations like Euclidean distances and dot products can be used for classification and comparison tasks. The proposed meta-embeddings are special embeddings that live in more general inner product spaces. They are designed to propagate uncertainty to the final output in speaker recognition and similar applications. The familiar Gaussian PLDA model (GPLDA) can be re-formulated as an extractor for Gaussian meta-embeddings (GMEs), such that likelihood ratio scores are given by Hilbert space inner products between Gaussian likelihood functions. GMEs extracted by the GPLDA model have fixed precisions and do not propagate uncertainty. We show that a generalization to heavy-tailed PLDA gives GMEs with variable precisions, which do propagate uncertainty. Experiments on NIST SRE 2010 and 2016 show that the proposed method applied to i-vectors without length normalization is up to 20% more accurate than GPLDA applied to length-normalized ivectors.",http://aclweb.org/anthology/D16-1089,D16-1,D16-1089,https://arxiv.org/abs/1802.09777,"('Tanmoy Mukherjee', 'Timothy Hospedales')",11,Gaussian Visual-Linguistic Embedding for Zero-Shot Recognition,EMNLP,2016
"Visual Question Answering (VQA) is the task of answering natural-language questions about images. We introduce the novel problem of determining the relevance of questions to images in VQA. Current VQA models do not reason about whether a question is even related to the given image (e.g. What is the capital of Argentina?) or if it requires information from external resources to answer correctly. This can break the continuity of a dialogue in human-machine interaction. Our approaches for determining relevance are composed of two stages. Given an image and a question, (1) we first determine whether the question is visual or not, (2) if visual, we determine whether the question is relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA model uncertainty, and caption-question similarity, are able to outperform strong baselines on both relevance tasks. We also present human studies showing that VQA models augmented with such question relevance reasoning are perceived as more intelligent, reasonable, and human-like.",http://aclweb.org/anthology/D16-1090,D16-1,D16-1090,https://arxiv.org/abs/1606.06622,"('Arijit Ray', 'Gordon Christie', 'Mohit Bansal', 'Dhruv Batra', 'Devi Parikh')",11,Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions,EMNLP,2016
"Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense.",http://aclweb.org/anthology/D16-1091,D16-1,D16-1091,https://arxiv.org/abs/1606.07493,"('Harsh Agrawal', 'Arjun Chandrasekaran', 'Dhruv Batra', 'Devi Parikh', 'Mohit Bansal')",11,Sort Story: Sorting Jumbled Images and Captions into Stories,EMNLP,2016
,http://aclweb.org/anthology/D16-1092,D16-1,D16-1092,,"('Abhishek Das', 'Harsh Agrawal', 'Larry Zitnick', 'Devi Parikh', 'Dhruv Batra')",11,"
      Human Attention in Visual Question Answering: Do Humans and Deep Networks
      look at the same regions?
    ",EMNLP,2016
"In this paper, we propose a recurrent neural network (RNN) with residual attention (RRA) to learn long-range dependencies from sequential data. We propose to add residual connections across timesteps to RNN, which explicitly enhances the interaction between current state and hidden states that are several timesteps apart. This also allows training errors to be directly back-propagated through residual connections and effectively alleviates gradient vanishing problem. We further reformulate an attention mechanism over residual connections. An attention gate is defined to summarize the individual contribution from multiple previous hidden states in computing the current state. We evaluate RRA on three tasks: the adding problem, pixel-by-pixel MNIST classification and sentiment analysis on the IMDB dataset. Our experiments demonstrate that RRA yields better performance, faster convergence and more stable training compared to a standard LSTM network. Furthermore, RRA shows highly competitive performance to the state-of-the-art methods.",http://aclweb.org/anthology/D16-1093,D16-1,D16-1093,https://arxiv.org/abs/1709.03714,"('Yiren Wang', 'Fei Tian')",11,Recurrent Residual Learning for Sequence Classification,EMNLP,2016
"We introduce a novel approach for building language models based on a systematic, recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing. Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case. In this paper we motivate, formalize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1% and 12.7% in comparison to traditional language models using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity.",http://aclweb.org/anthology/D16-1094,D16-1,D16-1094,https://arxiv.org/abs/1404.3377,"('Ehsan Shareghi', 'Trevor Cohn', 'Gholamreza Haffari')",11,Richer Interpolative Smoothing Based on Modified Kneser-Ney Language Modeling,EMNLP,2016
"We propose a general framework for unsupervised domain adaptation, which allows deep neural networks trained on a source domain to be tested on a different target domain without requiring any training annotations in the target domain. This is achieved by adding extra networks and losses that help regularize the features extracted by the backbone encoder network. To this end we propose the novel use of the recently proposed unpaired image-toimage translation framework to constrain the features extracted by the encoder network. Specifically, we require that the features extracted are able to reconstruct the images in both domains. In addition we require that the distribution of features extracted from images in the two domains are indistinguishable. Many recent works can be seen as specific cases of our general framework. We apply our method for domain adaptation between MNIST, USPS, and SVHN datasets, and Amazon, Webcam and DSLR Office datasets in classification tasks, and also between GTA5 and Cityscapes datasets for a segmentation task. We demonstrate state of the art performance on each of these datasets.",http://aclweb.org/anthology/D16-1095,D16-1,D16-1095,https://arxiv.org/abs/1712.00479,"('Wei Lu', 'Hai Leong Chieu', 'Jonathan Löfgren')",11,A General Regularization Framework for Domain Adaptation,EMNLP,2016
"In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.",http://aclweb.org/anthology/D16-1096,D16-1,D16-1096,https://arxiv.org/abs/1605.03148,"('Haitao Mi', 'Baskaran Sankaran', 'Zhiguo Wang', 'Abe Ittycheriah')",11,Coverage Embedding Models for Neural Machine Translation,EMNLP,2016
"Automatic segmentation of retinal blood vessels from fundus images plays an important role in the computer aided diagnosis of retinal diseases. The task of blood vessel segmentation is challenging due to the extreme variations in morphology of the vessels against noisy background. In this paper, we formulate the segmentation task as a multi-label inference task and utilize the implicit advantages of the combination of convolutional neural networks and structured prediction. Our proposed convolutional neural network based model achieves strong performance and significantly outperforms the state-of-the-art for automatic retinal blood vessel segmentation on DRIVE dataset with 95.33% accuracy and 0.974 AUC score.",http://aclweb.org/anthology/D16-1097,D16-1,D16-1097,https://arxiv.org/abs/1611.02064,"('Katharina Kann', 'Ryan Cotterell', 'Hinrich Schütze')",11,Neural Morphological Analysis: Encoding-Decoding Canonical Segments,EMNLP,2016
,http://aclweb.org/anthology/D16-1098,D16-1,D16-1098,,"('Peng Shi', 'Zhiyang Teng', 'Yue Zhang')",11,"
      Exploiting Mutual Benefits between Syntax and Semantic Roles using Neural
      Network
    ",EMNLP,2016
"This paper investigates the effects of data size and frequency range on distributional semantic models. We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted factorized model.",http://aclweb.org/anthology/D16-1099,D16-1,D16-1099,https://arxiv.org/abs/1609.08293,"('Magnus Sahlgren', 'Alessandro Lenci')",11,The Effects of Data Size and Frequency Range on Distributional Semantic Models,EMNLP,2016
"Intent classification has been widely researched on English data with deep learning approaches that are based on neural networks and word embeddings. The challenge for Chinese intent classification stems from the fact that, unlike English where most words are made up of 26 phonologic alphabet letters, Chinese is logographic, where a Chinese character is a more basic semantic unit that can be informative and its meaning does not vary too much in contexts. Chinese word embeddings alone can be inadequate for representing words, and pre-trained embeddings can suffer from not aligning well with the task at hand. To account for the inadequacy and leverage Chinese character information, we propose a low-effort and generic way to dynamically integrate character embedding based feature maps with word embedding based inputs, whose resulting word-character embeddings are stacked with a contextual information extraction module to further incorporate context information for predictions. On top of the proposed model, we employ an ensemble method to combine single models and obtain the final result. The approach is data-independent without relying on external sources like pre-trained word embeddings. The proposed model outperforms baseline models and existing methods.",http://aclweb.org/anthology/D16-1100,D16-1,D16-1100,https://arxiv.org/abs/1805.08914,"('Rongchao Yin', 'Quan Wang', 'Peng Li', 'Rui Li', 'Bin Wang')",11,Multi-Granularity Chinese Word Embedding,EMNLP,2016
"Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements.",http://aclweb.org/anthology/D16-1101,D16-1,D16-1101,https://arxiv.org/abs/1608.04147,"('Georgios Spithourakis', 'Isabelle Augenstein', 'Sebastian Riedel')",11,Numerically Grounded Language Models for Semantic Error Correction,EMNLP,2016
,http://aclweb.org/anthology/D16-1102,D16-1,D16-1102,,"('Alan Akbik', 'vishwajeet kumar', 'Yunyao Li')",11,"
      Towards Semi-Automatic Generation of Proposition Banks for Low-Resource
      Languages
    ",EMNLP,2016
"Text summarization and sentiment classification both aim to capture the main ideas of the text but at different levels. Text summarization is to describe the text within a few sentences, while sentiment classification can be regarded as a special type of summarization which ""summarizes"" the text into a even more abstract fashion, i.e., a sentiment class. Based on this idea, we propose a hierarchical end-to-end model for joint learning of text summarization and sentiment classification, where the sentiment classification label is treated as the further ""summarization"" of the text summarization output. Hence, the sentiment classification layer is put upon the text summarization layer, and a hierarchical structure is derived. Experimental results on Amazon online reviews datasets show that our model achieves better performance than the strong baseline systems on both abstractive summarization and sentiment classification.",http://aclweb.org/anthology/D16-1103,D16-1,D16-1103,https://arxiv.org/abs/1805.01089,"('Sebastian Ruder', 'Parsa Ghaffari', 'John G. Breslin')",11,A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis,EMNLP,2016
"Sarcasm is considered one of the most difficult problem in sentiment analysis. In our ob-servation on Indonesian social media, for cer-tain topics, people tend to criticize something using sarcasm. Here, we proposed two additional features to detect sarcasm after a common sentiment analysis is conducted. The features are the negativity information and the number of interjection words. We also employed translated SentiWordNet in the sentiment classification. All the classifications were conducted with machine learning algorithms. The experimental results showed that the additional features are quite effective in the sarcasm detection.",http://aclweb.org/anthology/D16-1104,D16-1,D16-1104,https://arxiv.org/abs/1505.03085,"('Aditya Joshi', 'Vaibhav Tripathi', 'Kevin Patel', 'Pushpak Bhattacharyya', 'Mark Carman')",11,Are Word Embedding-based Features Useful for Sarcasm Detection?,EMNLP,2016
"We can often detect from a person's utterances whether he/she is in favor of or against a given target entity -- their stance towards the target. However, a person may express the same stance towards a target by using negative or positive language. Here for the first time we present a dataset of tweet--target pairs annotated for both stance and sentiment. The targets may or may not be referred to in the tweets, and they may or may not be the target of opinion in the tweets. Partitions of this dataset were used as training and test sets in a SemEval-2016 shared task competition. We propose a simple stance detection system that outperforms submissions from all 19 teams that participated in the shared task. Additionally, access to both stance and sentiment annotations allows us to explore several research questions. We show that while knowing the sentiment expressed by a tweet is beneficial for stance classification, it alone is not sufficient. Finally, we use additional unlabeled data through distant supervision techniques and word embeddings to further improve stance classification.",http://aclweb.org/anthology/D16-1105,D16-1,D16-1105,https://arxiv.org/abs/1605.01655,"('Javid Ebrahimi', 'Dejing Dou', 'Daniel Lowd')",11,Weakly Supervised Tweet Stance Classification by Relational Bootstrapping,EMNLP,2016
"We describe the Gun Violence Database (GVDB), a large and growing database of gun violence incidents in the United States. The GVDB is built from the detailed information found in local news reports about gun violence, and is constructed via a large-scale crowdsourced annotation effort through our web site, http://gun-violence.org/. We argue that centralized and publicly available data about gun violence can facilitate scientific, fact-based discussion about a topic that is often dominated by politics and emotion. We describe our efforts to automate the construction of the database using state-of-the-art natural language processing (NLP) technologies, eventually enabling a fully-automated, highly-scalable resource for research on this important public health problem.",http://aclweb.org/anthology/D16-1106,D16-1,D16-1106,https://arxiv.org/abs/1610.01670,"('Ellie Pavlick', 'Heng Ji', 'Xiaoman Pan', 'Chris Callison-Burch')",11,The Gun Violence Database: A new task and data set for NLP,EMNLP,2016
"Although existing image caption models can produce promising results using recurrent neural networks (RNNs), it is difficult to guarantee that an object we care about is contained in generated descriptions, for example in the case that the object is inconspicuous in image. Problems become even harder when these objects did not appear in training stage. In this paper, we propose a novel approach for generating image captions with guiding objects (CGO). The CGO constrains the model to involve a human-concerned object, when the object is in the image, in the generated description while maintaining fluency. Instead of generating the sequence from left to right, we start description with a selected object and generate other parts of the sequence based on this object. To achieve this, we design a novel framework combining two LSTMs in opposite directions. We demonstrate the characteristics of our method on MSCOCO to generate descriptions for each detected object in images. With CGO, we can extend the ability of description to the objects being neglected in image caption labels and provide a set of more comprehensive and diverse descriptions for an image. CGO shows obvious advantages when applied to the task of describing novel objects. We show experiment results on both MSCOCO and ImageNet datasets. Evaluations show that our method outperforms the state-of-the-art models in the task with average F1 75.8, leading to better descriptions in terms of both content accuracy and fluency.",http://aclweb.org/anthology/D16-1107,D16-1,D16-1107,https://arxiv.org/abs/1811.07662,"('Tom Lippincott', 'Benjamin Van Durme')",11,Fluency detection on communication networks,EMNLP,2016
,http://aclweb.org/anthology/D16-1108,D16-1,D16-1108,,"('Trang Tran', 'Mari Ostendorf')",11,"
      Characterizing the Language of Online Communities and its Relation to
      Community Reception
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1109,D16-1,D16-1109,,"('Masashi Yoshikawa', 'Hiroyuki Shindo', 'Yuji Matsumoto')",11,"
      Joint Transition-based Dependency Parsing and Disfluency Detection for
      Automatic Speech Recognition Texts
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1110,D16-1,D16-1110,,"('Dario Bertero', 'Farhad Bin Siddique', 'Chien-Sheng Wu', 'Yan Wan', 'Ricky Ho Yin Chan', 'Pascale Fung')",11,"
      Real-Time Speech Emotion and Sentiment Recognition for Interactive
      Dialogue Systems
    ",EMNLP,2016
"Punctuation is a strong indicator of syntactic structure, and parsers trained on text with punctuation often rely heavily on this signal. Punctuation is a diversion, however, since human language processing does not rely on punctuation to the same extent, and in informal texts, we therefore often leave out punctuation. We also use punctuation ungrammatically for emphatic or creative purposes, or simply by mistake. We show that (a) dependency parsers are sensitive to both absence of punctuation and to alternative uses; (b) neural parsers tend to be more sensitive than vintage parsers; (c) training neural parsers without punctuation outperforms all out-of-the-box parsers across all scenarios where punctuation departs from standard punctuation. Our main experiments are on synthetically corrupted data to study the effect of punctuation in isolation and avoid potential confounds, but we also show effects on out-of-domain data.",http://aclweb.org/anthology/D16-1111,D16-1,D16-1111,https://arxiv.org/abs/1809.00070,"('Miguel Ballesteros', 'Leo Wanner')",11,A Neural Network Architecture for Multilingual Punctuation Generation,EMNLP,2016
"Recent neural headline generation models have shown great results, but are generally trained on very large datasets. We focus our efforts on improving headline quality on smaller datasets by the means of pretraining. We propose new methods that enable pre-training all the parameters of the model and utilize all available text, resulting in improvements by up to 32.4% relative in perplexity and 2.84 points in ROUGE.",http://aclweb.org/anthology/D16-1112,D16-1,D16-1112,https://arxiv.org/abs/1707.09769,"('Sho Takase', 'Jun Suzuki', 'Naoaki Okazaki', 'Tsutomu Hirao', 'Masaaki Nagata')",11,Neural Headline Generation on Abstract Meaning Representation,EMNLP,2016
"Any-gram kernels are a flexible and efficient way to employ bag-of-n-gram features when learning from textual data. They are also compatible with the use of word embeddings so that word similarities can be accounted for. While the original any-gram kernels are implemented on top of tree kernels, we propose a new approach which is independent of tree kernels and is more efficient. We also propose a more effective way to make use of word embeddings than the original any-gram formulation. When applied to the task of sentiment classification, our new formulation achieves significantly better performance.",http://aclweb.org/anthology/D16-1113,D16-1,D16-1113,https://arxiv.org/abs/1712.07004,"('Taygun Kekec', 'David M. J. Tax')",11,Robust Gram Embeddings,EMNLP,2016
"This paper describes and evaluates the Metalinguistic Operation Processor (MOP) system for automatic compilation of metalinguistic information from technical and scientific documents. This system is designed to extract non-standard terminological resources that we have called Metalinguistic Information Databases (or MIDs), in order to help update changing glossaries, knowledge bases and ontologies, as well as to reflect the metastable dynamics of special-domain knowledge.",http://aclweb.org/anthology/D16-1114,D16-1,D16-1114,https://arxiv.org/abs/cs/0504074,"('Yea Seul Kim', 'Jessica Hullman', 'Matthew Burgess', 'Eytan Adar')",11,SimpleScience: Lexical Simplification of Scientific Terminology,EMNLP,2016
"Automatic essay scoring (AES) refers to the process of scoring free text responses to given prompts, considering human grader scores as the gold standard. Writing such essays is an essential component of many language and aptitude exams. Hence, AES became an active and established area of research, and there are many proprietary systems used in real life applications today. However, not much is known about which specific linguistic features are useful for prediction and how much of this is consistent across datasets. This article addresses that by exploring the role of various linguistic features in automatic essay scoring using two publicly available datasets of non-native English essays written in test taking scenarios. The linguistic properties are modeled by encoding lexical, syntactic, discourse and error types of learner language in the feature set. Predictive models are then developed using these features on both datasets and the most predictive features are compared. While the results show that the feature set used results in good predictive models with both datasets, the question ""what are the most predictive features?"" has a different answer for each dataset.",http://aclweb.org/anthology/D16-1115,D16-1,D16-1115,https://arxiv.org/abs/1612.00729,"('Fei Dong', 'Yue Zhang')",11,Automatic Features for Essay Scoring – An Empirical Study,EMNLP,2016
We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms.,http://aclweb.org/anthology/D16-1116,D16-1,D16-1116,https://arxiv.org/abs/1609.09315,"('Tomáš Kočiský', 'Gábor Melis', 'Edward Grefenstette', 'Chris Dyer', 'Wang Ling', 'Phil Blunsom', 'Karl Moritz Hermann')",11,Semantic Parsing with Semi-Supervised Sequential Autoencoders,EMNLP,2016
"Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing -- given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in $70\%$ of the cases. In $60\%$ of the time, it also identifies the correct noun phrase $\rightarrow$ variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation.",http://aclweb.org/anthology/D16-1117,D16-1,D16-1117,https://arxiv.org/abs/1609.08824,"('Subhro Roy', 'Shyam Upadhyay', 'Dan Roth')",11,Equation Parsing : Mapping Sentences to Grounded Equations,EMNLP,2016
"Propositional term modal logic is interpreted over Kripke structures with unboundedly many accessibility relations and hence the syntax admits variables indexing modalities and quantification over them. This logic is undecidable, and we consider a variable-free propositional bi-modal logic with implicit quantification. Thus $[\forall] \alpha$ asserts necessity over all accessibility relations and $[\exists]\alpha$ is classical necessity over some accessibility relation. The logic is associated with a natural bisimulation relation over models and we show that the logic is exactly the bisimulation invariant fragment of a two sorted first order logic. The logic is easily seen to be decidable and admits a complete axiomatization of valid formulas. Moreover the decision procedure extends naturally to the `bundled fragment' of full term modal logic.",http://aclweb.org/anthology/D16-1118,D16-1,D16-1118,https://arxiv.org/abs/1811.09454,"('Jordan Sanders', 'Eduardo Blanco')",11,Automatic Extraction of Implicit Interpretations from Modal Constructions,EMNLP,2016
"This paper describes the resource- and system-building efforts of an eight-week Johns Hopkins University Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation (SIMT). We describe a new modality/negation (MN) annotation scheme, the creation of a (publicly available) MN lexicon, and two automated MN taggers that we built using the annotation scheme and lexicon. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation) and a holder (an experiencer of modality). We describe how our MN lexicon was semi-automatically produced and we demonstrate that a structure-based MN tagger results in precision around 86% (depending on genre) for tagging of a standard LDC data set.   We apply our MN annotation scheme to statistical machine translation using a syntactic framework that supports the inclusion of semantic annotations. Syntactic tags enriched with semantic annotations are assigned to parse trees in the target-language training texts through a process of tree grafting. While the focus of our work is modality and negation, the tree grafting procedure is general and supports other types of semantic information. We exploit this capability by including named entities, produced by a pre-existing tagger, in addition to the MN elements produced by the taggers described in this paper. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English test set. This finding supports the hypothesis that both syntactic and semantic information can improve translation quality.",http://aclweb.org/anthology/D16-1119,D16-1,D16-1119,https://arxiv.org/abs/1502.01682,"('Zahra Sarabi', 'Eduardo Blanco')",11,Understanding Negation in Positive Terms Using Syntactic Dependencies,EMNLP,2016
,http://aclweb.org/anthology/D16-1120,D16-1,D16-1120,,"('Su Lin Blodgett', 'Lisa Green', ""Brendan O'Connor"")",11,"
      Demographic Dialectal Variation in Social Media: A Case Study of
      African-American English
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1121,D16-1,D16-1121,,"('Koustav Rudra', 'Shruti Rijhwani', 'Rafiya Begum', 'Kalika Bali', 'Monojit Choudhury', 'Niloy Ganguly')",11,"
      Understanding Language Preference for Expression of Opinion and Sentiment:
      What do Hindi-English Speakers do on Twitter?
    ",EMNLP,2016
"This paper shows that characterizing co-occurrence between events is an important but non-trivial and neglected aspect of discovering potential causal relationships in multimedia event streams. First an introduction to the notion of event co-occurrence and its relation to co-occurrence pattern detection is given. Then a finite state automaton extended with a time model and event parameterization is introduced to convert high level co-occurrence pattern definition to its corresponding pattern matching automaton. Finally a processing algorithm is applied to count the occurrence frequency of a collection of patterns with only one pass through input event streams. The method proposed in this paper can be used for detecting co-occurrences between both events of one event stream (Auto co-occurrence), and events from multiple event streams (Cross co-occurrence). Some fundamental results concerning the characterization of event co-occurrence are presented in form of a visual co- occurrence matrix. Reusable causality rules can be extracted easily from co-occurrence matrix and fed into various analysis tools, such as recommendation systems and complex event processing systems for further analysis.",http://aclweb.org/anthology/D16-1122,D16-1,D16-1122,https://arxiv.org/abs/1603.09012,"('Allison Chaney', 'Hanna Wallach', 'Matthew Connelly', 'David Blei')",11,Detecting and Characterizing Events,EMNLP,2016
"We introduce a class of convolutional neural networks (CNNs) that utilize recurrent neural networks (RNNs) as convolution filters. A convolution filter is typically implemented as a linear affine transformation followed by a non-linear function, which fails to account for language compositionality. As a result, it limits the use of high-order filters that are often warranted for natural language processing tasks. In this work, we model convolution filters with RNNs that naturally capture compositionality and long-term dependencies in language. We show that simple CNN architectures equipped with recurrent neural filters (RNFs) achieve results that are on par with the best published ones on the Stanford Sentiment Treebank and two answer sentence selection datasets.",http://aclweb.org/anthology/D16-1123,D16-1,D16-1123,https://arxiv.org/abs/1808.09315,"('Ngoc-Quan Pham', 'Germán Kruszewski', 'Gemma Boleda')",11,Convolutional Neural Network Language Models,EMNLP,2016
"Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.",http://aclweb.org/anthology/D16-1124,D16-1,D16-1124,https://arxiv.org/abs/1606.00499,"('Graham Neubig', 'Chris Dyer')",11,Generalizing and Hybridizing Count-based and Neural Language Models,EMNLP,2016
"We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural ""listener"" and ""speaker"" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques.",http://aclweb.org/anthology/D16-1125,D16-1,D16-1125,https://arxiv.org/abs/1604.00562,"('Jacob Andreas', 'Dan Klein')",11,Reasoning about Pragmatics with Neural Listeners and Speakers,EMNLP,2016
"With the recent advances of neural models and natural language processing, automatic generation of classical Chinese poetry has drawn significant attention due to its artistic and cultural value. Previous works mainly focus on generating poetry given keywords or other text information, while visual inspirations for poetry have been rarely explored. Generating poetry from images is much more challenging than generating poetry from text, since images contain very rich visual information which cannot be described completely using several keywords, and a good poem should convey the image accurately. In this paper, we propose a memory based neural model which exploits images to generate poems. Specifically, an Encoder-Decoder model with a topic memory network is proposed to generate classical Chinese poetry from images. To the best of our knowledge, this is the first work attempting to generate classical Chinese poetry from images with neural networks. A comprehensive experimental investigation with both human evaluation and quantitative analysis demonstrates that the proposed model can generate poems which convey images accurately.",http://aclweb.org/anthology/D16-1126,D16-1,D16-1126,https://arxiv.org/abs/1803.02994,"('Marjan Ghazvininejad', 'Xing Shi', 'Yejin Choi', 'Kevin Knight')",11,Generating Topical Poetry,EMNLP,2016
"Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.",http://aclweb.org/anthology/D16-1127,D16-1,D16-1127,https://arxiv.org/abs/1606.01541,"('Jiwei Li', 'Will Monroe', 'Alan Ritter', 'Dan Jurafsky', 'Michel Galley', 'Jianfeng Gao')",11,Deep Reinforcement Learning for Dialogue Generation,EMNLP,2016
,http://aclweb.org/anthology/D16-1128,D16-1,D16-1128,,"('Rémi Lebret', 'David Grangier', 'Michael Auli')",11,"
      Neural Text Generation from Structured Data with Application to the
      Biography Domain
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1129,D16-1,D16-1129,,"('Ivan Habernal', 'Iryna Gurevych')",11,"
      What makes a convincing argument? Empirical analysis and detecting
      attributes of convincingness in Web argumentation
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1130,D16-1,D16-1130,,"('Yang Liu', 'Sujian Li')",11,"
      Recognizing Implicit Discourse Relations via Repeated Reading: Neural
      Networks with Multi-Level Attention
    ",EMNLP,2016
"Deep neural network models for Chinese zero pronoun resolution learn semantic information for zero pronoun and candidate antecedents, but tend to be short-sighted---they often make local decisions. They typically predict coreference chains between the zero pronoun and one single candidate antecedent one link at a time, while overlooking their long-term influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is critical when later predicting zero pronoun-candidate antecedent pairs. In this study, we show how to integrate local and global decision-making by exploiting deep reinforcement learning models. With the help of the reinforcement learning agent, our model learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 dataset show that our technique surpasses the state-of-the-art models.",http://aclweb.org/anthology/D16-1131,D16-1,D16-1131,https://arxiv.org/abs/1806.03711,"('Pranav Anand', 'Daniel Hardt')",11,Antecedent Selection for Sluicing: Structure and Content,EMNLP,2016
,http://aclweb.org/anthology/D16-1132,D16-1,D16-1132,,"('Ryu Iida', 'Kentaro Torisawa', 'Jong-Hoon Oh', 'Canasai Kruengkrai', 'Julien Kloetzer')",11,"
      Intra-Sentential Subject Zero Anaphora Resolution using Multi-Column
      Convolutional Neural Network
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1133,D16-1,D16-1133,,"('Antonios Anastasopoulos', 'David Chiang', 'Long Duong')",11,"
      An Unsupervised Probability Model for Speech-to-Translation Alignment of
      Low-Resource Languages
    ",EMNLP,2016
"Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.",http://aclweb.org/anthology/D16-1134,D16-1,D16-1134,https://arxiv.org/abs/1808.07048,"('Alexandra Birch', 'Omri Abend', 'Ondřej Bojar', 'Barry Haddow')",11,HUME: Human UCCA-Based Evaluation of Machine Translation,EMNLP,2016
,http://aclweb.org/anthology/D16-1135,D16-1,D16-1135,,"('Jian Ni', 'Radu Florian')",11,"
      Improving Multilingual Named Entity Recognition with Wikipedia Entity Type
      Mapping
    ",EMNLP,2016
"Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.",http://aclweb.org/anthology/D16-1136,D16-1,D16-1136,https://arxiv.org/abs/1606.09403,"('Long Duong', 'Hiroshi Kanayama', 'Tengfei Ma', 'Steven Bird', 'Trevor Cohn')",11,Learning Crosslingual Word Embeddings without Bilingual Corpora,EMNLP,2016
"We introduce several new black-box reductions that significantly improve the design of adaptive and parameter-free online learning algorithms by simplifying analysis, improving regret guarantees, and sometimes even improving runtime. We reduce parameter-free online learning to online exp-concave optimization, we reduce optimization in a Banach space to one-dimensional optimization, and we reduce optimization over a constrained domain to unconstrained optimization. All of our reductions run as fast as online gradient descent. We use our new techniques to improve upon the previously best regret bounds for parameter-free learning, and do so for arbitrary norms.",http://aclweb.org/anthology/D16-1137,D16-1,D16-1137,https://arxiv.org/abs/1802.06293,"('Sam Wiseman', 'Alexander M. Rush')",11,Sequence-to-Sequence Learning as Beam-Search Optimization,EMNLP,2016
"We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.",http://aclweb.org/anthology/D16-1138,D16-1,D16-1138,https://arxiv.org/abs/1609.08194,"('Lei Yu', 'Jan Buys', 'Phil Blunsom')",11,Online Segment to Segment Neural Transduction,EMNLP,2016
"Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.",http://aclweb.org/anthology/D16-1139,D16-1,D16-1139,https://arxiv.org/abs/1606.07947,"('Yoon Kim', 'Alexander M. Rush')",11,Sequence-Level Knowledge Distillation,EMNLP,2016
"Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task.",http://aclweb.org/anthology/D16-1140,D16-1,D16-1140,https://arxiv.org/abs/1609.09552,"('Yuta Kikuchi', 'Graham Neubig', 'Ryohei Sasano', 'Hiroya Takamura', 'Manabu Okumura')",11,Controlling Output Length in Neural Encoder-Decoders,EMNLP,2016
,http://aclweb.org/anthology/D16-1141,D16-1,D16-1141,,"('Cole Peterson', 'Alona Fyshe')",11,"
      Poet Admits // Mute Cypher: Beam Search to find Mutually Enciphering
      Poetic Texts
    ",EMNLP,2016
"Research accomplishment is usually measured by considering all citations with equal importance, thus ignoring the wide variety of purposes an article is being cited for. Here, we posit that measuring the intensity of a reference is crucial not only to perceive better understanding of research endeavor, but also to improve the quality of citation-based applications. To this end, we collect a rich annotated dataset with references labeled by the intensity, and propose a novel graph-based semi-supervised model, GraLap to label the intensity of references. Experiments with AAN datasets show a significant improvement compared to the baselines to achieve the true labels of the references (46% better correlation). Finally, we provide four applications to demonstrate how the knowledge of reference intensity leads to design better real-world applications.",http://aclweb.org/anthology/D16-1142,D16-1,D16-1142,https://arxiv.org/abs/1609.00081,"('Tanmoy Chakraborty', 'Ramasuri Narayanam')",11,All Fingers are not Equal: Intensity of References in Scientific Articles,EMNLP,2016
"With the growth of user-generated content, we observe the constant rise of the number of companies, such as search engines, content aggregators, etc., that operate with tremendous amounts of web content not being the services hosting it. Thus, aiming to locate the most important content and promote it to the users, they face the need of estimating the current and predicting the future content popularity.   In this paper, we approach the problem of video popularity prediction not from the side of a video hosting service, as done in all previous studies, but from the side of an operating company, which provides a popular video search service that aggregates content from different video hosting websites. We investigate video popularity prediction based on features from three primary sources available for a typical operating company: first, the content hosting provider may deliver its data via its API, second, the operating company makes use of its own search and browsing logs, third, the company crawls information about embeds of a video and links to a video page from publicly available resources on the Web. We show that video popularity prediction based on the embed and link data coupled with the internal search and browsing data significantly improves video popularity prediction based only on the data provided by the video hosting and can even adequately replace the API data in the cases when it is partly or completely unavailable.",http://aclweb.org/anthology/D16-1143,D16-1,D16-1143,https://arxiv.org/abs/1611.09083,"('Yuan Wang', 'Yang Xiao', 'Chao Ma', 'Zhen Xiao')",11,Improving Users' Demographic Prediction via the Videos They Talk about,EMNLP,2016
,http://aclweb.org/anthology/D16-1144,D16-1,D16-1144,,"('Xiang Ren', 'Wenqi He', 'Meng Qu', 'Lifu Huang', 'Heng Ji', 'Jiawei Han')",11,"
      AFET: Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label
      Embedding
    ",EMNLP,2016
"We introduce tools for inference in the multifractal random walk introduced by Bacry et al. (2001). These tools include formulas for smoothing, filtering and volatility forecasting. In addition, we present methods for computing conditional densities for one- and multi-step returns. The inference techniques presented in this paper, including maximum likelihood estimation, are applied to data from the Oslo Stock Exchange, and it is observed that the volatility forecasts based on the multifractal random walk have a much richer structure than the forecasts obtained from a basic stochastic volatility model.",http://aclweb.org/anthology/D16-1145,D16-1,D16-1145,https://arxiv.org/abs/1202.5376,"('Zhuoyu Wei', 'Jun Zhao', 'Kang Liu')",11,Mining Inference Formulas by Goal-Directed Random Walks,EMNLP,2016
"Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.",http://aclweb.org/anthology/D16-1146,D16-1,D16-1146,https://arxiv.org/abs/1606.08359,"('Thomas Demeester', 'Tim Rocktäschel', 'Sebastian Riedel')",11,Lifted Rule Injection for Relation Embeddings,EMNLP,2016
"Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark.",http://aclweb.org/anthology/D16-1147,D16-1,D16-1147,https://arxiv.org/abs/1606.03126,"('Alexander Miller', 'Adam Fisch', 'Jesse Dodge', 'Amir-Hossein Karimi', 'Antoine Bordes', 'Jason Weston')",11,Key-Value Memory Networks for Directly Reading Documents,EMNLP,2016
"Snapshot compressed sensing (CS) refers to compressive imaging systems where multiple frames are mapped into a single measurement frame. Each pixel in the acquired frame is a linear combination of the corresponding pixels in the frames that are collapsed together. While the problem can be cast as a CS problem, due to the very special structure of the sensing matrix, standard CS theory cannot be employed to study such systems. In this paper, a compression-based framework is proposed that enable a theoretical analysis of snapshot compressive sensing systems. This new framework leads to two novel, computationally-efficient and theoretically-analyzable snapshot compressive sensing recovery algorithms. The proposed algorithms are iterative and employ compression codes to impose structure on the recovered frames. Theoretical convergence guarantees are derived for both algorithms. In simulations, it is shown that combining the proposed algorithms with a customized video compression code designed to exploit nonlocal structures of video frames significantly improves the state-of-the-art performance.",http://aclweb.org/anthology/D16-1148,D16-1,D16-1148,https://arxiv.org/abs/1808.03661,"('Dallas Card', 'Justin Gross', 'Amber Boydstun', 'Noah A. Smith')",11,Analyzing Framing through the Casts of Characters in the News,EMNLP,2016
"Spoken dialogue systems promise efficient and natural access to a large variety of information sources and services from any phone. However, current spoken dialogue systems are deficient in their strategies for preventing, identifying and repairing problems that arise in the conversation. This paper reports results on automatically training a Problematic Dialogue Predictor to predict problematic human-computer dialogues using a corpus of 4692 dialogues collected with the 'How May I Help You' (SM) spoken dialogue system. The Problematic Dialogue Predictor can be immediately applied to the system's decision of whether to transfer the call to a human customer care agent, or be used as a cue to the system's dialogue manager to modify its behavior to repair problems, and even perhaps, to prevent them. We show that a Problematic Dialogue Predictor using automatically-obtainable features from the first two exchanges in the dialogue can predict problematic dialogues 13.2% more accurately than the baseline.",http://aclweb.org/anthology/D16-1149,D16-1,D16-1149,https://arxiv.org/abs/1106.1817,"('Diane Litman', 'Susannah Paletz', 'Zahra Rahimi', 'Stefani Allegretti', 'Caitlin Rice')",11,The Teams Corpus and Entrainment in Multi-Party Spoken Dialogues,EMNLP,2016
"In this paper, we present a study on personalized emphasis framing which can be used to tailor the content of a message to enhance its appeal to different individuals. With this framework, we directly model content selection decisions based on a set of psychologically-motivated domain-independent personal traits including personality (e.g., extraversion and conscientiousness) and basic human values (e.g., self-transcendence and hedonism). We also demonstrate how the analysis results can be used in automated personalized content selection for persuasive message generation.",http://aclweb.org/anthology/D16-1150,D16-1,D16-1150,https://arxiv.org/abs/1607.08898,"('Tao Ding', 'Shimei Pan')",11,Personalized Emphasis Framing for Persuasive Message Generation,EMNLP,2016
"We present a framework that couples the syntax and semantics of natural language sentences in a generative model, in order to develop a semantic parser that jointly infers the syntactic, morphological, and semantic representations of a given sentence under the guidance of background knowledge. To generate a sentence in our framework, a semantic statement is first sampled from a prior, such as from a set of beliefs in a knowledge base. Given this semantic statement, a grammar probabilistically generates the output sentence. A joint semantic-syntactic parser is derived that returns the $k$-best semantic and syntactic parses for a given sentence. The semantic prior is flexible, and can be used to incorporate background knowledge during parsing, in ways unlike previous semantic parsing approaches. For example, semantic statements corresponding to beliefs in a knowledge base can be given higher prior probability, type-correct statements can be given somewhat lower probability, and beliefs outside the knowledge base can be given lower probability. The construction of our grammar invokes a novel application of hierarchical Dirichlet processes (HDPs), which in turn, requires a novel and efficient inference approach. We present experimental results showing, for a simple grammar, that our parser outperforms a state-of-the-art CCG semantic parser and scales to knowledge bases with millions of beliefs.",http://aclweb.org/anthology/D16-1151,D16-1,D16-1151,https://arxiv.org/abs/1606.06361,"('Samuel Louvan', 'Chetan Naik', 'Sadhana Kumaravel', 'Heeyoung Kwon', 'Niranjan Balasubramanian', 'Peter Clark')",11,Cross Sentence Inference for Process Knowledge,EMNLP,2016
,http://aclweb.org/anthology/D16-1152,D16-1,D16-1152,,"('Yi Yang', 'Ming-Wei Chang', 'Jacob Eisenstein')",11,"
      Toward Socially-Infused Information Extraction: Embedding Authors,
      Mentions, and Entities
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1153,D16-1,D16-1153,,"('Akash Bharadwaj', 'David Mortensen', 'Chris Dyer', 'Jaime Carbonell')",11,"
      Phonologically Aware Neural Model for Named Entity Recognition in Low
      Resource Transfer Settings
    ",EMNLP,2016
"Language models, being at the heart of many NLP problems, are always of great interest to researchers. Neural language models come with the advantage of distributed representations and long range contexts. With its particular dynamics that allow the cycling of information within the network, `Recurrent neural network' (RNN) becomes an ideal paradigm for neural language modeling. Long Short-Term Memory (LSTM) architecture solves the inadequacies of the standard RNN in modeling long-range contexts. In spite of a plethora of RNN variants, possibility to add multiple memory cells in LSTM nodes was seldom explored. Here we propose a multi-cell node architecture for LSTMs and study its applicability for neural language modeling. The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup.",http://aclweb.org/anthology/D16-1154,D16-1,D16-1154,https://arxiv.org/abs/1811.06477,"('Youssef Oualil', 'Mittul Singh', 'Clayton Greenberg', 'Dietrich Klakow')",11,Long-Short Range Context Neural Networks for Language Modeling,EMNLP,2016
,http://aclweb.org/anthology/D16-1155,D16-1,D16-1155,,"('Changsong Liu', 'Shaohua Yang', 'Sari Saba-Sadiya', 'Nishant Shukla', 'Yunzhong He', 'Song-chun Zhu', 'Joyce Chai')",11,"
      Jointly Learning Grounded Task Structures from Language Instruction and
      Visual Demonstration
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1156,D16-1,D16-1156,,"('Gordon Christie', 'Ankit Laddha', 'Aishwarya Agrawal', 'Stanislaw Antol', 'Yash Goyal', 'Kevin Kochersberger', 'Dhruv Batra')",11,"
      Resolving Language and Vision Ambiguities Together: Joint Segmentation
      & Prepositional Attachment Resolution in Captioned Scenes
    ",EMNLP,2016
"We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.",http://aclweb.org/anthology/D16-1157,D16-1,D16-1157,https://arxiv.org/abs/1607.02789,"('John Wieting', 'Mohit Bansal', 'Kevin Gimpel', 'Karen Livescu')",11,Charagram: Embedding Words and Sentences via Character n-grams,EMNLP,2016
"Encoder-decoder networks are popular for modeling sequences probabilistically in many applications. These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables, unlike earlier models like CRFs that typically assumed conditional independence among non-adjacent variables. However in practice encoder-decoder models exhibit a bias towards short sequences that surprisingly gets worse with increasing beam size.   In this paper we show that such phenomenon is due to a discrepancy between the full sequence margin and the per-element margin enforced by the locally conditioned training objective of a encoder-decoder model. The discrepancy more adversely impacts long sequences, explaining the bias towards predicting short sequences.   For the case where the predicted sequences come from a closed set, we show that a globally conditioned model alleviates the above problems of encoder-decoder models. From a practical point of view, our proposed model also eliminates the need for a beam-search during inference, which reduces to an efficient dot-product based search in a vector-space.",http://aclweb.org/anthology/D16-1158,D16-1,D16-1158,https://arxiv.org/abs/1606.03402,"('Pavel Sountsov', 'Sunita Sarawagi')",11,Length bias in Encoder Decoder Models and a Case for Global Conditioning,EMNLP,2016
"Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task.",http://aclweb.org/anthology/D16-1159,D16-1,D16-1159,https://arxiv.org/abs/1603.00810,"('Xing Shi', 'Inkit Padhi', 'Kevin Knight')",11,Does String-Based Neural MT Learn Source Syntax?,EMNLP,2016
"While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.",http://aclweb.org/anthology/D16-1160,D16-1,D16-1160,https://arxiv.org/abs/1606.04596,"('Jiajun Zhang', 'Chengqing Zong')",11,Exploiting Source-side Monolingual Data in Neural Machine Translation,EMNLP,2016
,http://aclweb.org/anthology/D16-1161,D16-1,D16-1161,,"('Marcin Junczys-Dowmunt', 'Roman Grundkiewicz')",11,"
      Phrase-based Machine Translation is State-of-the-Art for Automatic
      Grammatical Error Correction
    ",EMNLP,2016
"Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.",http://aclweb.org/anthology/D16-1162,D16-1,D16-1162,https://arxiv.org/abs/1606.02006,"('Philip Arthur', 'Graham Neubig', 'Satoshi Nakamura')",11,Incorporating Discrete Translation Lexicons into Neural Machine Translation,EMNLP,2016
"Transfer learning has been proven as an effective technique for neural machine translation under low-resource conditions. Existing methods require a common target language, language relatedness, or specific training tricks and regimes. We present a simple transfer learning method, where we first train a ""parent"" model for a high-resource language pair and then continue the training on a lowresource pair only by replacing the training corpus. This ""child"" model performs significantly better than the baseline trained for lowresource pair only. We are the first to show this for targeting different languages, and we observe the improvements even for unrelated languages with different alphabets.",http://aclweb.org/anthology/D16-1163,D16-1,D16-1163,https://arxiv.org/abs/1809.00357,"('Barret Zoph', 'Deniz Yuret', 'Jonathan May', 'Kevin Knight')",11,Transfer Learning for Low-Resource Neural Machine Translation,EMNLP,2016
"Personal and private Web archives are proliferating due to the increase in the tools to create them and the realization that Internet Archive and other public Web archives are unable to capture personalized (e.g., Facebook) and private (e.g., banking) Web pages. We introduce a framework to mitigate issues of aggregation in private, personal, and public Web archives without compromising potential sensitive information contained in private captures. We amend Memento syntax and semantics to allow TimeMap enrichment to account for additional attributes to be expressed inclusive of the requirements for dereferencing private Web archive captures. We provide a method to involve the user further in the negotiation of archival captures in dimensions beyond time. We introduce a model for archival querying precedence and short-circuiting, as needed when aggregating private and personal Web archive captures with those from public Web archives through Memento. Negotiation of this sort is novel to Web archiving and allows for the more seamless aggregation of various types of Web archives to convey a more accurate picture of the past Web.",http://aclweb.org/anthology/D16-1164,D16-1,D16-1164,https://arxiv.org/abs/1806.00871,['Deepak P'],11,MixKMeans: Clustering Question-Answer Archives,EMNLP,2016
,http://aclweb.org/anthology/D16-1165,D16-1,D16-1165,,"('Preslav Nakov', 'Lluís Màrquez', 'Francisco Guzmán')",11,"
      It Takes Three to Tango: Triangulation Approach to Answer Ranking in
      Community Question Answering
    ",EMNLP,2016
"Video Question Answering is a challenging problem in visual information retrieval, which provides the answer to the referenced video content according to the question. However, the existing visual question answering approaches mainly tackle the problem of static image question, which may be ineffectively for video question answering due to the insufficiency of modeling the temporal dynamics of video contents. In this paper, we study the problem of video question answering by modeling its temporal dynamics with frame-level attention mechanism. We propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering. We then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance. We construct a large-scale video question answering dataset. We conduct the experiments on both multiple-choice and open-ended video question answering tasks to show the effectiveness of the proposed method.",http://aclweb.org/anthology/D16-1166,D16-1,D16-1166,https://arxiv.org/abs/1707.06355,"('Xiaodong He', 'David Golub')",11,Character-Level Question Answering with Attention,EMNLP,2016
"A vast amount of textual web streams is influenced by events or phenomena emerging in the real world. The social web forms an excellent modern paradigm, where unstructured user generated content is published on a regular basis and in most occasions is freely distributed. The present Ph.D. Thesis deals with the problem of inferring information - or patterns in general - about events emerging in real life based on the contents of this textual stream. We show that it is possible to extract valuable information about social phenomena, such as an epidemic or even rainfall rates, by automatic analysis of the content published in Social Media, and in particular Twitter, using Statistical Machine Learning methods. An important intermediate task regards the formation and identification of features which characterise a target event; we select and use those textual features in several linear, non-linear and hybrid inference approaches achieving a significantly good performance in terms of the applied loss function. By examining further this rich data set, we also propose methods for extracting various types of mood signals revealing how affective norms - at least within the social web's population - evolve during the day and how significant events emerging in the real world are influencing them. Lastly, we present some preliminary findings showing several spatiotemporal characteristics of this textual information as well as the potential of using it to tackle tasks such as the prediction of voting intentions.",http://aclweb.org/anthology/D16-1167,D16-1,D16-1167,https://arxiv.org/abs/1208.2873,"('Guillaume Bouchard', 'Pontus Stenetorp', 'Sebastian Riedel')",11,Learning to Generate Textual Data,EMNLP,2016
"We extend Borel's theorem on the dominance of word maps from semisimple algebraic groups to some perfect groups. In another direction, we generalize Borel's theorem to some words with constants. We also consider the surjectivity problem for particular words and groups, give a brief survey of recent results, present some generalizations and variations and discuss various approaches, with emphasis on new ideas, constructions and connections.",http://aclweb.org/anthology/D16-1168,D16-1,D16-1168,https://arxiv.org/abs/1801.00381,"('Rik Koncel-Kedziorski', 'Ioannis Konstas', 'Luke Zettlemoyer', 'Hannaneh Hajishirzi')",11,A Theme-Rewriting Approach for Generating Algebra Word Problems,EMNLP,2016
"With the advent of word embeddings, lexicons are no longer fully utilized for sentiment analysis although they still provide important features in the traditional setting. This paper introduces a novel approach to sentiment analysis that integrates lexicon embeddings and an attention mechanism into Convolutional Neural Networks. Our approach performs separate convolutions for word and lexicon embeddings and provides a global view of the document using attention. Our models are experimented on both the SemEval'16 Task 4 dataset and the Stanford Sentiment Treebank, and show comparative or better results against the existing state-of-the-art systems. Our analysis shows that lexicon embeddings allow to build high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for sentiment analysis.",http://aclweb.org/anthology/D16-1169,D16-1,D16-1169,https://arxiv.org/abs/1610.06272,"('Zhiyang Teng', 'Duy Tin Vo', 'Yue Zhang')",11,Context-Sensitive Lexicon Features for Neural Sentiment Analysis,EMNLP,2016
"We propose a graph-based mechanism to extract rich-emotion bearing patterns, which fosters a deeper analysis of online emotional expressions, from a corpus. The patterns are then enriched with word embeddings and evaluated through several emotion recognition tasks. Moreover, we conduct analysis on the emotion-oriented patterns to demonstrate its applicability and to explore its properties. Our experimental results demonstrate that the proposed techniques outperform most state-of-the-art emotion recognition techniques.",http://aclweb.org/anthology/D16-1170,D16-1,D16-1170,https://arxiv.org/abs/1804.08847,"('Lin Gui', 'Dongyin Wu', 'Ruifeng Xu', 'Qin Lu', 'Yu Zhou')",11,Event-Driven Emotion Cause Extraction with Corpus Construction,EMNLP,2016
"Neural network methods have achieved great success in reviews sentiment classification. Recently, some works achieved improvement by incorporating user and product information to generate a review representation. However, in reviews, we observe that some words or sentences show strong user's preference, and some others tend to indicate product's characteristic. The two kinds of information play different roles in determining the sentiment label of a review. Therefore, it is not reasonable to encode user and product information together into one representation. In this paper, we propose a novel framework to encode user and product information. Firstly, we apply two individual hierarchical neural networks to generate two representations, with user attention or with product attention. Then, we design a combined strategy to make full use of the two representations for training and final prediction. The experimental results show that our model obviously outperforms other state-of-the-art methods on IMDB and Yelp datasets. Through the visualization of attention over words related to user or product, we validate our observation mentioned above.",http://aclweb.org/anthology/D16-1171,D16-1,D16-1171,https://arxiv.org/abs/1801.07861,"('Huimin Chen', 'Maosong Sun', 'Cunchao Tu', 'Yankai Lin', 'Zhiyuan Liu')",11,Neural Sentiment Classification with User and Product Attention,EMNLP,2016
,http://aclweb.org/anthology/D16-1172,D16-1,D16-1172,,"('Jiacheng Xu', 'Danlu Chen', 'Xipeng Qiu', 'Xuanjing Huang')",11,"
      Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment
      Classification
    ",EMNLP,2016
"Understanding properties of deep neural networks is an important challenge in deep learning. In this paper, we take a step in this direction by proposing a rigorous way of verifying properties of a popular class of neural networks, Binarized Neural Networks, using the well-developed means of Boolean satisfiability. Our main contribution is a construction that creates a representation of a binarized neural network as a Boolean formula. Our encoding is the first exact Boolean representation of a deep neural network. Using this encoding, we leverage the power of modern SAT solvers along with a proposed counterexample-guided search procedure to verify various properties of these networks. A particular focus will be on the critical property of robustness to adversarial perturbations. For this property, our experimental results demonstrate that our approach scales to medium-size deep neural networks used in image classification tasks. To the best of our knowledge, this is the first work on verifying properties of deep neural networks using an exact Boolean encoding of the network.",http://aclweb.org/anthology/D16-1173,D16-1,D16-1173,https://arxiv.org/abs/1709.06662,"('Zhiting Hu', 'Zichao Yang', 'Ruslan Salakhutdinov', 'Eric Xing')",11,Deep Neural Networks with Massive Learned Knowledge,EMNLP,2016
"Semantic information is often represented as the entities and the relationships among them with conventional semantic models. This approach is straightforward but is not suitable for many posteriori requests in semantic data modeling. In this paper, we propose a meaning-oriented approach to modeling semantic data and establish a graph-based semantic data model. In this approach we use the meanings, i.e., the subjective views of the entities and relationships, to describe the semantic information, and use the semantic graphs containing the meaning nodes and the meta-meaning relations to specify the taxonomy and the compound construction of the semantic concepts. We demonstrate how this meaning-oriented approach can address many important semantic representation issues, including dynamic specialization and natural join.",http://aclweb.org/anthology/D16-1174,D16-1,D16-1174,https://arxiv.org/abs/1609.03346,"('Mohammad Taher Pilehvar', 'Nigel Collier')",11,De-Conflated Semantic Representations,EMNLP,2016
,http://aclweb.org/anthology/D16-1175,D16-1,D16-1175,,"('Thomas Kober', 'Julie Weeds', 'Jeremy Reffin', 'David Weir')",11,"
      Improving Sparse Word Representations with Distributional Inference for
      Semantic Composition
    ",EMNLP,2016
"Semantic matching of natural language sentences or identifying the relationship between two sentences is a core research problem underlying many natural language tasks. Depending on whether training data is available, prior research has proposed both unsupervised distance-based schemes and supervised deep learning schemes for sentence matching. However, previous approaches either omit or fail to fully utilize the ordered, hierarchical, and flexible structures of language objects, as well as the interactions between them. In this paper, we propose Hierarchical Sentence Factorization---a technique to factorize a sentence into a hierarchical representation, with the components at each different scale reordered into a ""predicate-argument"" form. The proposed sentence factorization technique leads to the invention of: 1) a new unsupervised distance metric which calculates the semantic distance between a pair of text snippets by solving a penalized optimal transport problem while preserving the logical relationship of words in the reordered sentences, and 2) new multi-scale deep learning models for supervised semantic training, based on factorized sentence hierarchies. We apply our techniques to text-pair similarity estimation and text-pair relationship classification tasks, based on multiple datasets such as STSbenchmark, the Microsoft Research paraphrase identification (MSRP) dataset, the SICK dataset, etc. Extensive experiments show that the proposed hierarchical sentence factorization can be used to significantly improve the performance of existing unsupervised distance-based metrics as well as multiple supervised deep learning models based on the convolutional neural network (CNN) and long short-term memory (LSTM).",http://aclweb.org/anthology/D16-1176,D16-1,D16-1176,https://arxiv.org/abs/1803.00179,"('Pengfei Liu', 'Xipeng Qiu', 'Yaqian Zhou', 'Jifan Chen', 'Xuanjing Huang')",11,Modelling Interaction of Sentence Pair with Coupled-LSTMs,EMNLP,2016
"The paper proposes the task of universal semantic tagging---tagging word tokens with language-neutral, semantically informative tags. We argue that the task, with its independent nature, contributes to better semantic analysis for wide-coverage multilingual text. We present the initial version of the semantic tagset and show that (a) the tags provide semantically fine-grained information, and (b) they are suitable for cross-lingual semantic parsing. An application of the semantic tagging in the Parallel Meaning Bank supports both of these points as the tags contribute to formal lexical semantics and their cross-lingual projection. As a part of the application, we annotate a small corpus with the semantic tags and present new baseline result for universal semantic tagging.",http://aclweb.org/anthology/D16-1177,D16-1,D16-1177,https://arxiv.org/abs/1709.10381,"('Aaron Steven White', 'Drew Reisinger', 'Keisuke Sakaguchi', 'Tim Vieira', 'Sheng Zhang', 'Rachel Rudinger', 'Kyle Rawlins', 'Benjamin Van Durme')",11,Universal Decompositional Semantics on Universal Dependencies,EMNLP,2016
"In an undirected social graph, a friendship link involves two users and the friendship is visible in both the users' friend lists. Such a dual visibility of the friendship may raise privacy threats. This is because both users can separately control the visibility of a friendship link to other users and their privacy policies for the link may not be consistent. Even if one of them conceals the link from a third user, the third user may find such a friendship link from another user's friend list. In addition, as most users allow their friends to see their friend lists in most social network systems, an adversary can exploit the inconsistent policies to launch privacy attacks to identify and infer many of a targeted user's friends. In this paper, we propose, analyze and evaluate such an attack which is called Friendship Identification and Inference (FII) attack. In a FII attack scenario, we assume that an adversary can only see his friend list and the friend lists of his friends who do not hide the friend lists from him. Then, a FII attack contains two attack steps: 1) friend identification and 2) friend inference. In the friend identification step, the adversary tries to identify a target's friends based on his friend list and those of his friends. In the friend inference step, the adversary attempts to infer the target's friends by using the proposed random walk with restart approach. We present experimental results using three real social network datasets and show that FII attacks are generally efficient and effective when adversaries and targets are friends or 2-distant neighbors. We also comprehensively analyze the attack results in order to find what values of parameters and network features could promote FII attacks. Currently, most popular social network systems with an undirected friendship graph, such as Facebook, LinkedIn and Foursquare, are susceptible to FII attacks.",http://aclweb.org/anthology/D16-1178,D16-1,D16-1178,https://arxiv.org/abs/1309.6204,"('Yanchuan Sim', 'Bryan Routledge', 'Noah A. Smith')",11,Friends with Motives: Using Text to Infer Influence on SCOTUS,EMNLP,2016
,http://aclweb.org/anthology/D16-1179,D16-1,D16-1179,,"('Kian Kenyon-Dean', 'Jackie Chi Kit Cheung', 'Doina Precup')",11,"
      Verb Phrase Ellipsis Resolution Using Discriminative and Margin-Infused
      Algorithms
    ",EMNLP,2016
"We introduce two first-order graph-based dependency parsers achieving a new state of the art. The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations. We cast this approach as minimum Bayes risk decoding (under the Hamming cost) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity. The second parser is a ""distillation"" of the ensemble into a single model. We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment, thereby avoiding the intractable cross-entropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German.",http://aclweb.org/anthology/D16-1180,D16-1,D16-1180,https://arxiv.org/abs/1609.07561,"('Adhiguna Kuncoro', 'Miguel Ballesteros', 'Lingpeng Kong', 'Chris Dyer', 'Noah A. Smith')",11,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,EMNLP,2016
"We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing.",http://aclweb.org/anthology/D16-1181,D16-1,D16-1181,https://arxiv.org/abs/1704.06936,['Wenduan Xu'],11,LSTM Shift-Reduce CCG Parsing,EMNLP,2016
"Robustness in a parser refers to an ability to deal with exceptional phenomena. A parser is robust if it deals with phenomena outside its normal range of inputs. This paper reports on a series of robustness evaluations of state-of-the-art parsers in which we concentrated on one aspect of robustness: its ability to parse sentences containing misspelled words. We propose two measures for robustness evaluation based on a comparison of a parser's output for grammatical input sentences and their noisy counterparts. In this paper, we use these measures to compare the overall robustness of the four evaluated parsers, and we present an analysis of the decline in parser performance with increasing error levels. Our results indicate that performance typically declines tens of percentage units when parsers are presented with texts containing misspellings. When it was tested on our purpose-built test set of 443 sentences, the best parser in the experiment (C&C parser) was able to return exactly the same parse tree for the grammatical and ungrammatical sentences for 60.8%, 34.0% and 14.9% of the sentences with one, two or three misspelled words respectively.",http://aclweb.org/anthology/D16-1182,D16-1,D16-1182,https://arxiv.org/abs/0801.3817,"('Homa B. Hashemi', 'Rebecca Hwa')",11,An Evaluation of Parser Robustness for Ungrammatical Sentences,EMNLP,2016
"Combinatory categorial grammar (CCG) is a grammar formalism used for natural language parsing. CCG assigns structured lexical categories to words and uses a small set of combinatory rules to combine these categories to parse a sentence. In this work we propose and implement a new approach to CCG parsing that relies on a prominent knowledge representation formalism, answer set programming (ASP) - a declarative programming paradigm. We formulate the task of CCG parsing as a planning problem and use an ASP computational tool to compute solutions that correspond to valid parses. Compared to other approaches, there is no need to implement a specific parsing algorithm using such a declarative method. Our approach aims at producing all semantically distinct parse trees for a given sentence. From this goal, normalization and efficiency issues arise, and we deal with them by combining and extending existing strategies. We have implemented a CCG parsing tool kit - AspCcgTk - that uses ASP as its main computational means. The C&C supertagger can be used as a preprocessor within AspCcgTk, which allows us to achieve wide-coverage natural language parsing.",http://aclweb.org/anthology/D16-1183,D16-1,D16-1183,https://arxiv.org/abs/1108.5567,"('Dipendra Kumar Misra', 'Yoav Artzi')",11,Neural Shift-Reduce CCG Semantic Parsing,EMNLP,2016
"A description and annotation guidelines for the Yahoo Webscope release of Query Treebank, Version 1.0, May 2016.",http://aclweb.org/anthology/D16-1184,D16-1,D16-1184,https://arxiv.org/abs/1605.02945,"('Xiangyan Sun', 'Haixun Wang', 'Yanghua Xiao', 'Zhongyuan Wang')",11,Syntactic Parsing of Web Queries,EMNLP,2016
"Inspired by the tremendous success of deep Convolutional Neural Networks as generic feature extractors for images, we propose TimeNet: a deep recurrent neural network (RNN) trained on diverse time series in an unsupervised manner using sequence to sequence (seq2seq) models to extract features from time series. Rather than relying on data from the problem domain, TimeNet attempts to generalize time series representation across domains by ingesting time series from several domains simultaneously. Once trained, TimeNet can be used as a generic off-the-shelf feature extractor for time series. The representations or embeddings given by a pre-trained TimeNet are found to be useful for time series classification (TSC). For several publicly available datasets from UCR TSC Archive and an industrial telematics sensor data from vehicles, we observe that a classifier learned over the TimeNet embeddings yields significantly better performance compared to (i) a classifier learned over the embeddings given by a domain-specific RNN, as well as (ii) a nearest neighbor classifier based on Dynamic Time Warping.",http://aclweb.org/anthology/D16-1185,D16-1,D16-1185,https://arxiv.org/abs/1706.08838,"('Hongliang Yu', 'Shikun Zhang', 'Louis-Philippe Morency')",11,Unsupervised Text Recap Extraction for TV Series,EMNLP,2016
,http://aclweb.org/anthology/D16-1186,D16-1,D16-1186,,"('Markus Dollmann', 'Michaela Geierhos')",11,"
      On- and Off-Topic Classification and Semantic Annotation of User-Generated
      Software Requirements
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1187,D16-1,D16-1187,,"('Zhen Hai', 'Peilin Zhao', 'Peng Cheng', 'Peng Yang', 'Xiao-Li Li', 'Guangxia Li')",11,"
      Deceptive Review Spam Detection via Exploiting Task Relatedness and
      Unlabeled Data
    ",EMNLP,2016
"Clustering a lexicon of words is a well-studied problem in natural language processing (NLP). Word clusters are used to deal with sparse data in statistical language processing, as well as features for solving various NLP tasks (text categorization, question answering, named entity recognition and others).   Spectral clustering is a widely used technique in the field of image processing and speech recognition. However, it has scarcely been explored in the context of NLP; specifically, the method used in this (Meila and Shi, 2001) has never been used to cluster a general word lexicon.   We apply spectral clustering to a lexicon of words, evaluating the resulting clusters by using them as features for solving two classical NLP tasks: semantic role labeling and dependency parsing. We compare performance with Brown clustering, a widely-used technique for word clustering, as well as with other clustering methods. We show that spectral clusters produce similar results to Brown clusters, and outperform other clustering methods. In addition, we quantify the overlap between spectral and Brown clusters, showing that each model captures some information which is uncaptured by the other.",http://aclweb.org/anthology/D16-1188,D16-1,D16-1188,https://arxiv.org/abs/1808.05374,"('Konstantinos Skianis', 'Francois Rousseau', 'Michalis Vazirgiannis')",11,Regularizing Text Categorization with Clusters of Words,EMNLP,2016
,http://aclweb.org/anthology/D16-1189,D16-1,D16-1189,,"('Ji He', 'Mari Ostendorf', 'Xiaodong He', 'Jianshu Chen', 'Jianfeng Gao', 'Lihong Li', 'Li Deng')",11,"
      Deep Reinforcement Learning with a Combinatorial Action Space for
      Predicting Popular Reddit Threads
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1190,D16-1,D16-1190,,"('Maria Moritz', 'Andreas Wiederhold', 'Barbara Pavlek', 'Yuri Bizzoni', 'Marco Büchler')",11,"
      Non-Literal Text Reuse in Historical Texts: An Approach to Identify Reuse
      Transformations and its Application to Bible Reuse
    ",EMNLP,2016
"Preliminary report on network based keyword extraction for Croatian is an unsupervised method for keyword extraction from the complex network. We build our approach with a new network measure the node selectivity, motivated by the research of the graph based centrality approaches. The node selectivity is defined as the average weight distribution on the links of the single node. We extract nodes (keyword candidates) based on the selectivity value. Furthermore, we expand extracted nodes to word-tuples ranked with the highest in/out selectivity values. Selectivity based extraction does not require linguistic knowledge while it is purely derived from statistical and structural information en-compassed in the source text which is reflected into the structure of the network. Obtained sets are evaluated on a manually annotated keywords: for the set of extracted keyword candidates average F1 score is 24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidates average F1 score is 25,9% and average F2 score is 24,47%.",http://aclweb.org/anthology/D16-1191,D16-1,D16-1191,https://arxiv.org/abs/1407.4723,"('Antoine Tixier', 'Fragkiskos Malliaros', 'Michalis Vazirgiannis')",11,A Graph Degeneracy-based Approach to Keyword Extraction,EMNLP,2016
,http://aclweb.org/anthology/D16-1192,D16-1,D16-1192,,"('Elliot Schumacher', 'Maxine Eskenazi', 'Gwen Frishkoff', 'Kevyn Collins-Thompson')",11,"
      Predicting the Relative Difficulty of Single Sentences With and Without
      Surrounding Context
    ",EMNLP,2016
"Grammatical error detection and automated essay scoring are two tasks in the area of automated assessment. Traditionally these tasks have been treated independently with different machine learning models and features used for each task. In this paper, we develop a multi-task neural network model that jointly optimises for both tasks, and in particular we show that neural automated essay scoring can be significantly improved. We show that while the essay score provides little evidence to inform grammatical error detection, the essay score is highly influenced by error detection.",http://aclweb.org/anthology/D16-1193,D16-1,D16-1193,https://arxiv.org/abs/1801.06830,"('Kaveh Taghipour', 'Hwee Tou Ng')",11,A Neural Approach to Automated Essay Scoring,EMNLP,2016
"Analyzing writing styles of non-native speakers is a challenging task. In this paper, we analyze the comments written in the discussion pages of the English Wikipedia. Using learning algorithms, we are able to detect native speakers' writing style with an accuracy of 74%. Given the diversity of the English Wikipedia users and the large number of languages they speak, we measure the similarities among their native languages by comparing the influence they have on their English writing style. Our results show that languages known to have the same origin and development path have similar footprint on their speakers' English writing style. To enable further studies, the dataset we extracted from Wikipedia will be made available publicly.",http://aclweb.org/anthology/D16-1194,D16-1,D16-1194,https://arxiv.org/abs/1211.0498,"('Weibo Wang', ""Abidalrahman Moh'd"", 'Aminul Islam', 'Axel Soto', 'Evangelos Milios')",11,Non-uniform Language Detection in Technical Writing,EMNLP,2016
,http://aclweb.org/anthology/D16-1195,D16-1,D16-1195,,"('Shamil Chollampatt', 'Duc Tam Hoang', 'Hwee Tou Ng')",11,"
      Adapting Grammatical Error Correction Based on the Native Language of
      Writers with Neural Network Joint Models
    ",EMNLP,2016
"We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora.",http://aclweb.org/anthology/D16-1196,D16-1,D16-1196,https://arxiv.org/abs/1610.00634,"('Anoop Kunchukuttan', 'Pushpak Bhattacharyya')",11,Orthographic Syllable as basic unit for SMT between Related Languages,EMNLP,2016
,http://aclweb.org/anthology/D16-1197,D16-1,D16-1197,,"('Nicholas Locascio', 'Karthik Narasimhan', 'Eduardo De Leon', 'Nate Kushman', 'Regina Barzilay')",11,"
      Neural Generation of Regular Expressions from Natural Language with
      Minimal Domain Knowledge
    ",EMNLP,2016
"Keyphrases are useful for a variety of purposes, including summarizing, indexing, labeling, categorizing, clustering, highlighting, browsing, and searching. The task of automatic keyphrase extraction is to select keyphrases from within the text of a given document. Automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. Good performance on this task has been obtained by approaching it as a supervised learning problem. An input document is treated as a set of candidate phrases that must be classified as either keyphrases or non-keyphrases. To classify a candidate phrase as a keyphrase, the most important features (attributes) appear to be the frequency and location of the candidate phrase in the document. Recent work has demonstrated that it is also useful to know the frequency of the candidate phrase as a manually assigned keyphrase for other documents in the same domain as the given document (e.g., the domain of computer science). Unfortunately, this keyphrase-frequency feature is domain-specific (the learning process must be repeated for each new domain) and training-intensive (good performance requires a relatively large number of training documents in the given domain, with manually assigned keyphrases). The aim of the work described here is to remove these limitations. In this paper, I introduce new features that are derived by mining lexical knowledge from a very large collection of unlabeled data, consisting of approximately 350 million Web pages without manually assigned keyphrases. I present experiments that show that the new features result in improved keyphrase extraction, although they are neither domain-specific nor training-intensive.",http://aclweb.org/anthology/D16-1198,D16-1,D16-1198,https://arxiv.org/abs/cs/0212011,"('Lucas Sterckx', 'Cornelia Caragea', 'Thomas Demeester', 'Chris Develder')",11,Supervised Keyphrase Extraction as Positive Unlabeled Learning,EMNLP,2016
"In this paper we present a question answering system using a neural network to interpret questions learned from the DBpedia repository. We train a sequence-to-sequence neural network model with n-triples extracted from the DBpedia Infobox Properties. Since these properties do not represent the natural language, we further used question-answer dialogues from movie subtitles. Although the automatic evaluation shows a low overlap of the generated answers compared to the gold standard set, a manual inspection of the showed promising outcomes from the experiment for further work.",http://aclweb.org/anthology/D16-1199,D16-1,D16-1199,https://arxiv.org/abs/1803.02914,"('Alvaro Morales', 'Varot Premtoon', 'Cordelia Avery', 'Sue Felshin', 'Boris Katz')",11,Learning to Answer Questions from Wikipedia Infoboxes,EMNLP,2016
"Distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data.",http://aclweb.org/anthology/D16-1200,D16-1,D16-1200,https://arxiv.org/abs/1509.03739,"('Savelie Cornegruta', 'Andreas Vlachos')",11,Timeline extraction using distant supervision and joint inference,EMNLP,2016
"We present results on combining supervised and unsupervised methods to ensemble multiple systems for two popular Knowledge Base Population (KBP) tasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery and Linking (TEDL). We demonstrate that our combined system along with auxiliary features outperforms the best performing system for both tasks in the 2015 competition, several ensembling baselines, as well as the state-of-the-art stacking approach to ensembling KBP systems. The success of our technique on two different and challenging problems demonstrates the power and generality of our combined approach to ensembling.",http://aclweb.org/anthology/D16-1201,D16-1,D16-1201,https://arxiv.org/abs/1604.04802,"('Nazneen Fatema Rajani', 'Raymond Mooney')",11,Combining Supervised and Unsupervised Enembles for Knowledge Base Population,EMNLP,2016
"We investigate a lattice-structured LSTM model for Chinese NER, which encodes a sequence of input characters as well as all potential words that match a lexicon. Compared with character-based methods, our model explicitly leverages word and word sequence information. Compared with word-based methods, lattice LSTM does not suffer from segmentation errors. Gated recurrent cells allow our model to choose the most relevant characters and words from a sentence for better NER results. Experiments on various datasets show that lattice LSTM outperforms both word-based and character-based LSTM baselines, achieving the best results.",http://aclweb.org/anthology/D16-1202,D16-1,D16-1202,https://arxiv.org/abs/1805.02023,"('Kazuya Kawakami', 'Chris Dyer', 'Bryan Routledge', 'Noah A. Smith')",11,Character Sequence Models for Colorful Words,EMNLP,2016
"Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.   Our behavior analysis reveals that despite recent progress, today's VQA models are ""myopic"" (tend to fail on sufficiently novel instances), often ""jump to conclusions"" (converge on a predicted answer after 'listening' to just half the question), and are ""stubborn"" (do not change their answers across images).",http://aclweb.org/anthology/D16-1203,D16-1,D16-1203,https://arxiv.org/abs/1606.07356,"('Aishwarya Agrawal', 'Dhruv Batra', 'Devi Parikh')",11,Analyzing the Behavior of Visual Question Answering Models,EMNLP,2016
,http://aclweb.org/anthology/D16-1204,D16-1,D16-1204,,"('Subhashini Venugopalan', 'Lisa Anne Hendricks', 'Raymond Mooney', 'Kate Saenko')",11,"
      Improving LSTM-based Video Description with Linguistic Knowledge Mined
      from Text
    ",EMNLP,2016
"Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus.",http://aclweb.org/anthology/D16-1205,D16-1,D16-1205,https://arxiv.org/abs/1607.02061,"('Emmanuele Chersoni', 'Enrico Santus', 'Alessandro Lenci', 'Philippe Blache', 'Chu-Ren Huang')",11,Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity,EMNLP,2016
,http://aclweb.org/anthology/D16-1206,D16-1,D16-1206,,"('Tim Vieira', 'Ryan Cotterell', 'Jason Eisner')",11,"
      Speed-Accuracy Tradeoffs in Tagging with Variable-Order CRFs and
      Structured Sparsity
    ",EMNLP,2016
"Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature computed globally from a whole image component (patch), where the cluttered background information may dominate true text features in the deep representation. This leads to less discriminative power and poorer robustness. In this work, we present a new system for scene text detection by proposing a novel Text-Attentional Convolutional Neural Network (Text-CNN) that particularly focuses on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text/nontext information. The rich supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where low-level supervised information greatly facilitates main task of text/non-text classification. In addition, a powerful low-level detector called Contrast- Enhancement Maximally Stable Extremal Regions (CE-MSERs) is developed, which extends the widely-used MSERs by enhancing intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2013 dataset, with a F-measure of 0.82, improving the state-of-the-art results substantially.",http://aclweb.org/anthology/D16-1207,D16-1,D16-1207,https://arxiv.org/abs/1510.03283,"('Yitong Li', 'Trevor Cohn', 'Timothy Baldwin')",11,Learning Robust Representations of Text,EMNLP,2016
,http://aclweb.org/anthology/D16-1208,D16-1,D16-1208,,['Kewei Tu'],11,"
      Modified Dirichlet Distribution: Allowing Negative Parameters to Induce
      Stronger Sparsity
    ",EMNLP,2016
"We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.",http://aclweb.org/anthology/D16-1209,D16-1,D16-1209,https://arxiv.org/abs/1612.06212,"('Yasumasa Miyamoto', 'Kyunghyun Cho')",11,Gated Word-Character Recurrent Language Model,EMNLP,2016
"We present a simple neural network for word alignment that builds source and target word window representations to compute alignment scores for sentence pairs. To enable unsupervised training, we use an aggregation operation that summarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on English-Czech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment.",http://aclweb.org/anthology/D16-1210,D16-1,D16-1210,https://arxiv.org/abs/1606.09560,"('Hidetaka Kamigaito', 'Akihiro Tamura', 'Hiroya Takamura', 'Manabu Okumura', 'Eiichiro Sumita')",11,Unsupervised Word Alignment by Agreement Under ITG Constraint,EMNLP,2016
"We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles(Goldberg and Nivre, 2013) instead of cross-entropy minimization. This form of training, which accounts for model predictions at training time rather than assuming an error-free action history, improves parsing accuracies for both English and Chinese, obtaining very strong results for both languages. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural-network.",http://aclweb.org/anthology/D16-1211,D16-1,D16-1211,https://arxiv.org/abs/1603.03793,"('Miguel Ballesteros', 'Yoav Goldberg', 'Chris Dyer', 'Noah A. Smith')",11,Training with Exploration Improves a Greedy Stack LSTM Parser,EMNLP,2016
"Semantic role labeling (SRL) is to recognize the predicate-argument structure of a sentence, including subtasks of predicate disambiguation and argument labeling. Previous studies usually formulate the entire SRL problem into two or more subtasks. For the first time, this paper introduces an end-to-end neural model which unifiedly tackles the predicate disambiguation and the argument labeling in one shot. Using a biaffine scorer, our model directly predicts all semantic role labels for all given word pairs in the sentence without relying on any syntactic parse information. Specifically, we augment the BiLSTM encoder with a non-linear transformation to further distinguish the predicate and the argument in a given sentence, and model the semantic role labeling process as a word pair classification task by employing the biaffine attentional mechanism. Though the proposed model is syntax-agnostic with local decoder, it outperforms the state-of-the-art syntax-aware SRL systems on the CoNLL-2008, 2009 benchmarks for both English and Chinese. To our best knowledge, we report the first syntax-agnostic SRL model that surpasses all known syntax-aware models.",http://aclweb.org/anthology/D16-1212,D16-1,D16-1212,https://arxiv.org/abs/1808.03815,"('Lei Sha', 'Sujian Li', 'Baobao Chang', 'Zhifang Sui', 'Tingsong Jiang')",11,Capturing Argument Relationship for Chinese Semantic Role Labeling,EMNLP,2016
"The modelling, specification and study of the semantics of concurrent reactive systems have been interesting research topics for many years now. The aim of this thesis is to exploit the strengths of the (co)algebraic framework in modelling reactive systems and reasoning on several types of associated semantics, in a uniform fashion. In particular, we are interested in handling notions of behavioural equivalence/preorder ranging from bisimilarity for systems that can be represented as non-deterministic coalgebras, to decorated trace semantics for labelled transition systems and probabilistic systems, and testing semantics for labelled transition systems with internal behaviour. Moreover, we aim at deriving a suite of corresponding verification algorithms suitable for implementation in automated tools.",http://aclweb.org/anthology/D16-1213,D16-1,D16-1213,https://arxiv.org/abs/1502.02910,"('Haoyan Xu', 'Brian Murphy', 'Alona Fyshe')",11,BrainBench: A Brain-Image Test Suite for Distributional Semantic Models,EMNLP,2016
"We compare the effectiveness of four different syntactic CCG parsers for a semantic slot-filling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation provides a unique window to explore the strengths and weaknesses of semantics captured by unsupervised grammar induction systems. We release a new Freebase semantic parsing dataset called SPADES (Semantic PArsing of DEclarative Sentences) containing 93K cloze-style questions paired with answers. We evaluate all our models on this dataset. Our code and data are available at https://github.com/sivareddyg/graph-parser.",http://aclweb.org/anthology/D16-1214,D16-1,D16-1214,https://arxiv.org/abs/1609.09405,"('Yonatan Bisk', 'Siva Reddy', 'John Blitzer', 'Julia Hockenmaier', 'Mark Steedman')",11,Evaluating Induced CCG Parsers on Grounded Semantic Parsing,EMNLP,2016
"An intuitive way for a human to write paraphrase sentences is to replace words or phrases in the original sentence with their corresponding synonyms and make necessary changes to ensure the new sentences are fluent and grammatically correct. We propose a novel approach to modeling the process with dictionary-guided editing networks which effectively conduct rewriting on the source sentence to generate paraphrase sentences. It jointly learns the selection of the appropriate word level and phrase level paraphrase pairs in the context of the original sentence from an off-the-shelf dictionary as well as the generation of fluent natural language sentences. Specifically, the system retrieves a set of word level and phrase level araphrased pairs derived from the Paraphrase Database (PPDB) for the original sentence, which is used to guide the decision of which the words might be deleted or inserted with the soft attention mechanism under the sequence-to-sequence framework. We conduct experiments on two benchmark datasets for paraphrase generation, namely the MSCOCO and Quora dataset. The evaluation results demonstrate that our dictionary-guided editing networks outperforms the baseline methods.",http://aclweb.org/anthology/D16-1215,D16-1,D16-1215,https://arxiv.org/abs/1806.08077,['Marianna Apidianaki'],11,Vector-space models for PPDB paraphrase ranking in context,EMNLP,2016
"We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks.",http://aclweb.org/anthology/D16-1216,D16-1,D16-1216,https://arxiv.org/abs/1610.02683,"('Malika Aubakirova', 'Mohit Bansal')",11,Interpreting Neural Networks to Improve Politeness Comprehension,EMNLP,2016
"The political requires a forum for its deliberation and Habermas has located it in the public spheres. Originally, mass media's role was one of a facilitator of these debates. However, under the immense pressures of free market competition and mobile audiences, mass media prefers episodic over thematic news. On the opposite end of the spectrum, social media has been heralded as a new forum, a reincarnation of the ailing public spheres to further the deliberation of the political. But do the followers of political parties in social media endorse thematic or episodic content?   To answer this question, I look at the most recent 3,200 tweets that were broadcast from the Republican and Democratic Twitter accounts. By employing Latent dirichlet allocation, I extract the prevailing topics of these tweets and linked websites. Generalized linear models are used to describe the relationship between episodicity, thematicity and the endorsement counts of the posts analyzed.   I find that there is a stark contrast between the behavior of Democratic and Republican followers. In general, there seems to be a slight preference for thematic messages. Interestingly, the distance to an election increases the odds of a message to be endorsed.",http://aclweb.org/anthology/D16-1217,D16-1,D16-1217,https://arxiv.org/abs/1408.6451,"('Laura Smith', 'Salvatore Giorgi', 'Rishi Solanki', 'Johannes Eichstaedt', 'H. Andrew Schwartz', 'Muhammad Abdul-Mageed', 'Anneke Buffone', 'Lyle Ungar')",11,Does ‘well-being’ translate on Twitter?,EMNLP,2016
"This analysis proposes a new topic model to study the yearly trends in Marvel Cinematic Universe fanfictions on three levels: character popularity, character images/topics, and vocabulary pattern of topics. It is found that character appearances in fanfictions have become more diverse over the years thanks to constant introduction of new characters in feature films, and in the case of Captain America, multi-dimensional character development is well-received by the fanfiction world.",http://aclweb.org/anthology/D16-1218,D16-1,D16-1218,https://arxiv.org/abs/1805.03774,"('Smitha Milli', 'David Bamman')",11,Beyond Canonical Texts: A Computational Analysis of Fanfiction,EMNLP,2016
,http://aclweb.org/anthology/D16-1219,D16-1,D16-1219,,"('Masoud Rouhizadeh', 'Lyle Ungar', 'Anneke Buffone', 'H. Andrew Schwartz')",11,"
      Using Syntactic and Semantic Context to Explore Psychodemographic
      Differences in Self-reference
    ",EMNLP,2016
"I present here an experimental system for identifying and annotating metaphor in corpora. It is designed to plug in to Metacorps, an experimental web app for annotating metaphor. As Metacorps users annotate metaphors, the system will use user annotations as training data. When the system is confident, it will suggest an identification and an annotation. Once approved by the user, this becomes more training data. This naturally allows for transfer learning, where the system can, with some known degree of reliability, classify one class of metaphor after only being trained on another class of metaphor. For example, in our metaphorical violence project, metaphors may be classified by the network they were observed on, the grammatical subject or object of the violence metaphor, or the violent word used (hit, attack, beat, etc.).",http://aclweb.org/anthology/D16-1220,D16-1,D16-1220,https://arxiv.org/abs/1810.08677,"('Gözde Özbal', 'Carlo Strapparava', 'Serra Sinem Tekiroglu', 'Daniele Pighin')",11,Learning to Identify Metaphors from a Corpus of Proverbs,EMNLP,2016
"Predicting how Congressional legislators will vote is important for understanding their past and future behavior. However, previous work on roll-call prediction has been limited to single session settings, thus did not consider generalization across sessions. In this paper, we show that metadata is crucial for modeling voting outcomes in new contexts, as changes between sessions lead to changes in the underlying data generation process. We show how augmenting bill text with the sponsors' ideologies in a neural network model can achieve an average of a 4% boost in accuracy over the previous state-of-the-art.",http://aclweb.org/anthology/D16-1221,D16-1,D16-1221,https://arxiv.org/abs/1805.08182,"('Peter Kraft', 'Hirsh Jain', 'Alexander M. Rush')",11,An Embedding Model for Predicting Roll-Call Votes,EMNLP,2016
"In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.",http://aclweb.org/anthology/D16-1222,D16-1,D16-1222,https://arxiv.org/abs/1511.04164,"('Young-Bum Kim', 'Alexandre Rochette', 'Ruhi Sarikaya')",11,Natural Language Model Re-usability for Scaling to Different Domains,EMNLP,2016
,http://aclweb.org/anthology/D16-1223,D16-1,D16-1223,,"('Gakuto Kurata', 'Bing Xiang', 'Bowen Zhou', 'Mo Yu')",11,"
      Leveraging Sentence-level Information with Encoder LSTM for Semantic Slot
      Filling
    ",EMNLP,2016
"In this short note, the dual problem for the traveling salesman problem is constructed through the classic Lagrangian. The existence of optimality conditions is expressed as a corresponding inverse problem. A general 4-cities instance is given, and the numerical experiment shows that the classic Lagrangian may not be applicable to the traveling salesman problem.",http://aclweb.org/anthology/D16-1224,D16-1,D16-1224,https://arxiv.org/abs/1405.1298,"('Linfeng Song', 'Yue Zhang', 'Xiaochang Peng', 'Zhiguo Wang', 'Daniel Gildea')",11,AMR-to-text generation as a Traveling Salesman Problem,EMNLP,2016
,http://aclweb.org/anthology/D16-1225,D16-1,D16-1225,,"('Raymond Hendy Susanto', 'Hai Leong Chieu', 'Wei Lu')",11,"
      Learning to Capitalize with Character-Level Recurrent Neural Networks: An
      Empirical Study
    ",EMNLP,2016
"Communication is now a standard tool in the central bank's monetary policy toolkit. Theoretically, communication provides the central bank an opportunity to guide public expectations, and it has been shown empirically that central bank communication can lead to financial market fluctuations. However, there has been little research into which dimensions or topics of information are most important in causing these fluctuations. We develop a semi-automatic methodology that summarizes the FOMC statements into its main themes, automatically selects the best model based on coherency, and assesses whether there is a significant impact of these themes on the shape of the U.S Treasury yield curve using topic modeling methods from the machine learning literature. Our findings suggest that the FOMC statements can be decomposed into three topics: (i) information related to the economic conditions and the mandates, (ii) information related to monetary policy tools and intermediate targets, and (iii) information related to financial markets and the financial crisis. We find that statements are most influential during the financial crisis and the effects are mostly present in the curvature of the yield curve through information related to the financial theme.",http://aclweb.org/anthology/D16-1226,D16-1,D16-1226,https://arxiv.org/abs/1809.08718,"('Christopher Rohlfs', 'Sunandan Chakraborty', 'Lakshminarayanan Subramanian')",11,The Effects of the Content of FOMC Communications on US Treasury Rates,EMNLP,2016
"Learning a good representation of text is key to many recommendation applications. Examples include news recommendation where texts to be recommended are constantly published everyday. However, most existing recommendation techniques, such as matrix factorization based methods, mainly rely on interaction histories to learn representations of items. While latent factors of items can be learned effectively from user interaction data, in many cases, such data is not available, especially for newly emerged items.   In this work, we aim to address the problem of personalized recommendation for completely new items with text information available. We cast the problem as a personalized text ranking problem and propose a general framework that combines text embedding with personalized recommendation. Users and textual content are embedded into latent feature space. The text embedding function can be learned end-to-end by predicting user interactions with items. To alleviate sparsity in interaction data, and leverage large amount of text data with little or no user interactions, we further propose a joint text embedding model that incorporates unsupervised text embedding with a combination module. Experimental results show that our model can significantly improve the effectiveness of recommendation systems on real-world datasets.",http://aclweb.org/anthology/D16-1227,D16-1,D16-1227,https://arxiv.org/abs/1706.01084,"('Youyang Gu', 'Tao Lei', 'Regina Barzilay', 'Tommi Jaakkola')",11,Learning to refine text based recommendations,EMNLP,2016
,http://aclweb.org/anthology/D16-1228,D16-1,D16-1228,,"('Courtney Napoles', 'Keisuke Sakaguchi', 'Joel Tetreault')",11,"
      There's No Comparison: Reference-less Evaluation Metrics in Grammatical
      Error Correction
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1229,D16-1,D16-1229,,"('William L. Hamilton', 'Jure Leskovec', 'Dan Jurafsky')",11,"
      Cultural Shift or Linguistic Drift? Comparing Two Computational Measures
      of Semantic Change
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1230,D16-1,D16-1230,,"('Chia-Wei Liu', 'Ryan Lowe', 'Iulian Serban', 'Mike Noseworthy', 'Laurent Charlin', 'Joelle Pineau')",11,"
      How NOT To Evaluate Your Dialogue System: An Empirical Study of
      Unsupervised Evaluation Metrics for Dialogue Response Generation
    ",EMNLP,2016
"Developing conversational systems that can converse in many languages is an interesting challenge for natural language processing. In this paper, we introduce multilingual addressee and response selection. In this task, a conversational system predicts an appropriate addressee and response for an input message in multiple languages. A key to developing such multilingual responding systems is how to utilize high-resource language data to compensate for low-resource language data. We present several knowledge transfer methods for conversational systems. To evaluate our methods, we create a new multilingual conversation dataset. Experiments on the dataset demonstrate the effectiveness of our methods.",http://aclweb.org/anthology/D16-1231,D16-1,D16-1231,https://arxiv.org/abs/1808.03915,"('Hiroki Ouchi', 'Yuta Tsuboi')",11,Addressee and Response Selection for Multi-Party Conversation,EMNLP,2016
"State of the art speech recognition systems use data-intensive context-dependent phonemes as acoustic units. However, these approaches do not translate well to low resourced languages where large amounts of training data is not available. For such languages, automatic discovery of acoustic units is critical. In this paper, we demonstrate the application of nonparametric Bayesian models to acoustic unit discovery. We show that the discovered units are correlated with phonemes and therefore are linguistically meaningful. We also present a spoken term detection (STD) by example query algorithm based on these automatically learned units. We show that our proposed system produces a P@N of 61.2% and an EER of 13.95% on the TIMIT dataset. The improvement in the EER is 5% while P@N is only slightly lower than the best reported system in the literature.",http://aclweb.org/anthology/D16-1232,D16-1,D16-1232,https://arxiv.org/abs/1606.05967,"('Kei Wakabayashi', 'Johane Takeuchi', 'Kotaro Funakoshi', 'Mikio Nakano')",11,Nonparametric Bayesian Models for Spoken Language Understanding,EMNLP,2016
"Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.",http://aclweb.org/anthology/D16-1233,D16-1,D16-1233,https://arxiv.org/abs/1606.03352,"('Tsung-Hsien Wen', 'Milica Gasic', 'Nikola Mrkšić', 'Lina M. Rojas Barahona', 'Pei-Hao Su', 'Stefan Ultes', 'David Vandyke', 'Steve Young')",11,Conditional Generation and Snapshot Learning in Neural Dialogue Systems,EMNLP,2016
,http://aclweb.org/anthology/D16-1234,D16-1,D16-1234,,"('Stephen Roller', 'Katrin Erk')",11,"
      Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in
      Distributional Vectors for Lexical Entailment
    ",EMNLP,2016
"Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning.",http://aclweb.org/anthology/D16-1235,D16-1,D16-1235,https://arxiv.org/abs/1608.00869,"('Daniela Gerz', 'Ivan Vulić', 'Felix Hill', 'Roi Reichart', 'Anna Korhonen')",11,SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,EMNLP,2016
"Using paraphrases, the expression of the same semantic meaning in different words, to improve generalization and translation performance is often useful. However, prior works only explore the use of paraphrases at the word or phrase level, not at the sentence or document level. Unlike previous works, we use different translations of the whole training data that are consistent in structure as paraphrases at the corpus level. Our corpus contains parallel paraphrases in multiple languages from various sources. We treat paraphrases as foreign languages, tag source sentences with paraphrase labels, and train in the style of multilingual Neural Machine Translation (NMT). Experimental results indicate that adding paraphrases improves the rare word translation, increases entropy and diversity in lexical choice. Moreover, adding the source paraphrases improves translation performance more effectively than adding the target paraphrases. Combining both the source and the target paraphrases boosts performance further; combining paraphrases with multilingual data also helps but has mixed performance. We achieve a BLEU score of 57.2 for French-to-English translation, training on 24 paraphrases of the Bible, which is ~+27 above the WMT'14 baseline.",http://aclweb.org/anthology/D16-1236,D16-1,D16-1236,https://arxiv.org/abs/1808.08438,"('Adam Grycner', 'Gerhard Weikum')",11,POLY: Mining Relational Paraphrases from Multilingual Sentences,EMNLP,2016
"Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.",http://aclweb.org/anthology/D16-1237,D16-1,D16-1237,https://arxiv.org/abs/1506.06724,"('Tao Li', 'Vivek Srikumar')",11,Exploiting Sentence Similarities for Better Alignments,EMNLP,2016
"We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of {\it soft} headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.",http://aclweb.org/anthology/D16-1238,D16-1,D16-1238,https://arxiv.org/abs/1608.02076,"('Hao Cheng', 'Hao Fang', 'Xiaodong He', 'Jianfeng Gao', 'Li Deng')",11,Bi-directional Attention with Agreement for Dependency Parsing,EMNLP,2016
"We present a study on two key characteristics of human syntactic annotations: anchoring and agreement. Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards pre-existing values. We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output. Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with human-based annotations. Using sentences from the Penn Treebank WSJ, we also report systematically obtained inter-annotator agreement estimates for English dependency parsing. Our agreement results control for parser bias, and are consequential in that they are on par with state of the art parsing performance for English newswire. We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations.",http://aclweb.org/anthology/D16-1239,D16-1,D16-1239,https://arxiv.org/abs/1605.04481,"('Yevgeni Berzak', 'Yan Huang', 'Andrei Barbu', 'Anna Korhonen', 'Boris Katz')",11,Anchoring and Agreement in Syntactic Annotations,EMNLP,2016
"We present a manually constructed seed lexicon encoding the inferential profiles of French event selecting predicates across different uses. The inferential profile (Karttunen, 1971a) of a verb is designed to capture the inferences triggered by the use of this verb in context. It reflects the influence of the clause-embedding verb on the factuality of the event described by the embedded clause. The resource developed provides evidence for the following three hypotheses: (i) French implicative verbs have an aspect dependent profile (their inferential profile varies with outer aspect), while factive verbs have an aspect independent profile (they keep the same inferential profile with both imperfective and perfective aspect); (ii) implicativity decreases with imperfective aspect: the inferences triggered by French implicative verbs combined with perfective aspect are often weakened when the same verbs are combined with imperfective aspect; (iii) implicativity decreases with an animate (deep) subject: the inferences triggered by a verb which is implicative with an inanimate subject are weakened when the same verb is used with an animate subject. The resource additionally shows that verbs with different inferential profiles display clearly distinct sub-categorisation patterns. In particular, verbs that have both factive and implicative readings are shown to prefer infinitival clauses in their implicative reading, and tensed clauses in their factive reading.",http://aclweb.org/anthology/D16-1240,D16-1,D16-1240,https://arxiv.org/abs/1710.01095,"('Ellie Pavlick', 'Chris Callison-Burch')",11,Tense Manages to Predict Implicative Behavior in Verbs,EMNLP,2016
"We have constructed a new ""Who-did-What"" dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles --- an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization --- each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.",http://aclweb.org/anthology/D16-1241,D16-1,D16-1241,https://arxiv.org/abs/1608.05457,"('Takeshi Onishi', 'Hai Wang', 'Mohit Bansal', 'Kevin Gimpel', 'David McAllester')",11,Who did What: A Large-Scale Person-Centered Cloze Dataset,EMNLP,2016
,http://aclweb.org/anthology/D16-1242,D16-1,D16-1242,,"('Koji Mineshima', 'Ribeka Tanaka', 'Pascual Martínez-Gómez', 'Yusuke Miyao', 'Daisuke Bekki')",11,"
      Building compositional semantics and higher-order inference system for a
      wide-coverage Japanese CCG parser
    ",EMNLP,2016
"The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fourier-transformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model's output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (""greenish""), bare modifiers (""bright"", ""dull""), and compositional phrases (""faded teal"") not seen in training.",http://aclweb.org/anthology/D16-1243,D16-1,D16-1243,https://arxiv.org/abs/1606.03821,"('Will Monroe', 'Noah D. Goodman', 'Christopher Potts')",11,Learning to Generate Compositional Color Descriptions,EMNLP,2016
"We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.",http://aclweb.org/anthology/D16-1244,D16-1,D16-1244,https://arxiv.org/abs/1606.01933,"('Ankur Parikh', 'Oscar Täckström', 'Dipanjan Das', 'Jakob Uszkoreit')",11,A Decomposable Attention Model for Natural Language Inference,EMNLP,2016
"Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.",http://aclweb.org/anthology/D16-1245,D16-1,D16-1245,https://arxiv.org/abs/1609.08667,"('Kevin Clark', 'Christopher D. Manning')",11,Deep Reinforcement Learning for Mention-Ranking Coreference Models,EMNLP,2016
,http://aclweb.org/anthology/D16-1246,D16-1,D16-1246,,"('Lianhui Qin', 'Zhisong Zhang', 'Hai Zhao')",11,"
      A Stacking Gated Neural Architecture for Implicit Discourse Relation
      Classification
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1247,D16-1,D16-1247,,"('Toshiaki Nakazawa', 'John Richardson', 'Sadao Kurohashi')",11,"
      Insertion Position Selection Model for Flexible Non-Terminals in
      Dependency Tree-to-Tree Machine Translation
    ",EMNLP,2016
"Beam search is widely used in neural machine translation, and usually improves translation quality compared to greedy search. It has been widely observed that, however, beam sizes larger than 5 hurt translation quality. We explain why this happens, and propose several methods to address this problem. Furthermore, we discuss the optimal stopping criteria for these methods. Results show that our hyperparameter-free methods outperform the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation.",http://aclweb.org/anthology/D16-1248,D16-1,D16-1248,https://arxiv.org/abs/1808.09582,"('Xing Shi', 'Kevin Knight', 'Deniz Yuret')",11,Why Neural Translations are the Right Length,EMNLP,2016
"The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of re- ordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the super- vised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.",http://aclweb.org/anthology/D16-1249,D16-1,D16-1249,https://arxiv.org/abs/1609.04186,"('Haitao Mi', 'Zhiguo Wang', 'Abe Ittycheriah')",11,Supervised Attentions for Neural Machine Translation,EMNLP,2016
,http://aclweb.org/anthology/D16-1250,D16-1,D16-1250,,"('Mikel Artetxe', 'Gorka Labaka', 'Eneko Agirre')",11,"
      Learning principled bilingual mappings of word embeddings while preserving
      monolingual invariance
    ",EMNLP,2016
,http://aclweb.org/anthology/D16-1251,D16-1,D16-1251,,"('Ben Russell', 'Duncan Gillespie')",11,"
      Measuring the behavioral impact of machine translation quality
      improvements with A/B testing
    ",EMNLP,2016
"Information Extraction (IE) refers to automatically extracting structured relation tuples from unstructured texts. Common IE solutions, including Relation Extraction (RE) and open IE systems, can hardly handle cross-sentence tuples, and are severely restricted by limited relation types as well as informal relation specifications (e.g., free-text based relation tuples). In order to overcome these weaknesses, we propose a novel IE framework named QA4IE, which leverages the flexible question answering (QA) approaches to produce high quality relation triples across sentences. Based on the framework, we develop a large IE benchmark with high quality human evaluation. This benchmark contains 293K documents, 2M golden relation triples, and 636 relation types. We compare our system with some IE baselines on our benchmark and the results show that our system achieves great improvements.",http://aclweb.org/anthology/D16-1252,D16-1,D16-1252,https://arxiv.org/abs/1804.03396,"('Gabriel Stanovsky', 'Ido Dagan')",11,Creating a Large Benchmark for Open Information Extraction,EMNLP,2016
,http://aclweb.org/anthology/D16-1253,D16-1,D16-1253,,"('Changxing Wu', 'xiaodong shi', 'Yidong Chen', 'Yanzhou Huang', 'jinsong su')",11,"
      Bilingually-constrained Synthetic Data for Implicit Discourse Relation
      Recognition
    ",EMNLP,2016
"A major drawback of backpropagation through time (BPTT) is the difficulty of learning long-term dependencies, coming from having to propagate credit information backwards through every single step of the forward computation. This makes BPTT both computationally impractical and biologically implausible. For this reason, full backpropagation through time is rarely used on long sequences, and truncated backpropagation through time is used as a heuristic. However, this usually leads to biased estimates of the gradient in which longer term dependencies are ignored. Addressing this issue, we propose an alternative algorithm, Sparse Attentive Backtracking, which might also be related to principles used by brains to learn long-term dependencies. Sparse Attentive Backtracking learns an attention mechanism over the hidden states of the past and selectively backpropagates through paths with high attention weights. This allows the model to learn long term dependencies while only backtracking for a small number of time steps, not just from the recent past but also from attended relevant past states.",http://aclweb.org/anthology/D16-1254,D16-1,D16-1254,https://arxiv.org/abs/1711.02326,"('Jacob Buckman', 'Miguel Ballesteros', 'Chris Dyer')",11,Transition-Based Dependency Parsing with Heuristic Backtracking,EMNLP,2016
"Existing entailment datasets mainly pose problems which can be answered without attention to grammar or word order. Learning syntax requires comparing examples where different grammar and word order change the desired classification. We introduce several datasets based on synthetic transformations of natural entailment examples in SNLI or FEVER, to teach aspects of grammar and word order. We show that without retraining, popular entailment models are unaware that these syntactic differences change meaning. With retraining, some but not all popular entailment models can learn to compare the syntax properly.",http://aclweb.org/anthology/D16-1255,D16-1,D16-1255,https://arxiv.org/abs/1810.11067,"('Allen Schmaltz', 'Alexander M. Rush', 'Stuart Shieber')",11,Word Ordering Without Syntax,EMNLP,2016
"Segmentations are often necessary for the analysis of image data. They are used to identify different objects, for example cell nuclei, mitochondria, or complete cells in microscopic images. There might be features in the data, that cannot be detected by segmentation approaches directly, because they are not characterized by their texture of boundaries, which are properties most segmentation techniques rely on, but morphologically. In this report we will introduce our algorithm for the extraction of suchlike morphological features of segmented objects from segmentations of neuromuscular junctions and its interface for informed parameter tuning.",http://aclweb.org/anthology/D16-1256,D16-1,D16-1256,https://arxiv.org/abs/1605.09250,"('Ryan Cotterell', 'Arun Kumar', 'Hinrich Schütze')",11,Morphological Segmentation Inside-Out,EMNLP,2016
"Dependency parsing is one of the important natural language processing tasks that assigns syntactic trees to texts. Due to the wider availability of dependency corpora and improved parsing and machine learning techniques, parsing accuracies of supervised learning-based systems have been significantly improved. However, due to the nature of supervised learning, those parsing systems highly rely on the manually annotated training corpora. They work reasonably good on the in-domain data but the performance drops significantly when tested on out-of-domain texts. To bridge the performance gap between in-domain and out-of-domain, this thesis investigates three semi-supervised techniques for out-of-domain dependency parsing, namely co-training, self-training and dependency language models. Our approaches use easily obtainable unlabelled data to improve out-of-domain parsing accuracies without the need of expensive corpora annotation. The evaluations on several English domains and multi-lingual data show quite good improvements on parsing accuracy. Overall this work conducted a survey of semi-supervised methods for out-of-domain dependency parsing, where I extended and compared a number of important semi-supervised methods in a unified framework. The comparison between those techniques shows that self-training works equally well as co-training on out-of-domain parsing, while dependency language models can improve both in- and out-of-domain accuracies.",http://aclweb.org/anthology/D16-1257,D16-1,D16-1257,https://arxiv.org/abs/1810.02100,"('Do Kook Choe', 'Eugene Charniak')",11,Parsing as Language Modeling,EMNLP,2016
"We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank. We isolate a number of dependency relations which previous models neglect but which contribute to higher parse accuracy.",http://aclweb.org/anthology/D16-1258,D16-1,D16-1258,https://arxiv.org/abs/cs/0011040,"('Luheng He', 'Julian Michael', 'Mike Lewis', 'Luke Zettlemoyer')",11,Human-in-the-Loop Parsing,EMNLP,2016
"In this paper, we propose a unsupervised framework to reconstruct a person's life history by creating a chronological list for {\it personal important events} (PIE) of individuals based on the tweets they published. By analyzing individual tweet collections, we find that what are suitable for inclusion in the personal timeline should be tweets talking about personal (as opposed to public) and time-specific (as opposed to time-general) topics. To further extract these types of topics, we introduce a non-parametric multi-level Dirichlet Process model to recognize four types of tweets: personal time-specific (PersonTS), personal time-general (PersonTG), public time-specific (PublicTS) and public time-general (PublicTG) topics, which, in turn, are used for further personal event extraction and timeline generation. To the best of our knowledge, this is the first work focused on the generation of timeline for individuals from twitter data. For evaluation, we have built a new golden standard Timelines based on Twitter and Wikipedia that contain PIE related events from 20 {\it ordinary twitter users} and 20 {\it celebrities}. Experiments on real Twitter data quantitatively demonstrate the effectiveness of our approach.",http://aclweb.org/anthology/D16-1259,D16-1,D16-1259,https://arxiv.org/abs/1309.7313,"('Sandro Bauer', 'Simone Teufel')",11,Unsupervised Timeline Generation for Wikipedia History Articles,EMNLP,2016
"Graphs are a commonly used construct for representing relationships between elements in complex high dimensional datasets. Many real-world phenomenon are dynamic in nature, meaning that any graph used to represent them is inherently temporal. However, many of the machine learning models designed to capture knowledge about the structure of these graphs ignore this rich temporal information when creating representations of the graph. This results in models which do not perform well when used to make predictions about the future state of the graph -- especially when the delta between time stamps is not small. In this work, we explore a novel training procedure and an associated unsupervised model which creates graph representations optimised to predict the future state of the graph. We make use of graph convolutional neural networks to encode the graph into a latent representation, which we then use to train our temporal offset reconstruction method, inspired by auto-encoders, to predict a later time point -- multiple time steps into the future. Using our method, we demonstrate superior performance for the task of future link prediction compared with none-temporal state-of-the-art baselines. We show our approach to be capable of outperforming non-temporal baselines by 38% on a real world dataset.",http://aclweb.org/anthology/D16-1260,D16-1,D16-1260,https://arxiv.org/abs/1811.08366,"('Tingsong Jiang', 'Tianyu Liu', 'Tao Ge', 'Lei Sha', 'Sujian Li', 'Baobao Chang', 'Zhifang Sui')",11,Encoding Temporal Information for Time-Aware Link Prediction,EMNLP,2016
,http://aclweb.org/anthology/D16-1261,D16-1,D16-1261,,"('Karthik Narasimhan', 'Adam Yala', 'Regina Barzilay')",11,"
      Improving Information Extraction by Acquiring External Evidence with
      Reinforcement Learning
    ",EMNLP,2016
"We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.",http://aclweb.org/anthology/D16-1262,D16-1,D16-1262,https://arxiv.org/abs/1607.01432,"('Kenton Lee', 'Mike Lewis', 'Luke Zettlemoyer')",11,Global Neural CCG Parsing with Optimality Guarantees,EMNLP,2016
"For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units.   In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",http://aclweb.org/anthology/D16-1263,D16-1,D16-1263,https://arxiv.org/abs/1712.01864,"('Oliver Adams', 'Graham Neubig', 'Trevor Cohn', 'Steven Bird', 'Quoc Truong Do', 'Satoshi Nakamura')",11,Learning a Lexicon and Translation Model from Phoneme Lattices,EMNLP,2016
"We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children's Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.",http://aclweb.org/anthology/D16-1264,D16-1,D16-1264,https://arxiv.org/abs/1606.02270,"('Pranav Rajpurkar', 'Jian Zhang', 'Konstantin Lopyrev', 'Percy Liang')",11,"SQuAD: 100,000+ Questions for Machine Comprehension of Text",EMNLP,2016
