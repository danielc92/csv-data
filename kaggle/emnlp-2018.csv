abstract,acl_url,anthology,anthology_id,arxiv_url,authors,month,title,venue,year
"This paper presents a corpus-based method to assign grammatical subject/object relations to ambiguous German constructs. It makes use of an unsupervised learning procedure to collect training and test data, and the back-off model to make assignment decisions.",http://aclweb.org/anthology/D18-1000,D18-1,D18-1000,https://arxiv.org/abs/cmp-lg/9706001,"('Ellen Riloff', 'David Chiang', 'Hockenmaier Julia', 'Tsujii Jun’ichi')",10,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,EMNLP,2018
"Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus.",http://aclweb.org/anthology/D18-1001,D18-1,D18-1001,https://arxiv.org/abs/1710.02318,"('Maximin Coavoux', 'Shashi Narayan', 'Shay B. Cohen')",10,Privacy-preserving Neural Representations of Text,EMNLP,2018
"Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation. We show that demographic information of authors is encoded in -- and can be recovered from -- the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual data are not agnostic to -- and likely condition on -- demographic attributes. When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several tasks, demographic properties and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features.",http://aclweb.org/anthology/D18-1002,D18-1,D18-1002,https://arxiv.org/abs/1808.06640,"('Yanai Elazar', 'Yoav Goldberg')",10,Adversarial Removal of Demographic Attributes from Text Data,EMNLP,2018
"Misinformation such as fake news is one of the big challenges of our society. Research on automated fact-checking has proposed methods based on supervised learning, but these approaches do not consider external evidence apart from labeled training instances. Recent approaches counter this deficit by considering external sources related to a claim. However, these methods require substantial feature modeling and rich lexicons. This paper overcomes these limitations of prior work with an end-to-end model for evidence-aware credibility assessment of arbitrary textual claims, without any human intervention. It presents a neural network model that judiciously aggregates signals from external evidence articles, the language of these articles and the trustworthiness of their sources. It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user. Experiments with four datasets and ablation studies show the strength of our method.",http://aclweb.org/anthology/D18-1003,D18-1,D18-1003,https://arxiv.org/abs/1809.06416,"('Kashyap Popat', 'Subhabrata Mukherjee', 'Andrew Yates', 'Gerhard Weikum')",10,DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning,EMNLP,2018
We propose an online access panel to support the evaluation process of Interactive Information Retrieval (IIR) systems - called IIRpanel. By maintaining an online access panel with users of IIR systems we assume that the recurring effort to recruit participants for web-based as well as for lab studies can be minimized. We target on using the online access panel not only for our own development processes but to open it for other interested researchers in the field of IIR. In this paper we present the concept of IIRpanel as well as first implementation details.,http://aclweb.org/anthology/D18-1004,D18-1,D18-1004,https://arxiv.org/abs/1407.1540,"('Zijian Wang', 'David Jurgens')",10,It's going to be okay: Measuring Access to Support in Online Communities,EMNLP,2018
"Gang-involved youth in cities such as Chicago have increasingly turned to social media to post about their experiences and intents online. In some situations, when they experience the loss of a loved one, their online expression of emotion may evolve into aggression towards rival gangs and ultimately into real-world violence. In this paper, we present a novel system for detecting Aggression and Loss in social media. Our system features the use of domain-specific resources automatically derived from a large unlabeled corpus, and contextual representations of the emotional and semantic content of the user's recent tweets as well as their interactions with other users. Incorporating context in our Convolutional Neural Network (CNN) leads to a significant improvement.",http://aclweb.org/anthology/D18-1005,D18-1,D18-1005,https://arxiv.org/abs/1809.03632,"('Serina Chang', 'Ruiqi Zhong', 'Ethan Adams', 'Fei-Tzin Lee', 'Siddharth Varia', 'Desmond Patton', 'William Frey', 'Chris Kedzie', 'Kathy McKeown')",10,Detecting Gang-Involved Escalation on Social Media Using Context,EMNLP,2018
"Comprehending procedural text, e.g., a paragraph describing photosynthesis, requires modeling actions and the state changes they produce, so that questions about entities at different timepoints can be answered. Although several recent systems have shown impressive progress in this task, their predictions can be globally inconsistent or highly improbable. In this paper, we show how the predicted effects of actions in the context of a paragraph can be improved in two ways: (1) by incorporating global, commonsense constraints (e.g., a non-existent entity cannot be destroyed), and (2) by biasing reading with preferences from large-scale corpora (e.g., trees rarely move). Unlike earlier methods, we treat the problem as a neural structured prediction task, allowing hard and soft constraints to steer the model away from unlikely predictions. We show that the new model significantly outperforms earlier systems on a benchmark dataset for procedural text comprehension (+8% relative gain), and that it also avoids some of the nonsensical predictions that earlier systems make.",http://aclweb.org/anthology/D18-1006,D18-1,D18-1006,https://arxiv.org/abs/1808.10012,"('Niket Tandon', 'Bhavana Dalvi', 'Joel Grus', 'Wen-tau Yih', 'Antoine Bosselut', 'Peter Clark')",10,Reasoning about Actions and State Changes by Injecting Commonsense Knowledge,EMNLP,2018
"We present a large-scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation captures distinct types of reasoning. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. We refer to our collection as the DNC: Diverse Natural Language Inference Collection. The DNC is available online at https://www.decomp.net, and will grow over time as additional resources are recast and added from novel sources.",http://aclweb.org/anthology/D18-1007,D18-1,D18-1007,https://arxiv.org/abs/1804.08207,"('Adam Poliak', 'Aparajita Haldar', 'Rachel Rudinger', 'J. Edward Hu', 'Ellie Pavlick', 'Aaron Steven White', 'Benjamin Van Durme')",10,Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation,EMNLP,2018
"To understand a sentence like ""whereas only 10% of White Americans live at or below the poverty line, 28% of African Americans do"" it is important not only to identify individual facts, e.g., poverty rates of distinct demographic groups, but also the higher-order relations between them, e.g., the disparity between them. In this paper, we propose the task of Textual Analogy Parsing (TAP) to model this higher-order meaning. The output of TAP is a frame-style meaning representation which explicitly specifies what is shared (e.g., poverty rates) and what is compared (e.g., White Americans vs. African Americans, 10% vs. 28%) between its component facts. Such a meaning representation can enable new applications that rely on discourse understanding such as automated chart generation from quantitative text. We present a new dataset for TAP, baselines, and a model that successfully uses an ILP to enforce the structural constraints of the problem.",http://aclweb.org/anthology/D18-1008,D18-1,D18-1008,https://arxiv.org/abs/1809.02700,"('Matthew Lamm', 'Arun Chaganty', 'Christopher D. Manning', 'Dan Jurafsky', 'Percy Liang')",10,Textual Analogy Parsing: What's Shared and What's Compared among Analogous Facts,EMNLP,2018
"Given a partial description like ""she opened the hood of the car,"" humans can reason about the situation and anticipate what might come next (""then, she examined the engine""). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.   We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",http://aclweb.org/anthology/D18-1009,D18-1,D18-1009,https://arxiv.org/abs/1808.05326,"('Rowan Zellers', 'Yonatan Bisk', 'Roy Schwartz', 'Yejin Choi')",10,SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,EMNLP,2018
"Determining whether a given claim is supported by evidence is a fundamental NLP problem that is best modeled as Textual Entailment. However, given a large collection of text, finding evidence that could support or refute a given claim is a challenge in itself, amplified by the fact that different evidence might be needed to support or refute a claim. Nevertheless, most prior work decouples evidence identification from determining the truth value of the claim given the evidence.   We propose to consider these two aspects jointly. We develop TwoWingOS (two-wing optimization strategy), a system that, while identifying appropriate evidence for a claim, also determines whether or not the claim is supported by the evidence. Given the claim, TwoWingOS attempts to identify a subset of the evidence candidates; given the predicted evidence, it then attempts to determine the truth value of the corresponding claim. We treat this challenge as coupled optimization problems, training a joint model for it. TwoWingOS offers two advantages: (i) Unlike pipeline systems, it facilitates flexible-size evidence set, and (ii) Joint training improves both the claim entailment and the evidence identification. Experiments on a benchmark dataset show state-of-the-art performance. Code: https://github.com/yinwenpeng/FEVER",http://aclweb.org/anthology/D18-1010,D18-1,D18-1010,https://arxiv.org/abs/1808.03465,"('Wenpeng Yin', 'Dan Roth')",10,TwoWingOS: A Two-Wing Optimization Strategy for Evidential Claim Verification,EMNLP,2018
"Continuous multimodal representations suitable for multimodal information retrieval are usually obtained with methods that heavily rely on multimodal autoencoders. In video hyperlinking, a task that aims at retrieving video segments, the state of the art is a variation of two interlocked networks working in opposing directions. These systems provide good multimodal embeddings and are also capable of translating from one representation space to the other. Operating on representation spaces, these networks lack the ability to operate in the original spaces (text or image), which makes it difficult to visualize the crossmodal function, and do not generalize well to unseen data. Recently, generative adversarial networks have gained popularity and have been used for generating realistic synthetic data and for obtaining high-level, single-modal latent representation spaces. In this work, we evaluate the feasibility of using GANs to obtain multimodal representations. We show that GANs can be used for multimodal representation learning and that they provide multimodal representations that are superior to representations obtained with multimodal autoencoders. Additionally, we illustrate the ability of visualizing crossmodal translations that can provide human-interpretable insights on learned GAN-based video hyperlinking models.",http://aclweb.org/anthology/D18-1011,D18-1,D18-1011,https://arxiv.org/abs/1705.05103,"('Shaonan Wang', 'Jiajun Zhang', 'Chengqing Zong')",10,Associative Multichannel Autoencoder for Multimodal Word Representation,EMNLP,2018
"In this paper, we present a statistical approach for dialogue act processing in the dialogue component of the speech-to-speech translation system \vm. Statistics in dialogue processing is used to predict follow-up dialogue acts. As an application example we show how it supports repair when unexpected dialogue states occur.",http://aclweb.org/anthology/D18-1012,D18-1,D18-1012,https://arxiv.org/abs/cmp-lg/9505013,"('Ramakanth Pasunuru', 'Mohit Bansal')",10,Game-Based Video-Context Dialogue,EMNLP,2018
"The encode-decoder framework has shown recent success in image captioning. Visual attention, which is good at detailedness, and semantic attention, which is good at comprehensiveness, have been separately proposed to ground the caption on the image. In this paper, we propose the Stepwise Image-Topic Merging Network (simNet) that makes use of the two kinds of attention at the same time. At each time step when generating the caption, the decoder adaptively merges the attentive information in the extracted topics and the image according to the generated context, so that the visual information and the semantic information can be effectively combined. The proposed approach is evaluated on two benchmark datasets and reaches the state-of-the-art performances.(The code is available at https://github.com/lancopku/simNet)",http://aclweb.org/anthology/D18-1013,D18-1,D18-1013,https://arxiv.org/abs/1808.08732,"('Fenglin Liu', 'Xuancheng Ren', 'Yuanxin Liu', 'Houfeng Wang', 'Xu Sun')",10,simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions,EMNLP,2018
"Computational modeling of human multimodal language is an emerging research area in natural language processing spanning the language, visual and acoustic modalities. Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions). In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of them focused on a subset of multimodal signals for specialized, effective fusion. Cross-modal interactions are modeled using this multistage fusion approach which builds upon intermediate representations of previous stages. Temporal and intra-modal interactions are modeled by integrating our proposed fusion approach with a system of recurrent neural networks. The RMFN displays state-of-the-art performance in modeling human multimodal language across three public datasets relating to multimodal sentiment analysis, emotion recognition, and speaker traits recognition. We provide visualizations to show that each stage of fusion focuses on a different subset of multimodal signals, learning increasingly discriminative multimodal representations.",http://aclweb.org/anthology/D18-1014,D18-1,D18-1014,https://arxiv.org/abs/1808.03920,"('Paul Pu Liang', 'Ziyin Liu', 'AmirAli Bagher Zadeh', 'Louis-Philippe Morency')",10,Multimodal Language Analysis with Recurrent Multistage Fusion,EMNLP,2018
"Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset. Our dataset consists of two parts: a dataset with real videos and template sentences (TEMPO - Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO - Human Language).",http://aclweb.org/anthology/D18-1015,D18-1,D18-1015,https://arxiv.org/abs/1809.01337,"('Jingyuan Chen', 'Xinpeng Chen', 'Lin Ma', 'Zequn Jie', 'Tat-Seng Chua')",10,Temporally Grounding Natural Sentence in Video,EMNLP,2018
"We introduce PreCo, a large-scale English dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. To strengthen the training-test overlap, we collect a large corpus of about 38K documents and 12.4M words which are mostly from the vocabulary of English-speaking preschoolers. Experiments show that with higher training-test overlap, error analysis on PreCo is more efficient than the one on OntoNotes, a popular existing dataset. Furthermore, we annotate singleton mentions making it possible for the first time to quantify the influence that a mention detector makes on coreference resolution performance. The dataset is freely available at https://preschool-lab.github.io/PreCo/.",http://aclweb.org/anthology/D18-1016,D18-1,D18-1016,https://arxiv.org/abs/1810.09807,"('Hong Chen', 'Zhenhua Fan', 'Hao Lu', 'Alan Yuille', 'Shu Rong')",10,PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution,EMNLP,2018
"To quickly obtain new labeled data, we can choose crowdsourcing as an alternative way at lower cost in a short time. But as an exchange, crowd annotations from non-experts may be of lower quality than those from experts. In this paper, we propose an approach to performing crowd annotation learning for Chinese Named Entity Recognition (NER) to make full use of the noisy sequence labels from multiple annotators. Inspired by adversarial learning, our approach uses a common Bi-LSTM and a private Bi-LSTM for representing annotator-generic and -specific information. The annotator-generic information is the common knowledge for entities easily mastered by the crowd. Finally, we build our Chinese NE tagger based on the LSTM-CRF model. In our experiments, we create two data sets for Chinese NER tasks from two domains. The experimental results show that our system achieves better scores than strong baseline systems.",http://aclweb.org/anthology/D18-1017,D18-1,D18-1017,https://arxiv.org/abs/1801.05147,"('Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao', 'Shengping Liu')",10,Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism,EMNLP,2018
"Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on the CoNLL dataset, they struggle to generalize properly to new domains or datasets. In this paper, we investigate the role of linguistic features in building more generalizable coreference resolvers. We show that generalization improves only slightly by merely using a set of additional linguistic features. However, employing features and subsets of their values that are informative for coreference resolution, considerably improves generalization. Thanks to better generalization, our system achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our system, which is trained on CoNLL, achieves on-par performance with a system designed for this dataset.",http://aclweb.org/anthology/D18-1018,D18-1,D18-1018,https://arxiv.org/abs/1708.00160,"('Nafise Sadat Moosavi', 'Michael Strube')",10,Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers,EMNLP,2018
"In this work, we propose a novel segmental hypergraph representation to model overlapping entity mentions that are prevalent in many practical datasets. We show that our model built on top of such a new representation is able to capture features and interactions that cannot be captured by previous models while maintaining a low time complexity for inference. We also present a theoretical analysis to formally assess how our representation is better than alternative representations reported in the literature in terms of representational power. Coupled with neural networks for feature learning, our model achieves the state-of-the-art performance in three benchmark datasets annotated with overlapping mentions.",http://aclweb.org/anthology/D18-1019,D18-1,D18-1019,https://arxiv.org/abs/1810.01817,"('Bailin Wang', 'Wei Lu')",10,Neural Segmental Hypergraphs for Overlapping Mention Recognition,EMNLP,2018
"Unsupervised structure learning in high-dimensional time series data has attracted a lot of research interests. For example, segmenting and labelling high dimensional time series can be helpful in behavior understanding and medical diagnosis. Recent advances in generative sequential modeling have suggested to combine recurrent neural networks with state space models (e.g., Hidden Markov Models). This combination can model not only the long term dependency in sequential data, but also the uncertainty included in the hidden states. Inheriting these advantages of stochastic neural sequential models, we propose a structured and stochastic sequential neural network, which models both the long-term dependencies via recurrent neural networks and the uncertainty in the segmentation and labels via discrete random variables. For accurate and efficient inference, we present a bi-directional inference network by reparamterizing the categorical segmentation and labels with the recent proposed Gumbel-Softmax approximation and resort to the Stochastic Gradient Variational Bayes. We evaluate the proposed model in a number of tasks, including speech modeling, automatic segmentation and labeling in behavior understanding, and sequential multi-objects recognition. Experimental results have demonstrated that our proposed model can achieve significant improvement over the state-of-the-art methods.",http://aclweb.org/anthology/D18-1020,D18-1,D18-1020,https://arxiv.org/abs/1705.08695,"('Mingda Chen', 'Qingming Tang', 'Karen Livescu', 'Kevin Gimpel')",10,Variational Sequential Labelers for Semi-Supervised Learning,EMNLP,2018
"Joint representation learning of words and entities benefits many NLP tasks, but has not been well explored in cross-lingual settings. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. It captures mutually complementary knowledge, and enables cross-lingual inferences among knowledge bases and texts. Our method does not require parallel corpora, and automatically generates comparable data via distant supervision using multi-lingual knowledge bases. We utilize two types of regularizers to align cross-lingual words and entities, and design knowledge attention and cross-lingual attention to further reduce noises. We conducted a series of experiments on three tasks: word translation, entity relatedness, and cross-lingual entity linking. The results, both qualitatively and quantitatively, demonstrate the significance of our method.",http://aclweb.org/anthology/D18-1021,D18-1,D18-1021,https://arxiv.org/abs/1811.10776,"('Yixin Cao', 'Lei Hou', 'Juanzi Li', 'Zhiyuan Liu', 'Chengjiang Li', 'Xu Chen', 'Tiansi Dong')",10,Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision,EMNLP,2018
"Joint image filters leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or enhancing spatial resolution. Existing methods either rely on various explicit filter constructions or hand-designed objective functions, thereby making it difficult to understand, improve, and accelerate these filters in a coherent framework. In this paper, we propose a learning-based approach for constructing joint filters based on Convolutional Neural Networks. In contrast to existing methods that consider only the guidance image, the proposed algorithm can selectively transfer salient structures that are consistent with both guidance and target images. We show that the model trained on a certain type of data, e.g., RGB and depth images, generalizes well to other modalities, e.g., flash/non-Flash and RGB/NIR images. We validate the effectiveness of the proposed joint filter through extensive experimental evaluations with state-of-the-art methods.",http://aclweb.org/anthology/D18-1022,D18-1,D18-1022,https://arxiv.org/abs/1710.04200,"('Yftah Ziser', 'Roi Reichart')",10,Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance,EMNLP,2018
"We construct a multilingual common semantic space based on distributional semantics, where words from multiple languages are projected into a shared space to enable knowledge and resource transfer across languages. Beyond word alignment, we introduce multiple cluster-level alignments and enforce the word clusters to be consistently distributed across multiple languages. We exploit three signals for clustering: (1) neighbor words in the monolingual word embedding space; (2) character-level information; and (3) linguistic properties (e.g., apposition, locative suffix) derived from linguistic structure knowledge bases available for thousands of languages. We introduce a new cluster-consistent correlational neural network to construct the common semantic space by aligning words as well as clusters. Intrinsic evaluation on monolingual and multilingual QVEC tasks shows our approach achieves significantly higher correlation with linguistic features than state-of-the-art multi-lingual embedding learning methods do. Using low-resource language name tagging as a case study for extrinsic evaluation, our approach achieves up to 24.5\% absolute F-score gain over the state of the art.",http://aclweb.org/anthology/D18-1023,D18-1,D18-1023,https://arxiv.org/abs/1804.07875,"('Lifu Huang', 'Kyunghyun Cho', 'Boliang Zhang', 'Heng Ji', 'Kevin Knight')",10,Multi-lingual Common Semantic Space Construction via Cluster-consistent Word Embedding,EMNLP,2018
"Multilingual Word Embeddings (MWEs) represent words from multiple languages in a single distributional vector space. Unsupervised MWE (UMWE) methods acquire multilingual embeddings without cross-lingual supervision, which is a significant advantage over traditional supervised approaches and opens many new possibilities for low-resource languages. Prior art for learning UMWEs, however, merely relies on a number of independently trained Unsupervised Bilingual Word Embeddings (UBWEs) to obtain multilingual embeddings. These methods fail to leverage the interdependencies that exist among many languages. To address this shortcoming, we propose a fully unsupervised framework for learning MWEs that directly exploits the relations between all language pairs. Our model substantially outperforms previous approaches in the experiments on multilingual word translation and cross-lingual word similarity. In addition, our model even beats supervised approaches trained with cross-lingual resources.",http://aclweb.org/anthology/D18-1024,D18-1,D18-1024,https://arxiv.org/abs/1808.08933,"('Xilun Chen', 'Claire Cardie')",10,Unsupervised Multilingual Word Embeddings,EMNLP,2018
"Unsupervised learned representations of polysemous words generate a large of pseudo multi senses since unsupervised methods are overly sensitive to contextual variations. In this paper, we address the pseudo multi-sense detection for word embeddings by dimensionality reduction of sense pairs. We propose a novel principal analysis method, termed Ex-RPCA, designed to detect both pseudo multi senses and real multi senses. With Ex-RPCA, we empirically show that pseudo multi senses are generated systematically in unsupervised method. Moreover, the multi-sense word embeddings can by improved by a simple linear transformation based on Ex-RPCA. Our improved word embedding outperform the original one by 5.6 points on Stanford contextual word similarity (SCWS) dataset. We hope our simple yet effective approach will help the linguistic analysis of multi-sense word embeddings in the future.",http://aclweb.org/anthology/D18-1025,D18-1,D18-1025,https://arxiv.org/abs/1803.01255,"('Ta Chung Chi', 'Yun-Nung Chen')",10,CLUSE: Cross-Lingual Unsupervised Sense Embeddings,EMNLP,2018
"Semantic specialization is the process of fine-tuning pre-trained distributional word vectors using external lexical knowledge (e.g., WordNet) to accentuate a particular semantic relation in the specialized vector space. While post-processing specialization methods are applicable to arbitrary distributional vectors, they are limited to updating only the vectors of words occurring in external lexicons (i.e., seen words), leaving the vectors of all other words unchanged. We propose a novel approach to specializing the full distributional vocabulary. Our adversarial post-specialization method propagates the external lexical knowledge to the full distributional space. We exploit words seen in the resources as training examples for learning a global specialization function. This function is learned by combining a standard L2-distance loss with an adversarial loss: the adversarial component produces more realistic output vectors. We show the effectiveness and robustness of the proposed method across three languages and on three tasks: word similarity, dialog state tracking, and lexical simplification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data.",http://aclweb.org/anthology/D18-1026,D18-1,D18-1026,https://arxiv.org/abs/1809.04163,"('Edoardo Maria Ponti', 'Ivan Vulić', 'Goran Glavaš', 'Nikola Mrkšić', 'Anna Korhonen')",10,Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization,EMNLP,2018
"Cross-lingual word embeddings are becoming increasingly important in multilingual NLP. Recently, it has been shown that these embeddings can be effectively learned by aligning two disjoint monolingual vector spaces through linear transformations, using no more than a small bilingual dictionary as supervision. In this work, we propose to apply an additional transformation after the initial alignment step, which moves cross-lingual synonyms towards a middle point between them. By applying this transformation our aim is to obtain a better cross-lingual integration of the vector spaces. In addition, and perhaps surprisingly, the monolingual spaces also improve by this transformation. This is in contrast to the original alignment, which is typically learned such that the structure of the monolingual spaces is preserved. Our experiments confirm that the resulting cross-lingual embeddings outperform state-of-the-art models in both monolingual and cross-lingual evaluation tasks.",http://aclweb.org/anthology/D18-1027,D18-1,D18-1027,https://arxiv.org/abs/1808.08780,"('Yerai Doval', 'Jose Camacho-Collados', 'Luis Espinosa Anke', 'Steven Schockaert')",10,Improving Cross-Lingual Word Embeddings by Meeting in the Middle,EMNLP,2018
"We release a corpus of 43 million atomic edits across 8 languages. These edits are mined from Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. We use the collected data to show that the language generated during editing differs from the language that we observe in standard corpora, and that models trained on edits encode different aspects of semantics and discourse than models trained on raw, unstructured text. We release the full corpus as a resource to aid ongoing research in semantics, discourse, and representation learning.",http://aclweb.org/anthology/D18-1028,D18-1,D18-1028,https://arxiv.org/abs/1808.09422,"('Manaal Faruqui', 'Ellie Pavlick', 'Ian Tenney', 'Dipanjan Das')",10,WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse,EMNLP,2018
"Addressing the cross-lingual variation of grammatical structures and meaning categorization is a key challenge for multilingual Natural Language Processing. The lack of resources for the majority of the world's languages makes supervised learning not viable. Moreover, the performance of most algorithms is hampered by language-specific biases and the neglect of informative multilingual data. The discipline of Linguistic Typology provides a principled framework to compare languages systematically and empirically and documents their variation in publicly available databases. These enshrine crucial information to design language-independent algorithms and refine techniques devised to mitigate the above-mentioned issues, including cross-lingual transfer and multilingual joint models, with typological features. In this survey, we demonstrate that typology is beneficial to several NLP applications, involving both semantic and syntactic tasks. Moreover, we outline several techniques to extract features from databases or acquire them automatically: these features can be subsequently integrated into multilingual models to tie parameters together cross-lingually or gear a model towards a specific language. Finally, we advocate for a new typology that accounts for the patterns within individual examples rather than entire languages, and for graded categories rather than discrete ones, in oder to bridge the gap with the contextual and continuous nature of machine learning algorithms.",http://aclweb.org/anthology/D18-1029,D18-1,D18-1029,https://arxiv.org/abs/1807.00914,"('Daniela Gerz', 'Ivan Vulić', 'Edoardo Maria Ponti', 'Roi Reichart', 'Anna Korhonen')",10,On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling,EMNLP,2018
"We address fine-grained multilingual language identification: providing a language code for every token in a sentence, including codemixed text containing multiple languages. Such text is prevalent online, in documents, social media, and message boards. We show that a feed-forward network with a simple globally constrained decoder can accurately and rapidly label both codemixed and monolingual text in 100 languages and 100 language pairs. This model outperforms previously published multilingual approaches in terms of both accuracy and speed, yielding an 800x speed-up and a 19.5% averaged absolute gain on three codemixed datasets. It furthermore outperforms several benchmark systems on monolingual language identification.",http://aclweb.org/anthology/D18-1030,D18-1,D18-1030,https://arxiv.org/abs/1810.04142,"('Yuan Zhang', 'Jason Riesa', 'Daniel Gillick', 'Anton Bakalov', 'Jason Baldridge', 'David Weiss')",10,"A Fast, Compact, Accurate Model for Language Identification of Codemixed Text",EMNLP,2018
"User profiling means exploiting the technology of machine learning to predict attributes of users, such as demographic attributes, hobby attributes, preference attributes, etc. It's a powerful data support of precision marketing. Existing methods mainly study network behavior, personal preferences, post texts to build user profile. Through our data analysis of micro-blog, we find that females show more positive and have richer emotions than males in online social platform. This difference is very conducive to the distinction between genders. Therefore, we argue that sentiment context is important as well for user profiling.This paper focuses on exploiting microblog user posts to predict one of the demographic labels: gender. We propose a Sentiment Representation Learning based Multi-Layer Perceptron(SRL-MLP) model to classify gender. First we build a sentiment polarity classifier in advance by training Long Short-Term Memory(LSTM) model on e-commerce review corpus. Next we transfer sentiment representation to a basic MLP network. Last we conduct experiments on gender classification by sentiment representation. Experimental results show that our approach can improve gender classification accuracy by 5.53\%, from 84.20\% to 89.73\%.",http://aclweb.org/anthology/D18-1031,D18-1,D18-1031,https://arxiv.org/abs/1810.06645,"('Weichao Wang', 'Shi Feng', 'Wei Gao', 'Daling Wang', 'Yifei Zhang')",10,Personalized Microblog Sentiment Classification via Adversarial Cross-lingual Multi-task Learning,EMNLP,2018
"Many real-world problems can be represented as graph-based learning problems. In this paper, we propose a novel framework for learning spatial and attentional convolution neural networks on arbitrary graphs. Different from previous convolutional neural networks on graphs, we first design a motif-matching guided subgraph normalization method to capture neighborhood information. Then we implement self-attentional layers to learn different importances from different subgraphs to solve graph classification problems. Analogous to image-based attentional convolution networks that operate on locally connected and weighted regions of the input, we also extend graph normalization from one-dimensional node sequence to two-dimensional node grid by leveraging motif-matching, and design self-attentional layers without requiring any kinds of cost depending on prior knowledge of the graph structure. Our results on both bioinformatics and social network datasets show that we can significantly improve graph classification benchmarks over traditional graph kernel and existing deep models.",http://aclweb.org/anthology/D18-1032,D18-1,D18-1032,https://arxiv.org/abs/1811.08270,"('Zhichun Wang', 'Qingsong Lv', 'Xiaohan Lan', 'Yu Zhang')",10,Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks,EMNLP,2018
"Sememes are minimum semantic units of concepts in human languages, such that each word sense is composed of one or multiple sememes. Words are usually manually annotated with their sememes by linguists, and form linguistic common-sense knowledge bases widely used in various NLP tasks. Recently, the lexical sememe prediction task has been introduced. It consists of automatically recommending sememes for words, which is expected to improve annotation efficiency and consistency. However, existing methods of lexical sememe prediction typically rely on the external context of words to represent the meaning, which usually fails to deal with low-frequency and out-of-vocabulary words. To address this issue for Chinese, we propose a novel framework to take advantage of both internal character information and external context information of words. We experiment on HowNet, a Chinese sememe knowledge base, and demonstrate that our framework outperforms state-of-the-art baselines by a large margin, and maintains a robust performance even for low-frequency words.",http://aclweb.org/anthology/D18-1033,D18-1,D18-1033,https://arxiv.org/abs/1806.06349,"('Fanchao Qi', 'Yankai Lin', 'Maosong Sun', 'Hao Zhu', 'Ruobing Xie', 'Zhiyuan Liu')",10,Cross-lingual Lexical Sememe Prediction,EMNLP,2018
"Named Entity Recognition (NER) aims at locating and classifying named entities in text. In some use cases of NER, including cases where detected named entities are used in creating content recommendations, it is crucial to have a reliable confidence level for the detected named entities. In this work we study the problem of finding confidence levels for detected named entities. We refer to this problem as Named Entity Sequence Classification (NESC). We frame NESC as a binary classification problem and we use NER as well as recurrent neural networks to find the probability of candidate named entity is a real named entity. We apply this approach to Tweet texts and we show how we could find named entities with high confidence levels from Tweets.",http://aclweb.org/anthology/D18-1034,D18-1,D18-1034,https://arxiv.org/abs/1712.02316,"('Jiateng Xie', 'Zhilin Yang', 'Graham Neubig', 'Noah A. Smith', 'Jaime Carbonell')",10,Neural Cross-Lingual Named Entity Recognition with Minimal Resources,EMNLP,2018
"Beam search is a widely used approximate search strategy for neural network decoders, and it generally outperforms simple greedy decoding on tasks like machine translation. However, this improvement comes at substantial computational cost. In this paper, we propose a flexible new method that allows us to reap nearly the full benefits of beam search with nearly no additional computational cost. The method revolves around a small neural network actor that is trained to observe and manipulate the hidden state of a previously-trained decoder. To train this actor network, we introduce the use of a pseudo-parallel corpus built using the output of beam search on a base model, ranked by a target quality metric like BLEU. Our method is inspired by earlier work on this problem, but requires no reinforcement learning, and can be trained reliably on a range of models. Experiments on three parallel corpora and three architectures show that the method yields substantial improvements in translation quality and speed over each base system.",http://aclweb.org/anthology/D18-1035,D18-1,D18-1035,https://arxiv.org/abs/1804.07915,"('Yun Chen', 'Victor O.K. Li', 'Kyunghyun Cho', 'Samuel Bowman')",10,A Stable and Effective Learning Strategy for Trainable Greedy Decoding,EMNLP,2018
"Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",http://aclweb.org/anthology/D18-1036,D18-1,D18-1036,https://arxiv.org/abs/1508.07909,"('Yang Zhao', 'Jiajun Zhang', 'Zhongjun He', 'Chengqing Zong', 'Hua Wu')",10,Addressing Troublesome Words in Neural Machine Translation,EMNLP,2018
"The addition of syntax-aware decoding in Neural Machine Translation (NMT) systems requires an effective tree-structured neural network, a syntax-aware attention model and a language generation model that is sensitive to sentence structure. We exploit a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakola (2017) to create an NMT model called Seq2DRNN that combines a sequential encoder with tree-structured decoding augmented with a syntax-aware attention model. Unlike previous approaches to syntax-based NMT which use dependency parsing models our method uses constituency parsing which we argue provides useful information for translation. In addition, we use the syntactic structure of the sentence to add new connections to the tree-structured decoder neural network (Seq2DRNN+SynC). We compare our NMT model with sequential and state of the art syntax-based NMT models and show that our model produces more fluent translations with better reordering. Since our model is capable of doing translation and constituency parsing at the same time we also compare our parsing accuracy against other neural parsing models.",http://aclweb.org/anthology/D18-1037,D18-1,D18-1037,https://arxiv.org/abs/1809.01854,"('Jetic Gū', 'Hassan S. Shavarani', 'Anoop Sarkar')",10,Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing,EMNLP,2018
"This paper proposes an improvement to the existing data-driven Neural Belief Tracking (NBT) framework for Dialogue State Tracking (DST). The existing NBT model uses a hand-crafted belief state update mechanism which involves an expensive manual retuning step whenever the model is deployed to a new dialogue domain. We show that this update mechanism can be learned jointly with the semantic decoding and context modelling parts of the NBT model, eliminating the last rule-based module from this DST framework. We propose two different statistical update mechanisms and show that dialogue dynamics can be modelled with a very small number of additional model parameters. In our DST evaluation over three languages, we show that this model achieves competitive performance and provides a robust framework for building resource-light DST models.",http://aclweb.org/anthology/D18-1038,D18-1,D18-1038,https://arxiv.org/abs/1805.11350,"('Wenhu Chen', 'Jianshu Chen', 'Yu Su', 'Xin Wang', 'Dong Yu', 'Xifeng Yan', 'William Yang Wang')",10,XL-NBT: A Cross-lingual Neural Belief Tracking Framework,EMNLP,2018
"We propose a simple modification to existing neural machine translation (NMT) models that enables using a single universal model to translate between multiple languages while allowing for language specific parameterization, and that can also be used for domain adaptation. Our approach requires no changes to the model architecture of a standard NMT system, but instead introduces a new component, the contextual parameter generator (CPG), that generates the parameters of the system (e.g., weights in a neural network). This parameter generator accepts source and target language embeddings as input, and generates the parameters for the encoder and the decoder, respectively. The rest of the model remains unchanged and is shared across all languages. We show how this simple modification enables the system to use monolingual data for training and also perform zero-shot translation. We further show it is able to surpass state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and that the learned language embeddings are able to uncover interesting relationships between languages.",http://aclweb.org/anthology/D18-1039,D18-1,D18-1039,https://arxiv.org/abs/1808.08493,"('Emmanouil Antonios Platanios', 'Mrinmaya Sachan', 'Graham Neubig', 'Tom Mitchell')",10,Contextual Parameter Generation for Universal Neural Machine Translation,EMNLP,2018
"Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively.",http://aclweb.org/anthology/D18-1040,D18-1,D18-1040,https://arxiv.org/abs/1808.09006,"('Marzieh Fadaee', 'Christof Monz')",10,Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation,EMNLP,2018
"Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.",http://aclweb.org/anthology/D18-1041,D18-1,D18-1041,https://arxiv.org/abs/1607.01149,"('Jiali Zeng', 'Jinsong Su', 'Huating Wen', 'Yang Liu', 'Jun Xie', 'Yongjing Yin', 'Jianqiang Zhao')",10,Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination,EMNLP,2018
"We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.",http://aclweb.org/anthology/D18-1042,D18-1,D18-1042,https://arxiv.org/abs/1808.09334,"('Sebastian Ruder', 'Ryan Cotterell', 'Yova Kementchedjhieva', 'Anders Søgaard')",10,A Discriminative Latent-Variable Model for Bilingual Lexicon Induction,EMNLP,2018
"Unsupervised machine translation---i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora---seems impossible, but nevertheless, Lample et al. (2018) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised alignment of word embedding spaces for bilingual dictionary induction (Conneau et al., 2018), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction, and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.",http://aclweb.org/anthology/D18-1043,D18-1,D18-1043,https://arxiv.org/abs/1805.03620,"('Yedid Hoshen', 'Lior Wolf')",10,Non-Adversarial Unsupervised Word Translation,EMNLP,2018
"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.",http://aclweb.org/anthology/D18-1044,D18-1,D18-1044,https://arxiv.org/abs/1706.03872,"('Chunqi Wang', 'Ji Zhang', 'Haiqing Chen')",10,Semi-Autoregressive Neural Machine Translation,EMNLP,2018
"Recent MHD dynamo simulations for magnetic Prandtl number $>1$ demonstrate that when MHD turbulence is forced with sufficient kinetic helicity, the saturated magnetic energy spectrum evolves from having a single peak below the forcing scale to become doubly peaked with one peak at the system (=largest) scale and one at the forcing scale. The system scale field growth is well modeled by a recent nonlinear two-scale nonlinear helical dynamo theory in which the system and forcing scales carry magnetic helicity of opposite sign. But a two-scale theory cannot model the shift of the small-scale peak toward the forcing scale. Here I develop a four-scale helical dynamo theory which shows that the small-scale helical magnetic energy first saturates at very small scales, but then successively saturates at larger values at larger scales, eventually becoming dominated by the forcing scale. The transfer of the small scale peak to the forcing scale is completed by the end of the kinematic growth regime of the large scale field, and does not depend on magnetic Reynolds number $R_M$ for large $R_M$. The four-scale and two-scale theories subsequently evolve almost identically, and both show significant field growth on the system and forcing scales that is independent of $R_M$. In the present approach, the helical and nonhelical parts of the spectrum are largely decoupled. Implications for fractionally helical turbulence are discussed.",http://aclweb.org/anthology/D18-1045,D18-1,D18-1045,https://arxiv.org/abs/astro-ph/0301432,"('Sergey Edunov', 'Myle Ott', 'Michael Auli', 'David Grangier')",10,Understanding Back-Translation at Scale,EMNLP,2018
"Generating the English transliteration of a name written in a foreign script is an important and challenging step in multilingual knowledge acquisition and information extraction. Existing approaches to transliteration generation require a large (>5000) number of training examples. This difficulty contrasts with transliteration discovery, a somewhat easier task that involves picking a plausible transliteration from a given list. In this work, we present a bootstrapping algorithm that uses constrained discovery to improve generation, and can be used with as few as 500 training examples, which we show can be sourced from annotators in a matter of hours. This opens the task to languages for which large number of training examples are unavailable. We evaluate transliteration generation performance itself, as well the improvement it brings to cross-lingual candidate generation for entity linking, a typical downstream task. We present a comprehensive evaluation of our approach on nine languages, each written in a unique script.",http://aclweb.org/anthology/D18-1046,D18-1,D18-1046,https://arxiv.org/abs/1809.07807,"('Shyam Upadhyay', 'Jordan Kodner', 'Dan Roth')",10,Bootstrapping Transliteration with Constrained Discovery for Low-Resource Languages,EMNLP,2018
"We analyze a word embedding method in supervised tasks. It maps words on a sphere such that words co-occurring in similar contexts lie closely. The similarity of contexts is measured by the distribution of substitutes that can fill them. We compared word embeddings, including more recent representations, in Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine our framework in multilingual dependency parsing as well. The results show that the proposed method achieves as good as or better results compared to the other word embeddings in the tasks we investigate. It achieves state-of-the-art results in multilingual dependency parsing. Word embeddings in 7 languages are available for public use.",http://aclweb.org/anthology/D18-1047,D18-1,D18-1047,https://arxiv.org/abs/1407.6853,['Ndapa Nakashole'],10,NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings,EMNLP,2018
"Attention-based sequence-to-sequence model has proved successful in Neural Machine Translation (NMT). However, the attention without consideration of decoding history, which includes the past information in the decoder and the attention mechanism, often causes much repetition. To address this problem, we propose the decoding-history-based Adaptive Control of Attention (ACA) for the NMT model. ACA learns to control the attention by keeping track of the decoding history and the current information with a memory vector, so that the model can take the translated contents and the current information into consideration. Experiments on Chinese-English translation and the English-Vietnamese translation have demonstrated that our model significantly outperforms the strong baselines. The analysis shows that our model is capable of generating translation with less repetition and higher accuracy. The code will be available at https://github.com/lancopku",http://aclweb.org/anthology/D18-1048,D18-1,D18-1048,https://arxiv.org/abs/1802.01812,"('Xinwei Geng', 'Xiaocheng Feng', 'Bing Qin', 'Ting Liu')",10,Adaptive Multi-pass Decoder for Neural Machine Translation,EMNLP,2018
"Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.",http://aclweb.org/anthology/D18-1049,D18-1,D18-1049,https://arxiv.org/abs/1810.03581,"('Jiacheng Zhang', 'Huanbo Luan', 'Maosong Sun', 'Feifei Zhai', 'Jingfang Xu', 'Min Zhang', 'Yang Liu')",10,Improving the Transformer Translation Model with Document-Level Context,EMNLP,2018
"Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT. The data is publicly available at www.cs.cmu.edu/~pmichel1/mtnt/.",http://aclweb.org/anthology/D18-1050,D18-1,D18-1050,https://arxiv.org/abs/1809.00388,"('Paul Michel', 'Graham Neubig')",10,MTNT: A Testbed for Machine Translation of Noisy Text,EMNLP,2018
"The SimpleQuestions dataset is one of the most commonly used benchmarks for studying single-relation factoid questions. In this paper, we present new evidence that this benchmark can be nearly solved by standard methods. First we show that ambiguity in the data bounds performance on this benchmark at 83.4%; there are often multiple answers that cannot be disambiguated from the linguistic signal alone. Second we introduce a baseline that sets a new state-of-the-art performance level at 78.1% accuracy, despite using standard methods. Finally, we report an empirical analysis showing that the upperbound is loose; roughly a third of the remaining errors are also not resolvable from the linguistic signal. Together, these results suggest that the SimpleQuestions dataset is nearly solved.",http://aclweb.org/anthology/D18-1051,D18-1,D18-1051,https://arxiv.org/abs/1804.08798,"('Michael Petrochuk', 'Luke Zettlemoyer')",10,SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach,EMNLP,2018
"We formalize a new modular variant of current question answering tasks by enforcing complete independence of the document encoder from the question encoder. This formulation addresses a key challenge in machine comprehension by requiring a standalone representation of the document discourse. It additionally leads to a significant scalability advantage since the encoding of the answer candidate phrases in the document can be pre-computed and indexed offline for efficient retrieval. We experiment with baseline models for the new task, which achieve a reasonable accuracy but significantly underperform unconstrained QA models. We invite the QA research community to engage in Phrase-Indexed Question Answering (PIQA, pika) for closing the gap. The leaderboard is at: nlp.cs.washington.edu/piqa",http://aclweb.org/anthology/D18-1052,D18-1,D18-1052,https://arxiv.org/abs/1804.07726,"('Minjoon Seo', 'Tom Kwiatkowski', 'Ankur Parikh', 'Ali Farhadi', 'Hannaneh Hajishirzi')",10,Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension,EMNLP,2018
"Recently, open-domain question answering (QA) has been combined with machine comprehension models to find answers in a large knowledge source. As open-domain QA requires retrieving relevant documents from text corpora to answer questions, its performance largely depends on the performance of document retrievers. However, since traditional information retrieval systems are not effective in obtaining documents with a high probability of containing answers, they lower the performance of QA systems. Simply extracting more documents increases the number of irrelevant documents, which also degrades the performance of QA systems. In this paper, we introduce Paragraph Ranker which ranks paragraphs of retrieved documents for a higher answer recall with less noise. We show that ranking paragraphs and aggregating answers using Paragraph Ranker improves performance of open-domain QA pipeline on the four open-domain QA datasets by 7.8% on average.",http://aclweb.org/anthology/D18-1053,D18-1,D18-1053,https://arxiv.org/abs/1810.00494,"('Jinhyuk Lee', 'Seongjun Yun', 'Hyunjae Kim', 'Miyoung Ko', 'Jaewoo Kang')",10,Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering,EMNLP,2018
"Progress in text understanding has been driven by large datasets that test particular capabilities, like recent datasets for reading comprehension (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a reading comprehension problem and apply comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 49%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed.",http://aclweb.org/anthology/D18-1054,D18-1,D18-1054,https://arxiv.org/abs/1610.08431,"('Sathish Reddy Indurthi', 'Seunghak Yu', 'Seohyun Back', 'Heriberto Cuayahuitl')",10,Cut to the Chase: A Context Zoom-in Network for Reading Comprehension,EMNLP,2018
"State-of-the-art systems in deep question answering proceed as follows: (1) an initial document retrieval selects relevant documents, which (2) are then processed by a neural network in order to extract the final answer. Yet the exact interplay between both components is poorly understood, especially concerning the number of candidate documents that should be retrieved. We show that choosing a static number of documents -- as used in prior research -- suffers from a noise-information trade-off and yields suboptimal results. As a remedy, we propose an adaptive document retrieval model. This learns the optimal candidate number for document retrieval, conditional on the size of the corpus and the query. We report extensive experimental results showing that our adaptive approach outperforms state-of-the-art methods on multiple benchmark datasets, as well as in the context of corpora with variable sizes.",http://aclweb.org/anthology/D18-1055,D18-1,D18-1055,https://arxiv.org/abs/1808.06528,"('Bernhard Kratzwald', 'Stefan Feuerriegel')",10,Adaptive Document Retrieval for Deep Question Answering,EMNLP,2018
"This paper presents a challenge to the community: Generative adversarial networks (GANs) can perfectly align independent English word embeddings induced using the same algorithm, based on distributional information alone; but fails to do so, for two different embeddings algorithms. Why is that? We believe understanding why, is key to understand both modern word embedding algorithms and the limitations and instability dynamics of GANs. This paper shows that (a) in all these cases, where alignment fails, there exists a linear transform between the two embeddings (so algorithm biases do not lead to non-linear differences), and (b) similar effects can not easily be obtained by varying hyper-parameters. One plausible suggestion based on our initial experiments is that the differences in the inductive biases of the embedding algorithms lead to an optimization landscape that is riddled with local optima, leading to a very small basin of convergence, but we present this more as a challenge paper than a technical contribution.",http://aclweb.org/anthology/D18-1056,D18-1,D18-1056,https://arxiv.org/abs/1809.00150,"('Mareike Hartmann', 'Yova Kementchedjhieva', 'Anders Søgaard')",10,Why is unsupervised alignment of English embeddings from different algorithms so hard?,EMNLP,2018
"There have been some works that learn a lexicon together with the corpus to improve the word embeddings. However, they either model the lexicon separately but update the neural networks for both the corpus and the lexicon by the same likelihood, or minimize the distance between all of the synonym pairs in the lexicon. Such methods do not consider the relatedness and difference of the corpus and the lexicon, and may not be the best optimized. In this paper, we propose a novel method that considers the relatedness and difference of the corpus and the lexicon. It trains word embeddings by learning the corpus to predicate a word and its corresponding synonym under the context at the same time. For polysemous words, we use a word sense disambiguation filter to eliminate the synonyms that have different meanings for the context. To evaluate the proposed method, we compare the performance of the word embeddings trained by our proposed model, the control groups without the filter or the lexicon, and the prior works in the word similarity tasks and text classification task. The experimental results show that the proposed model provides better embeddings for polysemous words and improves the performance for text classification.",http://aclweb.org/anthology/D18-1057,D18-1,D18-1057,https://arxiv.org/abs/1707.07628,"('Yimeng Zhuang', 'Jinghui Xie', 'Yinhe Zheng', 'Xuan Zhu')",10,Quantifying Context Overlap for Training Word Embeddings,EMNLP,2018
"Capturing the semantic relations of words in a vector space contributes to many natural language processing tasks. One promising approach exploits lexico-syntactic patterns as features of word pairs. In this paper, we propose a novel model of this pattern-based approach, neural latent relational analysis (NLRA). NLRA can generalize co-occurrences of word pairs and lexico-syntactic patterns, and obtain embeddings of the word pairs that do not co-occur. This overcomes the critical data sparseness problem encountered in previous pattern-based models. Our experimental results on measuring relational similarity demonstrate that NLRA outperforms the previous pattern-based models. In addition, when combined with a vector offset model, NLRA achieves a performance comparable to that of the state-of-the-art model that exploits additional semantic relational data.",http://aclweb.org/anthology/D18-1058,D18-1,D18-1058,https://arxiv.org/abs/1809.03401,"('Koki Washio', 'Tsuneaki Kato')",10,Neural Latent Relational Analysis to Capture Lexical Semantic Relations in a Vector Space,EMNLP,2018
"We approach the problem of generalizing pre-trained word embeddings beyond fixed-size vocabularies without using additional contextual information. We propose a subword-level word vector generation model that views words as bags of character $n$-grams. The model is simple, fast to train and provides good vectors for rare or unseen words. Experiments show that our model achieves state-of-the-art performances in English word similarity task and in joint prediction of part-of-speech tag and morphosyntactic attributes in 23 languages, suggesting our model's ability in capturing the relationship between words' textual representations and their embeddings.",http://aclweb.org/anthology/D18-1059,D18-1,D18-1059,https://arxiv.org/abs/1809.04259,"('Jinman Zhao', 'Sidharth Mudgal', 'Yingyu Liang')",10,Generalizing Word Embeddings using Bag of Subwords,EMNLP,2018
"We present end-to-end neural models for detecting metaphorical word use in context. We show that relatively standard BiLSTM models which operate on complete sentences work well in this setting, in comparison to previous work that used more restricted forms of linguistic context. These models establish a new state-of-the-art on existing verb metaphor detection benchmarks, and show strong performance on jointly predicting the metaphoricity of all words in a running text.",http://aclweb.org/anthology/D18-1060,D18-1,D18-1060,https://arxiv.org/abs/1808.09653,"('Ge Gao', 'Eunsol Choi', 'Yejin Choi', 'Luke Zettlemoyer')",10,Neural Metaphor Detection in Context,EMNLP,2018
"We introduce DsDs: a cross-lingual neural part-of-speech tagger that learns from disparate sources of distant supervision, and realistically scales to hundreds of low-resource languages. The model exploits annotation projection, instance selection, tag dictionaries, morphological lexicons, and distributed representations, all in a uniform framework. The approach is simple, yet surprisingly effective, resulting in a new state of the art without access to any gold annotated data.",http://aclweb.org/anthology/D18-1061,D18-1,D18-1061,https://arxiv.org/abs/1808.09733,"('Barbara Plank', 'Željko Agić')",10,Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging,EMNLP,2018
"We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.",http://aclweb.org/anthology/D18-1062,D18-1,D18-1062,https://arxiv.org/abs/1808.09334,"('Zi-Yi Dou', 'Zhi-Hao Zhou', 'Shujian Huang')",10,Unsupervised Bilingual Lexicon Induction via Latent Variable Models,EMNLP,2018
"Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.",http://aclweb.org/anthology/D18-1063,D18-1,D18-1063,https://arxiv.org/abs/1805.07467,"('Tanmoy Mukherjee', 'Makoto Yamada', 'Timothy Hospedales')",10,Learning Unsupervised Word Translations Without Adversaries,EMNLP,2018
"Dialogue Act (DA) classification is a challenging problem in dialogue interpretation, which aims to attach semantic labels to utterances and characterize the speaker's intention. Currently, many existing approaches formulate the DA classification problem ranging from multi-classification to structured prediction, which suffer from two limitations: a) these methods are either handcrafted feature-based or have limited memories. b) adversarial examples can't be correctly classified by traditional training methods. To address these issues, in this paper we first cast the problem into a question and answering problem and proposed an improved dynamic memory networks with hierarchical pyramidal utterance encoder. Moreover, we apply adversarial training to train our proposed model. We evaluate our model on two public datasets, i.e., Switchboard dialogue act corpus and the MapTask corpus. Extensive experiments show that our proposed model is not only robust, but also achieves better performance when compared with some state-of-the-art baselines.",http://aclweb.org/anthology/D18-1064,D18-1,D18-1064,https://arxiv.org/abs/1811.05021,"('Ryo Masumura', 'Yusuke Shinohara', 'Ryuichiro Higashinaka', 'Yushi Aono')",10,Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification,EMNLP,2018
"A strongly polynomial sequence of graphs $(G_n)$ is a sequence $(G_n)_{n\in\mathbb{N}}$ of finite graphs such that, for every graph $F$, the number of homomorphisms from $F$ to $G_n$ is a fixed polynomial function of $n$ (depending on $F$). For example, $(K_n)$ is strongly polynomial since the number of homomorphisms from $F$ to $K_n$ is the chromatic polynomial of $F$ evaluated at $n$. In earlier work of de la Harpe and Jaeger, and more recently of Averbouch, Garijo, Godlin, Goodall, Makowsky, Ne\v{s}et\v{r}il, Tittmann, Zilber and others, various examples of strongly polynomial sequences and constructions for families of such sequences have been found.   We give a new model-theoretic method of constructing strongly polynomial sequences of graphs that uses interpretation schemes of graphs in more general relational structures. This surprisingly easy yet general method encompasses all previous constructions and produces many more. We conjecture that, under mild assumptions, all strongly polynomial sequences of graphs can be produced by the general method of quantifier-free interpretation of graphs in certain basic relational structures (essentially disjoint unions of transitive tournaments with added unary relations). We verify this conjecture for strongly polynomial sequences of graphs with uniformly bounded degree.",http://aclweb.org/anthology/D18-1065,D18-1,D18-1065,https://arxiv.org/abs/1405.2449,"('Shiv Shankar', 'Siddhant Garg', 'Sunita Sarawagi')",10,Surprisingly Easy Hard-Attention for Sequence to Sequence Learning,EMNLP,2018
"It is important for machines to interpret human emotions properly for better human-machine communications, as emotion is an essential part of human-to-human communications. One aspect of emotion is reflected in the language we use. How to represent emotions in texts is a challenge in natural language processing (NLP). Although continuous vector representations like word2vec have become the new norm for NLP problems, their limitations are that they do not take emotions into consideration and can unintentionally contain bias toward certain identities like different genders.   This thesis focuses on improving existing representations in both word and sentence levels by explicitly taking emotions inside text and model bias into account in their training process. Our improved representations can help to build more robust machine learning models for affect-related text classification like sentiment/emotion analysis and abusive language detection.   We first propose representations called emotional word vectors (EVEC), which is learned from a convolutional neural network model with an emotion-labeled corpus, which is constructed using hashtags. Secondly, we extend to learning sentence-level representations with a huge corpus of texts with the pseudo task of recognizing emojis. Our results show that, with the representations trained from millions of tweets with weakly supervised labels such as hashtags and emojis, we can solve sentiment/emotion analysis tasks more effectively.   Lastly, as examples of model bias in representations of existing approaches, we explore a specific problem of automatic detection of abusive language. We address the issue of gender bias in various neural network models by conducting experiments to measure and reduce those biases in the representations in order to build more robust classification models.",http://aclweb.org/anthology/D18-1066,D18-1,D18-1066,https://arxiv.org/abs/1808.07235,"('Ying Chen', 'Wenjun Hou', 'Xiyao Cheng', 'Shoushan Li')",10,Joint Learning for Emotion Classification and Emotion Cause Detection,EMNLP,2018
"In this paper, we propose the Quantile Option Architecture (QUOTA) for exploration based on recent advances in distributional reinforcement learning (RL). In QUOTA, decision making is based on quantiles of a value distribution, not only the mean. QUOTA provides a new dimension for exploration via making use of both optimism and pessimism of a value distribution. We demonstrate the performance advantage of QUOTA in both challenging video games and physical robot simulators.",http://aclweb.org/anthology/D18-1067,D18-1,D18-1067,https://arxiv.org/abs/1811.02073,"('Cornelia Caragea', 'Liviu P. Dinu', 'Bogdan Dumitru')",10,Exploring Optimism and Pessimism in Twitter Using Deep Learning,EMNLP,2018
"We present a software tool that employs state-of-the-art natural language processing (NLP) and machine learning techniques to help newspaper editors compose effective headlines for online publication. The system identifies the most salient keywords in a news article and ranks them based on both their overall popularity and their direct relevance to the article. The system also uses a supervised regression model to identify headlines that are likely to be widely shared on social media. The user interface is designed to simplify and speed the editor's decision process on the composition of the headline. As such, the tool provides an efficient way to combine the benefits of automated predictors of engagement and search-engine optimization (SEO) with human judgments of overall headline quality.",http://aclweb.org/anthology/D18-1068,D18-1,D18-1068,https://arxiv.org/abs/1705.09656,"('Sotiris Lamprinidis', 'Daniel Hardt', 'Dirk Hovy')",10,Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning,EMNLP,2018
"Attention-based end-to-end (E2E) speech recognition models such as Listen, Attend, and Spell (LAS) can achieve better results than traditional automatic speech recognition (ASR) hybrid models on LVCSR tasks. LAS combines acoustic, pronunciation and language model components of a traditional ASR system into a single neural network. However, such architectures are hard to be used for streaming speech recognition for its bidirectional listener architecture and attention mechanism. In this work, we propose to use latency-controlled bidirectional long short-term memory (LC- BLSTM) listener to reduce the delay of forward computing of listener. On the attention side, we propose an adaptive monotonic chunk-wise attention (AMoChA) to make LAS online. We explore how each part performs when it is used alone and obtain comparable or better results than LAS baseline. By combining the above two methods, we successfully stream LAS baseline with only 3.5% relative degradation of character error rate (CER) on our Mandarin corpus. We believe that our methods can also have the same effect on other languages.",http://aclweb.org/anthology/D18-1069,D18-1,D18-1069,https://arxiv.org/abs/1811.05247,"('Di Chen', 'Jiachen Du', 'Lidong Bing', 'Ruifeng Xu')",10,Hybrid Neural Attention for Agreement/Disagreement Inference in Online Debates,EMNLP,2018
"Knowledge graphs are a versatile framework to encode richly structured data relationships, but it can be challenging to combine these graphs with unstructured data. Methods for retrofitting pre-trained entity representations to the structure of a knowledge graph typically assume that entities are embedded in a connected space and that relations imply similarity. However, useful knowledge graphs often contain diverse entities and relations (with potentially disjoint underlying corpora) which do not accord with these assumptions. To overcome these limitations, we present Functional Retrofitting, a framework that generalizes current retrofitting methods by explicitly modeling pairwise relations. Our framework can directly incorporate a variety of pairwise penalty functions previously developed for knowledge graph completion. Further, it allows users to encode, learn, and extract information about relation semantics. We present both linear and neural instantiations of the framework. Functional Retrofitting significantly outperforms existing retrofitting methods on complex knowledge graphs and loses no accuracy on simpler graphs (in which relations do imply similarity). Finally, we demonstrate the utility of the framework by predicting new drug--disease treatment pairs in a large, complex health knowledge graph.",http://aclweb.org/anthology/D18-1070,D18-1,D18-1070,https://arxiv.org/abs/1708.00112,"('Dirk Hovy', 'Tommaso Fornaciari')",10,Increasing In-Class Similarity by Retrofitting Embeddings with Demographic Information,EMNLP,2018
"Traditional neural language models tend to generate generic replies with poor logic and no emotion. In this paper, a syntactically constrained bidirectional-asynchronous approach for emotional conversation generation (E-SCBA) is proposed to address this issue. In our model, pre-generated emotion keywords and topic keywords are asynchronously introduced into the process of decoding. It is much different from most existing methods which generate replies from the first word to the last. Through experiments, the results indicate that our approach not only improves the diversity of replies, but gains a boost on both logic and emotion compared with baselines.",http://aclweb.org/anthology/D18-1071,D18-1,D18-1071,https://arxiv.org/abs/1806.07000,"('Jingyuan Li', 'Xiao Sun')",10,A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation,EMNLP,2018
"The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user's intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.",http://aclweb.org/anthology/D18-1072,D18-1,D18-1072,https://arxiv.org/abs/1605.07669,"('Chen Shi', 'Qi Chen', 'Lei Sha', 'Sujian Li', 'Xu Sun', 'Houfeng Wang', 'Lintao Zhang')",10,Auto-Dialabel: Labeling Dialogue Data with Unsupervised Learning,EMNLP,2018
"The use of connectionist approaches in conversational agents has been progressing rapidly due to the availability of large corpora. However current generative dialogue models often lack coherence and are content poor. This work proposes an architecture to incorporate unstructured knowledge sources to enhance the next utterance prediction in chit-chat type of generative dialogue models. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with the Reddit News dataset, and consider incorporating external knowledge from Wikipedia summaries as well as from the NELL knowledge base. Our experiments show faster training time and improved perplexity when leveraging external knowledge.",http://aclweb.org/anthology/D18-1073,D18-1,D18-1073,https://arxiv.org/abs/1809.05524,"('Prasanna Parthasarathi', 'Joelle Pineau')",10,Extending Neural Generative Conversational Model using External Knowledge Sources,EMNLP,2018
"Human teams can be exceptionally efficient at adapting and collaborating during manipulation tasks using shared mental models. However, the same shared mental models that can be used by humans to perform robust low-level force and motion control during collaborative manipulation tasks are non-existent for robots. For robots to perform collaborative tasks with people naturally and efficiently, understanding and predicting human intent is necessary. However, humans are difficult to predict and model. We have completed an exploratory study recording motion and force for 20 human dyads moving an object in tandem in order to better understand how they move and how their movement can be predicted. In this paper, we show how past motion data can be used to predict human intent. In order to predict human intent, which we equate with the human team's velocity for a short time horizon, we used a neural network. Using the previous 150 time steps at a rate of 200 Hz, human intent can be predicted for the next 50 time steps with a mean squared error of 0.02 (m/s)^2. We also show that human intent can be estimated in a human-robot dyad. This work is an important first step in enabling future work of integrating human intent estimation on a robot controller to execute a short-term collaborative trajectory.",http://aclweb.org/anthology/D18-1074,D18-1,D18-1074,https://arxiv.org/abs/1705.10851,"('Xiaolei Huang', 'Lixing Liu', 'Kate Carey', 'Joshua Woolley', 'Stefan Scherer', 'Brian Borsari')",10,Modeling Temporality of Human Intentions by Domain Adaptation,EMNLP,2018
"Generating semantically coherent responses is still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and responses in conversations is more complicated, which highly demands the understanding of utterance-level semantic dependency, a relation between the whole meanings of inputs and outputs. To address this problem, we propose an Auto-Encoder Matching (AEM) model to learn such dependency. The model contains two auto-encoders and one mapping module. The auto-encoders learn the semantic representations of inputs and responses, and the mapping module learns to connect the utterance-level representations. Experimental results from automatic and human evaluations demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models. The code is available at https://github.com/lancopku/AMM",http://aclweb.org/anthology/D18-1075,D18-1,D18-1075,https://arxiv.org/abs/1808.08795,"('Liangchen Luo', 'Jingjing Xu', 'Junyang Lin', 'Qi Zeng', 'Xu Sun')",10,An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation,EMNLP,2018
"This paper introduces a document grounded dataset for text conversations. We define ""Document Grounded Conversations"" as conversations that are about the contents of a specified document. In this dataset the specified documents were Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation. This positions this dataset to not only provide a relevant chat history while generating responses but also provide a source of information that the models could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for engagement and fluency, and find that the information from the document helps in generating more engaging and fluent responses.",http://aclweb.org/anthology/D18-1076,D18-1,D18-1076,https://arxiv.org/abs/1809.07358,"('Kangyan Zhou', 'Shrimai Prabhumoye', 'Alan W Black')",10,A Dataset for Document Grounded Conversations,EMNLP,2018
"Deep learning has greatly improved visual recognition in recent years. However, recent research has shown that there exist many adversarial examples that can negatively impact the performance of such an architecture. This paper focuses on detecting those adversarial examples by analyzing whether they come from the same distribution as the normal examples. Instead of directly training a deep neural network to detect adversarials, a much simpler approach was proposed based on statistics on outputs from convolutional layers. A cascade classifier was designed to efficiently detect adversarials. Furthermore, trained from one particular adversarial generating mechanism, the resulting classifier can successfully detect adversarials from a completely different mechanism as well. The resulting classifier is non-subdifferentiable, hence creates a difficulty for adversaries to attack by using the gradient of the classifier. After detecting adversarial examples, we show that many of them can be recovered by simply performing a small average filter on the image. Those findings should lead to more insights about the classification mechanisms in deep convolutional neural networks.",http://aclweb.org/anthology/D18-1077,D18-1,D18-1077,https://arxiv.org/abs/1612.07767,"('Seonghan Ryu', 'Sangjun Koo', 'Hwanjo Yu', 'Gary Geunbae Lee')",10,Out-of-domain Detection based on Generative Adversarial Network,EMNLP,2018
"Multimedia or spoken content presents more attractive information than plain text content, but it's more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's highly attractive to develop a machine which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, we propose a new task of machine comprehension of spoken content. We define the initial goal as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native language is not English. We further propose an Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture for this task, achieving encouraging results in the initial tests. Initial results also have shown that word-level attention is probably more robust than sentence-level attention for this task with ASR errors.",http://aclweb.org/anthology/D18-1078,D18-1,D18-1078,https://arxiv.org/abs/1608.06378,"('Shachar Mirkin', 'Guy Moshkowich', 'Matan Orbach', 'Lili Kotlerman', 'Yoav Kantor', 'Tamar Lavee', 'Michal Jacovi', 'Yonatan Bilu', 'Ranit Aharonov', 'Noam Slonim')",10,Listening Comprehension over Argumentative Content,EMNLP,2018
"Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce neural discourse relation models to approximate the prior and posterior distributions of the latent variable, and employ these approximated distributions to optimize a reparameterized variational lower bound. This allows VarNDRR to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VarNDRR can achieve comparable results against stateof- the-art baselines without using any manual features.",http://aclweb.org/anthology/D18-1079,D18-1,D18-1079,https://arxiv.org/abs/1603.03876,"('Yang Xu', 'Yu Hong', 'Huibin Ruan', 'Jianmin Yao', 'Min Zhang', 'Guodong Zhou')",10,Using active learning to expand training data for implicit discourse relation recognition,EMNLP,2018
"Split and rephrase is the task of breaking down a sentence into shorter ones that together convey the same meaning. We extract a rich new dataset for this task by mining Wikipedia's edit history: WikiSplit contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task. Incorporating WikiSplit as training data produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark.",http://aclweb.org/anthology/D18-1080,D18-1,D18-1080,https://arxiv.org/abs/1808.09468,"('Jan A. Botha', 'Manaal Faruqui', 'John Alex', 'Jason Baldridge', 'Dipanjan Das')",10,Learning To Split and Rephrase From Wikipedia Edit History,EMNLP,2018
"BLEU is widely considered to be an informative metric for text-to-text generation, including Text Simplification (TS). TS includes both lexical and structural aspects. In this paper we show that BLEU is not suitable for the evaluation of sentence splitting, the major structural simplification operation. We manually compiled a sentence splitting gold standard corpus containing multiple structural paraphrases, and performed a correlation analysis with human judgments. We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences.",http://aclweb.org/anthology/D18-1081,D18-1,D18-1081,https://arxiv.org/abs/1810.05995,"('Elior Sulem', 'Omri Abend', 'Ari Rappoport')",10,BLEU is Not Suitable for the Evaluation of Text Simplification,EMNLP,2018
"Using a sequence-to-sequence framework, many neural conversation models for chit-chat succeed in naturalness of the response. Nevertheless, the neural conversation models tend to give generic responses which are not specific to given messages, and it still remains as a challenge. To alleviate the tendency, we propose a method to promote message-relevant and diverse responses for neural conversation model by using self-attention, which is time-efficient as well as effective. Furthermore, we present an investigation of why and how effective self-attention is in deep comparison with the standard dialogue generation. The experiment results show that the proposed method improves the standard dialogue generation in various evaluation metrics.",http://aclweb.org/anthology/D18-1082,D18-1,D18-1082,https://arxiv.org/abs/1805.08983,"('Jiaxin Pei', 'Chenliang Li')",10,S2SPMN: A Simple and Effective Framework for Response Generation with Relevant Information,EMNLP,2018
"Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the metric used for testing. However, this shaped reward introduces learning biases, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow. To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on benchmarks show that RL with the simple add-on module performs favorably against its counterpart in terms of both readability and speed of convergence. Human evaluation results show that our model is more human readable and graceful. The implementation will become publicly available upon the acceptance of the paper.",http://aclweb.org/anthology/D18-1083,D18-1,D18-1083,https://arxiv.org/abs/1809.06227,"('Tszhang Guo', 'Shiyu Chang', 'Mo Yu', 'Kun Bai')",10,Improving Reinforcement Learning Based Image Captioning with Natural Language Prior,EMNLP,2018
"We study how to generate captions that are not only accurate in describing an image but also discriminative across different images. The problem is both fundamental and interesting, as most machine-generated captions, despite phenomenal research progresses in the past several years, are expressed in a very monotonic and featureless format. While such captions are normally accurate, they often lack important characteristics in human languages - distinctiveness for each caption and diversity for different images. To address this problem, we propose a novel conditional generative adversarial network for generating diverse captions across images. Instead of estimating the quality of a caption solely on one image, the proposed comparative adversarial learning framework better assesses the quality of captions by comparing a set of captions within the image-caption joint space. By contrasting with human-written captions and image-mismatched captions, the caption generator effectively exploits the inherent characteristics of human languages, and generates more discriminative captions. We show that our proposed network is capable of producing accurate and diverse captions across images.",http://aclweb.org/anthology/D18-1084,D18-1,D18-1084,https://arxiv.org/abs/1804.00861,"('Luke Melas-Kyriazi', 'Alexander Rush', 'George Han')",10,Training for Diversity in Image Paragraph Captioning,EMNLP,2018
"Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.",http://aclweb.org/anthology/D18-1085,D18-1,D18-1085,https://arxiv.org/abs/1604.00400,"('Elaheh ShafieiBavani', 'Mohammad Ebrahimi', 'Raymond Wong', 'Fang Chen')",10,A Graph-theoretic Summary Evaluation for ROUGE,EMNLP,2018
"Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance using the latter is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset. Code is available at \url{https://github.com/sheffieldnlp/AMR2Text-summ}",http://aclweb.org/anthology/D18-1086,D18-1,D18-1086,https://arxiv.org/abs/1808.09160,"('Hardy Hardy', 'Andreas Vlachos')",10,Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation,EMNLP,2018
"Video summaries come in many forms, from traditional single-image thumbnails, animated thumbnails, storyboards, to trailer-like video summaries. Content creators use the summaries to display the most attractive portion of their videos; the users use them to quickly evaluate if a video is worth watching. All forms of summaries are essential to video viewers, content creators, and advertisers. Often video content management systems have to generate multiple versions of summaries that vary in duration and presentational forms. We present a framework ReconstSum that utilizes LSTM-based autoencoder architecture to extract and select a sparse subset of video frames or keyshots that optimally represent the input video in an unsupervised manner. The encoder selects a subset from the input video while the decoder seeks to reconstruct the video from the selection. The goal is to minimize the difference between the original input video and the reconstructed video. Our method is easily extendable to generate a variety of applications including static video thumbnails, animated thumbnails, storyboards and ""trailer-like"" highlights. We specifically study and evaluate two most popular use cases: thumbnail generation and storyboard generation. We demonstrate that our methods generate better results than the state-of-the-art techniques in both use cases.",http://aclweb.org/anthology/D18-1087,D18-1,D18-1087,https://arxiv.org/abs/1808.00184,"('Ori Shapira', 'David Gabay', 'Hadar Ronen', 'Judit Bar-Ilan', 'Yael Amsterdamer', 'Ani Nenkova', 'Ido Dagan')",10,Evaluating Multiple System Summary Lengths: A Case Study,EMNLP,2018
"Extractive summarization models require sentence-level labels, which are usually created heuristically (e.g., with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes \emph{directly} from gold summaries. Experiments on the CNN/Dailymail dataset show that our model improves over a strong extractive baseline trained on heuristically approximated labels and also performs competitively to several recent models.",http://aclweb.org/anthology/D18-1088,D18-1,D18-1088,https://arxiv.org/abs/1808.07187,"('Xingxing Zhang', 'Mirella Lapata', 'Furu Wei', 'Ming Zhou')",10,Neural Latent Extractive Document Summarization,EMNLP,2018
"Till now, neural abstractive summarization methods have achieved great success for single document summarization (SDS). However, due to the lack of large scale multi-document summaries, such methods can be hardly applied to multi-document summarization (MDS). In this paper, we investigate neural abstractive methods for MDS by adapting a state-of-the-art neural abstractive summarization model for SDS. We propose an approach to extend the neural abstractive model trained on large scale SDS data to the MDS task. Our approach only makes use of a small number of multi-document summaries for fine tuning. Experimental results on two benchmark DUC datasets demonstrate that our approach can outperform a variety of baseline neural models.",http://aclweb.org/anthology/D18-1089,D18-1,D18-1089,https://arxiv.org/abs/1804.09010,"('Fangfang Zhang', 'Jin-ge Yao', 'Rui Yan')",10,On the Abstractiveness of Neural Document Summarization,EMNLP,2018
"We introduce a novel schema for sequence to sequence learning with a Deep Q-Network (DQN), which decodes the output sequence iteratively. The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts. Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal states of and formulate a list of potential actions for the DQN. Take rephrasing a natural sentence as an example. This list can contain ranked potential words. Next, the DQN learns to make decision on which action (e.g., word) will be selected from the list to modify the current decoded sequence. The newly modified output sequence is subsequently used as the input to the DQN for the next decoding iteration. In each iteration, we also bias the reinforcement learning's attention to explore sequence portions which are previously difficult to be decoded. For evaluation, the proposed strategy was trained to decode ten thousands natural sentences. Our experiments indicate that, when compared to a left-to-right greedy beam search LSTM decoder, the proposed method performed competitively well when decoding sentences from the training set, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU score obtained.",http://aclweb.org/anthology/D18-1090,D18-1,D18-1090,https://arxiv.org/abs/1510.09202,"('Yucheng Wang', 'Zhongyu Wei', 'Yaqian Zhou', 'Xuanjing Huang')",10,Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning,EMNLP,2018
"Question answering (QA) system aims at retrieving precise information from a large collection of documents against a query. This paper describes the architecture of a Natural Language Question Answering (NLQA) system for a specific domain based on the ontological information, a step towards semantic web question answering. The proposed architecture defines four basic modules suitable for enhancing current QA capabilities with the ability of processing complex questions. The first module was the question processing, which analyses and classifies the question and also reformulates the user query. The second module allows the process of retrieving the relevant documents. The next module processes the retrieved documents, and the last module performs the extraction and generation of a response. Natural language processing techniques are used for processing the question and documents and also for answer extraction. Ontology and domain knowledge are used for reformulating queries and identifying the relations. The aim of the system is to generate short and specific answer to the question that is asked in the natural language in a specific domain. We have achieved 94 % accuracy of natural language question answering in our implementation.",http://aclweb.org/anthology/D18-1091,D18-1,D18-1091,https://arxiv.org/abs/1311.3175,"('Manaal Faruqui', 'Dipanjan Das')",10,Identifying Well-formed Natural Language Questions,EMNLP,2018
"Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.",http://aclweb.org/anthology/D18-1092,D18-1,D18-1092,https://arxiv.org/abs/1603.03827,"('Sujith Ravi', 'Zornitsa Kozareva')",10,Self-Governing Neural Networks for On-Device Short Text Classification,EMNLP,2018
"Many applications require categorization of text documents using predefined categories. The main approach to performing text categorization is learning from labeled examples. For many tasks, it may be difficult to find examples in one language but easy in others. The problem of learning from examples in one or more languages and classifying (categorizing) in another is called cross-lingual learning. In this work, we present a novel approach that solves the general cross-lingual text categorization problem. Our method generates, for each training document, a set of language-independent features. Using these features for training yields a language-independent classifier. At the classification stage, we generate language-independent features for the unlabeled document, and apply the classifier on the new representation.   To build the feature generator, we utilize a hierarchical language-independent ontology, where each concept has a set of support documents for each language involved. In the preprocessing stage, we use the support documents to build a set of language-independent feature generators, one for each language. The collection of these generators is used to map any document into the language-independent feature space.   Our methodology works on the most general cross-lingual text categorization problems, being able to learn from any mix of languages and classify documents in any other language. We also present a method for exploiting the hierarchical structure of the ontology to create virtual supporting documents for languages that do not have them. We tested our method, using Wikipedia as our ontology, on the most commonly used test collections in cross-lingual text categorization, and found that it outperforms existing methods.",http://aclweb.org/anthology/D18-1093,D18-1,D18-1093,https://arxiv.org/abs/1802.04028,"('Kazuya Shimura', 'Jiyi Li', 'Fumiyo Fukumoto')",10,HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization,EMNLP,2018
"We explore solutions for automated labeling of content in bug trackers and customer support systems. In order to do that, we classify content in terms of several criteria, such as priority or product area. In the first part of the paper, we provide an overview of existing methods used for text classification. These methods fall into two categories - the ones that rely on neural networks and the ones that don't. We evaluate results of several solutions of both kinds. In the second part of the paper we present our own recurrent neural network solution based on hierarchical attention paradigm. It consists of several Hierarchical Attention network blocks with varying Gated Recurrent Unit cell sizes and a complementary shallow network that goes alongside. Lastly, we evaluate above-mentioned methods when predicting fields from two datasets - Arch Linux bug tracker and Chromium bug tracker. Our contributions include a comprehensive benchmark between a variety of methods on relevant datasets; a novel solution that outperforms previous generation methods; and two new datasets that are made public for further research.",http://aclweb.org/anthology/D18-1094,D18-1,D18-1094,https://arxiv.org/abs/1807.02892,"('Koustuv Sinha', 'Yue Dong', 'Jackie Chi Kit Cheung', 'Derek Ruths')",10,A Hierarchical Neural Attention-based Text Classifier,EMNLP,2018
"We analyze the RI-TIMEXes in temporally annotated corpora and propose two hypotheses regarding the normalization of RI-TIMEXes in the clinical narrative domain: the anchor point hypothesis and the anchor relation hypothesis. We annotate the RI-TIMEXes in three corpora to study the characteristics of RI-TMEXes in different domains. This informed the design of our RI-TIMEX normalization system for the clinical domain, which consists of an anchor point classifier, an anchor relation classifier and a rule-based RI-TIMEX text span parser. We experiment with different feature sets and perform error analysis for each system component. The annotation confirmed the hypotheses that we can simplify the RI-TIMEXes normalization task using two multi-label classifiers. Our system achieves anchor point classification, anchor relation classification and rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% under relaxed matching criteria) respectively on the held-out test set of the 2012 i2b2 temporal relation challenge. Experiments with feature sets reveals some interesting findings such as the verbal tense feature does not inform the anchor relation classification in clinical narratives as much as the tokens near the RI-TIMEX. Error analysis shows that underrepresented anchor point and anchor relation classes are difficult to detect. We formulate the RI-TIMEX normalization problem as a pair of multi-label classification problems. Considering only the RI-TIMEX extraction and normalization, the system achieves statistically significant improvement over the RI-TIMEX results of the best systems in the 2012 i2b2 challenge.",http://aclweb.org/anthology/D18-1095,D18-1,D18-1095,https://arxiv.org/abs/1510.04972,"('Jeffrey Lund', 'Stephen Cowley', 'Wilson Fearn', 'Emily Hales', 'Kevin Seppi')",10,"Labeled Anchors and a Scalable, Transparent, and Interactive Classifier",EMNLP,2018
"Neural models have recently been used in text summarization including headline generation. The model can be trained using a set of document-headline pairs. However, the model does not explicitly consider topical similarities and differences of documents. We suggest to categorizing documents into various topics so that documents within the same topic are similar in content and share similar summarization patterns. Taking advantage of topic information of documents, we propose topic sensitive neural headline generation model. Our model can generate more accurate summaries guided by document topics. We test our model on LCSTS dataset, and experiments show that our method outperforms other baselines on each topic and achieves the state-of-art performance.",http://aclweb.org/anthology/D18-1096,D18-1,D18-1096,https://arxiv.org/abs/1608.05777,"('Ran Ding', 'Ramesh Nallapati', 'Bing Xiang')",10,Coherence-Aware Neural Topic Modeling,EMNLP,2018
"Text normalization is an important enabling technology for several NLP tasks. Recently, neural-network-based approaches have outperformed well-established models in this task. However, in languages other than English, there has been little exploration in this direction. Both the scarcity of annotated data and the complexity of the language increase the difficulty of the problem. To address these challenges, we use a sequence-to-sequence model with character-based attention, which in addition to its self-learned character embeddings, uses word embeddings pre-trained with an approach that also models subword information. This provides the neural model with access to more linguistic information especially suitable for text normalization, without large parallel corpora. We show that providing the model with word-level features bridges the gap for the neural network approach to achieve a state-of-the-art F1 score on a standard Arabic language correction shared task dataset.",http://aclweb.org/anthology/D18-1097,D18-1,D18-1097,https://arxiv.org/abs/1809.01534,"('Daniel Watson', 'Nasser Zalmout', 'Nizar Habash')",10,Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models,EMNLP,2018
"Topic models jointly learn topics and document-level topic distribution. Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topic- and document-level model quality, and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of document-level topic allocations, and provide empirical evidence for its robustness.",http://aclweb.org/anthology/D18-1098,D18-1,D18-1098,https://arxiv.org/abs/1706.05140,"('Shraey Bhatia', 'Jey Han Lau', 'Timothy Baldwin')",10,Topic Intrusion for Automatic Topic Model Evaluation,EMNLP,2018
"We introduce an unsupervised discriminative model for the task of retrieving experts in online document collections. We exclusively employ textual evidence and avoid explicit feature engineering by learning distributed word representations in an unsupervised way. We compare our model to state-of-the-art unsupervised statistical vector space and probabilistic generative approaches. Our proposed log-linear model achieves the retrieval performance levels of state-of-the-art document-centric methods with the low inference cost of so-called profile-centric approaches. It yields a statistically significant improved ranking over vector space and generative models in most cases, matching the performance of supervised methods on various benchmarks. That is, by using solely text we can do as well as methods that work with external evidence and/or relevance feedback. A contrastive analysis of rankings produced by discriminative and generative approaches shows that they have complementary strengths due to the ability of the unsupervised discriminative model to perform semantic matching.",http://aclweb.org/anthology/D18-1099,D18-1,D18-1099,https://arxiv.org/abs/1608.06651,"('Abhijith Athreya Mysore Gopinath', 'Shomir Wilson', 'Norman Sadeh')",10,Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents,EMNLP,2018
"In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix.",http://aclweb.org/anthology/D18-1100,D18-1,D18-1100,https://arxiv.org/abs/1808.07512,"('Xinyi Wang', 'Hieu Pham', 'Zihang Dai', 'Graham Neubig')",10,SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation,EMNLP,2018
"We show how a deep denoising autoencoder with lateral connections can be used as an auxiliary unsupervised learning task to support supervised learning. The proposed model is trained to minimize simultaneously the sum of supervised and unsupervised cost functions by back-propagation, avoiding the need for layer-wise pretraining. It improves the state of the art significantly in the permutation-invariant MNIST classification task.",http://aclweb.org/anthology/D18-1101,D18-1,D18-1101,https://arxiv.org/abs/1504.08215,"('Yunsu Kim', 'Jiahui Geng', 'Hermann Ney')",10,Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder,EMNLP,2018
"European libraries and archives are filled with enciphered manuscripts from the early modern period. These include military and diplomatic correspondence, records of secret societies, private letters, and so on. Although they are enciphered with classical cryptographic algorithms, their contents are unavailable to working historians. We therefore attack the problem of automatically converting cipher manuscript images into plaintext. We develop unsupervised models for character segmentation, character-image clustering, and decipherment of cluster sequences. We experiment with both pipelined and joint models, and we give empirical results for multiple ciphers.",http://aclweb.org/anthology/D18-1102,D18-1,D18-1102,https://arxiv.org/abs/1810.04297,"('Nishant Kambhatla', 'Anahita Mansouri Bigvand', 'Anoop Sarkar')",10,Decipherment of Substitution Ciphers with Neural Language Models,EMNLP,2018
"This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual ""seed models"", which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of ""similar-language regularization"", where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings. Code to reproduce experiments at https://github.com/neubig/rapid-adaptation",http://aclweb.org/anthology/D18-1103,D18-1,D18-1103,https://arxiv.org/abs/1808.04189,"('Graham Neubig', 'Junjie Hu')",10,Rapid Adaptation of Neural Machine Translation to New Languages,EMNLP,2018
We propose and compare methods for gradient-based domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture - combining a state-of-the-art self-attentive model with compact domain adaptation - provides high quality personalized machine translation that is both space and time efficient.,http://aclweb.org/anthology/D18-1104,D18-1,D18-1104,https://arxiv.org/abs/1811.01990,"('Joern Wuebker', 'Patrick Simianer', 'John DeNero')",10,Compact Personalized Models for Neural Machine Translation,EMNLP,2018
"Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.",http://aclweb.org/anthology/D18-1105,D18-1,D18-1105,https://arxiv.org/abs/1603.03827,"('Sujith Ravi', 'Zornitsa Kozareva')",10,Self-Governing Neural Networks for On-Device Short Text Classification,EMNLP,2018
"In this paper, we explore the task of mapping spoken language utterances to one of thousands of natural language understanding domains in intelligent personal digital assistants (IPDAs). This scenario is observed for many mainstream IPDAs in industry that allow third parties to develop thousands of new domains to augment built-in ones to rapidly increase domain coverage and overall IPDA capabilities. We propose a scalable neural model architecture with a shared encoder, a novel attention mechanism that incorporates personalization information and domain-specific classifiers that solves the problem efficiently. Our architecture is designed to efficiently accommodate new domains that appear in-between full model retraining cycles with a rapid bootstrapping mechanism two orders of magnitude faster than retraining. We account for practical constraints in real-time production systems, and design to minimize memory footprint and runtime latency. We demonstrate that incorporating personalization results in significantly more accurate domain classification in the setting with thousands of overlapping domains.",http://aclweb.org/anthology/D18-1106,D18-1,D18-1106,https://arxiv.org/abs/1804.08065,"('Joo-Kyung Kim', 'Young-Bum Kim')",10,Supervised Domain Enablement Attention for Personalized Domain Classification,EMNLP,2018
"In the sentence classification task, context formed from sentences adjacent to the sentence being classified can provide important information for classification. This context is, however, often ignored. Where methods do make use of context, only small amounts are considered, making it difficult to scale. We present a new method for sentence classification, Context-LSTM-CNN, that makes use of potentially large contexts. The method also utilizes long-range dependencies within the sentence being classified, using an LSTM, and short-span features, using a stacked CNN. Our experiments demonstrate that this approach consistently improves over previous methods on two different datasets.",http://aclweb.org/anthology/D18-1107,D18-1,D18-1107,https://arxiv.org/abs/1809.00934,"('Xingyi Song', 'Johann Petrak', 'Angus Roberts')",10,A Deep Neural Network Sentence Level Classification Method with Context Information,EMNLP,2018
"Deep NLP models benefit from underlying structures in the data---e.g., parse trees---typically extracted using off-the-shelf parsers. Recent attempts to jointly learn the latent structure encounter a tradeoff: either make factorization assumptions that limit expressiveness, or sacrifice end-to-end differentiability. Using the recently proposed SparseMAP inference, which retrieves a sparse distribution over latent structures, we propose a novel approach for end-to-end learning of latent structure predictors jointly with a downstream predictor. To the best of our knowledge, our method is the first to enable unrestricted dynamic computation graph construction from the global latent structure, while maintaining differentiability.",http://aclweb.org/anthology/D18-1108,D18-1,D18-1108,https://arxiv.org/abs/1809.00653,"('Vlad Niculae', 'André F. T. Martins', 'Claire Cardie')",10,Towards Dynamic Computation Graphs via Sparse Latent Structure,EMNLP,2018
"We introduce a class of convolutional neural networks (CNNs) that utilize recurrent neural networks (RNNs) as convolution filters. A convolution filter is typically implemented as a linear affine transformation followed by a non-linear function, which fails to account for language compositionality. As a result, it limits the use of high-order filters that are often warranted for natural language processing tasks. In this work, we model convolution filters with RNNs that naturally capture compositionality and long-term dependencies in language. We show that simple CNN architectures equipped with recurrent neural filters (RNFs) achieve results that are on par with the best published ones on the Stanford Sentiment Treebank and two answer sentence selection datasets.",http://aclweb.org/anthology/D18-1109,D18-1,D18-1109,https://arxiv.org/abs/1808.09315,['Yi Yang'],10,Convolutional Neural Networks with Recurrent Neural Filters,EMNLP,2018
"Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency graph or constituent trees. In this paper, we first propose to use the \textit{syntactic graph} to represent three types of syntactic information, i.e., word order, dependency and constituency features. We further employ a graph-to-sequence model to encode the syntactic graph and decode a logical form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.",http://aclweb.org/anthology/D18-1110,D18-1,D18-1110,https://arxiv.org/abs/1808.07624,"('Kun Xu', 'Lingfei Wu', 'Zhiguo Wang', 'Mo Yu', 'Liwei Chen', 'Vadim Sheinin')",10,Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model,EMNLP,2018
"Neural codes are collections of binary strings motivated by patterns of neural activity. In this paper, we study algorithmic and enumerative aspects of convex neural codes in dimension 1 (i.e. on a line or a circle). We use the theory of consecutive-ones matrices to obtain some structural and algorithmic results; we use generating functions to obtain enumerative results.",http://aclweb.org/anthology/D18-1111,D18-1,D18-1111,https://arxiv.org/abs/1702.06907,"('Shirley Anugrah Hayati', 'Raphael Olivier', 'Pravalika Avvaru', 'Pengcheng Yin', 'Anthony Tomasic', 'Graham Neubig')",10,Retrieval-Based Neural Code Generation,EMNLP,2018
"We study finitely generated models of countable theories, having at most countably many nonisomorphic finitely generated models. We intro- duce a notion of rank of finitely generated models and we prove, when T has at most countably many nonisomorphic finitely generated models, that every finitely generated model has an ordinal rank. This rank is used to give a prop- erty of finitely generated models analogue to the Hopf property of groups and also to give a necessary and sufficient condition for a finitely generated model to be prime of its complete theory. We investigate some properties of limit groups of equationally noetherian groups, in respect to their ranks.",http://aclweb.org/anthology/D18-1112,D18-1,D18-1112,https://arxiv.org/abs/0804.2908,"('Kun Xu', 'Lingfei Wu', 'Zhiguo Wang', 'Yansong Feng', 'Vadim Sheinin')",10,SQL-to-Text Generation with Graph-to-Sequence Model,EMNLP,2018
"We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) ""fool"" pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.",http://aclweb.org/anthology/D18-1113,D18-1,D18-1113,https://arxiv.org/abs/1804.06059,"('Emilie Colin', 'Claire Gardent')",10,Generating Syntactic Paraphrases,EMNLP,2018
"Multi-label learning has attracted significant interests in computer vision recently, finding applications in many vision tasks such as multiple object recognition and automatic image annotation. Associating multiple labels to a complex image is very difficult, not only due to the intricacy of describing the image, but also because of the incompleteness nature of the observed labels. Existing works on the problem either ignore the label-label and instance-instance correlations or just assume these correlations are linear and unstructured. Considering that semantic correlations between images are actually structured, in this paper we propose to incorporate structured semantic correlations to solve the missing label problem of multi-label learning. Specifically, we project images to the semantic space with an effective semantic descriptor. A semantic graph is then constructed on these images to capture the structured correlations between them. We utilize the semantic graph Laplacian as a smooth term in the multi-label learning formulation to incorporate the structured semantic correlations. Experimental results demonstrate the effectiveness of the proposed semantic descriptor and the usefulness of incorporating the structured semantic correlations. We achieve better results than state-of-the-art multi-label learning methods on four benchmark datasets.",http://aclweb.org/anthology/D18-1114,D18-1,D18-1114,https://arxiv.org/abs/1608.01441,"('Rachel Rudinger', 'Adam Teichert', 'Ryan Culkin', 'Sheng Zhang', 'Benjamin Van Durme')",10,Neural-Davidsonian Semantic Proto-role Labeling,EMNLP,2018
"We have compiled and analyzed historical Korean meteor and meteor shower records in three Korean official history books, Samguksagi which covers the three Kingdoms period (57 B.C -- A.D. 935), Goryeosa of Goryeo dynasty (A.D. 918 -- 1392), and Joseonwangjosillok of Joseon dynasty (A.D. 1392 -- 1910). We have found 3861 meteor and 31 meteor shower records. We have confirmed the peaks of Perseids and an excess due to the mixture of Orionids, north-Taurids, or Leonids through the Monte-Carlo test. The peaks persist from the period of Goryeo dynasty to that of Joseon dynasty, for almost one thousand years. Korean records show a decrease of Perseids activity and an increase of Orionids/north-Taurids/Leonids activity. We have also analyzed seasonal variation of sporadic meteors from Korean records. We confirm the seasonal variation of sporadic meteors from the records of Joseon dynasty with the maximum number of events being roughly 1.7 times the minimum. The Korean records are compared with Chinese and Japanese records for the same periods. Major features in Chinese meteor shower records are quite consistent with those of Korean records, particularly for the last millennium. Japanese records also show Perseids feature and Orionids/north-Taurids/Leonids feature, although they are less prominent compared to those of Korean or Chinese records.",http://aclweb.org/anthology/D18-1115,D18-1,D18-1115,https://arxiv.org/abs/astro-ph/0501216,"('JinYeong Bak', 'Alice Oh')",10,Conversational Decision-Making Model for Predicting the King’s Decision in the Annals of the Joseon Dynasty,EMNLP,2018
"Discourse segmentation, which segments texts into Elementary Discourse Units, is a fundamental step in discourse analysis. Previous discourse segmenters rely on complicated hand-crafted features and are not practical in actual use. In this paper, we propose an end-to-end neural segmenter based on BiLSTM-CRF framework. To improve its accuracy, we address the problem of data insufficiency by transferring a word representation model that is trained on a large corpus. We also propose a restricted self-attention mechanism in order to capture useful information within a neighborhood. Experiments on the RST-DT corpus show that our model is significantly faster than previous methods, while achieving new state-of-the-art performance.",http://aclweb.org/anthology/D18-1116,D18-1,D18-1116,https://arxiv.org/abs/1808.09147,"('Yizhong Wang', 'Sujian Li', 'Jingfeng Yang')",10,Toward Fast and Accurate Neural Discourse Segmentation,EMNLP,2018
"Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. We develop and test a novel method for learning fine-grained common-sense knowledge from these stories about contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for text and story understanding, information extraction, question answering, and text summarization. We test and compare different methods for learning contingency relation, and compare what is learned from topic-sorted story collections vs. general-domain stories. Our experiments show that using topic-specific datasets enables learning finer-grained knowledge about events and results in significant improvement over the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the relations between events that we learn from topic-sorted stories are judged as contingent.",http://aclweb.org/anthology/D18-1117,D18-1,D18-1117,https://arxiv.org/abs/1708.09450,"('Spandana Gella', 'Mike Lewis', 'Marcus Rohrbach')",10,A Dataset for Telling the Stories of Social Media Videos,EMNLP,2018
"Visual reasoning is a special visual question answering problem that is multi-step and compositional by nature, and also requires intensive text-vision interactions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end visual reasoning model. CMM includes a multi-step comprehension process for both question and image. In each step, we use a Feature-wise Linear Modulation (FiLM) technique to enable textual/visual pipeline to mutually control each other. Experiments show that CMM significantly outperforms most related models, and reach state-of-the-arts on two visual reasoning benchmarks: CLEVR and NLVR, collected from both synthetic and natural languages. Ablation studies confirm that both our multistep framework and our visual-guided language modulation are critical to the task. Our code is available at https://github.com/FlamingHorizon/CMM-VR.",http://aclweb.org/anthology/D18-1118,D18-1,D18-1118,https://arxiv.org/abs/1809.01943,"('Yiqun Yao', 'Jiaming Xu', 'Feng Wang', 'Bo Xu')",10,Cascaded Mutual Modulation for Visual Reasoning,EMNLP,2018
"There is growing interest in the language developed by agents interacting in emergent-communication settings. Earlier studies have focused on the agents' symbol usage, rather than on their representation of visual input. In this paper, we consider the referential games of Lazaridou et al. (2017) and investigate the representations the agents develop during their evolving interaction. We find that the agents establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images. We conclude that, if we are interested in developing language-like communication systems, we must pay more attention to the visual semantics agents associate to the symbols they use.",http://aclweb.org/anthology/D18-1119,D18-1,D18-1119,https://arxiv.org/abs/1808.10696,"('Diane Bouchacourt', 'Marco Baroni')",10,How agents see things: On visual representations in an emergent language game,EMNLP,2018
"Capsule Networks (CapsNet) are recently proposed multi-stage computational models specialized for entity representation and discovery in image data. CapsNet employs iterative routing that shapes how the information cascades through different levels of interpretations. In this work, we investigate i) how the routing affects the CapsNet model fitting, ii) how the representation by capsules helps discover global structures in data distribution and iii) how learned data representation adapts and generalizes to new tasks. Our investigation shows: i) routing operation determines the certainty with which one layer of capsules pass information to the layer above, and the appropriate level of certainty is related to the model fitness, ii) in a designed experiment using data with a known 2D structure, capsule representations allow more meaningful 2D manifold embedding than neurons in a standard CNN do and iii) compared to neurons of standard CNN, capsules of successive layers are less coupled and more adaptive to new data distribution.",http://aclweb.org/anthology/D18-1120,D18-1,D18-1120,https://arxiv.org/abs/1810.04041,"('Ningyu Zhang', 'Shumin Deng', 'Zhanling Sun', 'Xi Chen', 'Wei Zhang', 'Huajun Chen')",10,Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction,EMNLP,2018
"Is the human language understander a collection of modular processes operating with relative autonomy, or is it a single integrated process? This ongoing debate has polarized the language processing community, with two fundamentally different types of model posited, and with each camp concluding that the other is wrong. One camp puts forth a model with separate processors and distinct knowledge sources to explain one body of data, and the other proposes a model with a single processor and a homogeneous, monolithic knowledge source to explain the other body of data. In this paper we argue that a hybrid approach which combines a unified processor with separate knowledge sources provides an explanation of both bodies of data, and we demonstrate the feasibility of this approach with the computational model called COMPERE. We believe that this approach brings the language processing community significantly closer to offering human-like language processing systems.",http://aclweb.org/anthology/D18-1121,D18-1,D18-1121,https://arxiv.org/abs/cmp-lg/9408020,"('Ji Xin', 'Hao Zhu', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun')",10,Put It Back: Entity Typing with Language Model Enhancement,EMNLP,2018
"Detecting events and classifying them into predefined types is an important step in knowledge extraction from natural language texts. While the neural network models have generally led the state-of-the-art, the differences in performance between different architectures have not been rigorously studied. In this paper we present a novel GRU-based model that combines syntactic information along with temporal structure through an attention mechanism. We show that it is competitive with other neural network architectures through empirical evaluations under different random initializations and training-validation-test splits of ACE2005 dataset.",http://aclweb.org/anthology/D18-1122,D18-1,D18-1122,https://arxiv.org/abs/1808.08504,"('Walker Orr', 'Prasad Tadepalli', 'Xiaoli Fern')",10,Event Detection with Neural Networks: A Rigorous Empirical Evaluation,EMNLP,2018
"Despite recent evidence that Microsoft Academic is an extensive source of citation counts for journal articles, it is not known if the same is true for academic books. This paper fills this gap by comparing citations to 16,463 books from 2013-2016 in the Book Citation Index (BKCI) against automatically extracted citations from Microsoft Academic and Google Books in 17 fields. About 60% of the BKCI books had records in Microsoft Academic, varying by year and field. Citation counts from Microsoft Academic were 1.5 to 3.6 times higher than from BKCI in nine subject areas across all years for books indexed by both. Microsoft Academic found more citations than BKCI because it indexes more scholarly publications and combines citations to different editions and chapters. In contrast, BKCI only found more citations than Microsoft Academic for books in three fields from 2013-2014. Microsoft Academic also found more citations than Google Books in six fields for all years. Thus, Microsoft Academic may be a useful source for the impact assessment of books when comprehensive coverage is not essential.",http://aclweb.org/anthology/D18-1123,D18-1,D18-1123,https://arxiv.org/abs/1808.01474,"('Yiqing Zhang', 'Jianzhong Qi', 'Rui Zhang', 'Chuandong Yin')",10,PubSE: A Hierarchical Model for Publication Extraction from Academic Homepages,EMNLP,2018
"It is common that entity mentions can contain other mentions recursively. This paper introduces a scalable transition-based method to model the nested structure of mentions. We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest. Our shift-reduce based system then learns to construct the forest structure in a bottom-up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length. Based on Stack-LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space, our system is further incorporated with a character-based component to capture letter-level patterns. Our model achieves the state-of-the-art results on ACE datasets, showing its effectiveness in detecting nested mentions.",http://aclweb.org/anthology/D18-1124,D18-1,D18-1124,https://arxiv.org/abs/1810.01808,"('Bailin Wang', 'Wei Lu', 'Yu Wang', 'Hongxia Jin')",10,A Neural Transition-based Model for Nested Mention Recognition,EMNLP,2018
"In this paper, we propose to infer music genre embeddings from audio datasets carrying semantic information about genres. We show that such embeddings can be used for disambiguating genre tags (identification of different labels for the same genre, tag translation from a tag system to another, inference of hierarchical taxonomies on these genre tags). These embeddings are built by training a deep convolutional neural network genre classifier with large audio datasets annotated with a flat tag system. We show empirically that they makes it possible to retrieve the original taxonomy of a tag system, spot duplicates tags and translate tags from a tag system to another.",http://aclweb.org/anthology/D18-1125,D18-1,D18-1125,https://arxiv.org/abs/1809.07256,"('Ge Shi', 'Chong Feng', 'Lifu Huang', 'Boliang Zhang', 'Heng Ji', 'Lejian Liao', 'Heyan Huang')",10,Genre Separation Network with Adversarial Training for Cross-genre Relation Extraction,EMNLP,2018
"A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).",http://aclweb.org/anthology/D18-1126,D18-1,D18-1126,https://arxiv.org/abs/1604.00734,"('David Mueller', 'Greg Durrett')",10,Effective Use of Context in Noisy Entity Linking,EMNLP,2018
"The task of event detection involves identifying and categorizing event triggers. Contextual information has been shown effective on the task. However, existing methods which utilize contextual information only process the context once. We argue that the context can be better exploited by processing the context multiple times, allowing the model to perform complex reasoning and to generate better context representation, thus improving the overall performance. Meanwhile, dynamic memory network (DMN) has demonstrated promising capability in capturing contextual information and has been applied successfully to various tasks. In light of the multi-hop mechanism of the DMN to model the context, we propose the trigger detection dynamic memory network (TD-DMN) to tackle the event detection problem. We performed a five-fold cross-validation on the ACE-2005 dataset and experimental results show that the multi-hop mechanism does improve the performance and the proposed model achieves best $F_1$ score compared to the state-of-the-art methods.",http://aclweb.org/anthology/D18-1127,D18-1,D18-1127,https://arxiv.org/abs/1810.03449,"('Shaobo Liu', 'Rui Cheng', 'Xiaoming Yu', 'Xueqi Cheng')",10,Exploiting Contextual Information via Dynamic Memory Network for Event Detection,EMNLP,2018
"A rich line of research attempts to make deep neural networks more transparent by generating human-interpretable 'explanations' of their decision process, especially for interactive tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed make a VQA model -- its responses as well as failures -- more predictable to a human. Surprisingly, we find that they do not. On the other hand, we find that human-in-the-loop approaches that treat the model as a black-box do.",http://aclweb.org/anthology/D18-1128,D18-1,D18-1128,https://arxiv.org/abs/1810.12366,"('Arjun Chandrasekaran', 'Viraj Prabhu', 'Deshraj Yadav', 'Prithvijit Chattopadhyay', 'Devi Parikh')",10,Do explanations make VQA models more predictable to a human?,EMNLP,2018
"We discuss a model for a universe with discrete matter content instead of the continuous perfect fluid taken in FRW models. We show how the redshift in such a universe deviates from the corresponding one in an FRW cosmology. This illustrates the fact that averaging the matter content in a universe and then evolving it in time, is not the same as evolving a universe with discrete matter content. The main reason for such deviation is the fact that the photons in such a universe mainly travel in an empty space rather than the continuous perfect fluid in FRW geometry.",http://aclweb.org/anthology/D18-1129,D18-1,D18-1129,https://arxiv.org/abs/1205.6877,"('Marco Ponza', 'Luciano Del Corro', 'Gerhard Weikum')",10,Facts That Matter,EMNLP,2018
"Reading comprehension tasks test the ability of models to process long-term context and remember salient information. Recent work has shown that relatively simple neural methods such as the Attention Sum-Reader can perform well on these tasks; however, these systems still significantly trail human performance. Analysis suggests that many of the remaining hard instances are related to the inability to track entity-references throughout documents. This work focuses on these hard entity tracking cases with two extensions: (1) additional entity features, and (2) training with a multi-task tracking objective. We show that these simple modifications improve performance both independently and in combination, and we outperform the previous state of the art on the LAMBADA dataset, particularly on difficult entity examples.",http://aclweb.org/anthology/D18-1130,D18-1,D18-1130,https://arxiv.org/abs/1810.02891,"('Luong Hoang', 'Sam Wiseman', 'Alexander Rush')",10,Entity Tracking Improves Cloze-style Reading Comprehension,EMNLP,2018
"We address the problem of detecting duplicate questions in forums, which is an important step towards automating the process of answering new questions. As finding and annotating such potential duplicates manually is very tedious and costly, automatic methods based on machine learning are a viable alternative. However, many forums do not have annotated data, i.e., questions labeled by experts as duplicates, and thus a promising solution is to use domain adaptation from another forum that has such annotations. Here we focus on adversarial domain adaptation, deriving important findings about when it performs well and what properties of the domains are important in this regard. Our experiments with StackExchange data show an average improvement of 5.6% over the best baseline across multiple pairs of domains.",http://aclweb.org/anthology/D18-1131,D18-1,D18-1131,https://arxiv.org/abs/1809.02255,"('Darsh Shah', 'Tao Lei', 'Alessandro Moschitti', 'Salvatore Romeo', 'Preslav Nakov')",10,Adversarial Domain Adaptation for Duplicate Question Detection,EMNLP,2018
"Sequence-to-sequence (SEQ2SEQ) models have been successfully applied to automatic math word problem solving. Despite its simplicity, a drawback still remains: a math word problem can be correctly solved by more than one equations. This non-deterministic transduction harms the performance of maximum likelihood estimation. In this paper, by considering the uniqueness of expression tree, we propose an equation normalization method to normalize the duplicated equations. Moreover, we analyze the performance of three popular SEQ2SEQ models on the math word problem solving. We find that each model has its own specialty in solving problems, consequently an ensemble model is then proposed to combine their advantages. Experiments on dataset Math23K show that the ensemble model with equation normalization significantly outperforms the previous state-of-the-art methods.",http://aclweb.org/anthology/D18-1132,D18-1,D18-1132,https://arxiv.org/abs/1811.05632,"('Lei Wang', 'Yan Wang', 'Deng Cai', 'Dongxiang Zhang', 'Xiaojiang Liu')",10,Translating a Math Word Problem to a Expression Tree,EMNLP,2018
"In this paper, we propose convolutional neural networks for learning an optimal representation of question and answer sentences. Their main aspect is the use of relational information given by the matches between words from the two members of the pair. The matches are encoded as embeddings with additional parameters (dimensions), which are tuned by the network. These allows for better capturing interactions between questions and answers, resulting in a significant boost in accuracy. We test our models on two widely used answer sentence selection benchmarks. The results clearly show the effectiveness of our relational information, which allows our relatively simple network to approach the state of the art.",http://aclweb.org/anthology/D18-1133,D18-1,D18-1133,https://arxiv.org/abs/1604.01178,"('Massimo Nicosia', 'Alessandro Moschitti')",10,Semantic Linking in Convolutional Neural Networks for Answer Sentence Selection,EMNLP,2018
"We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.",http://aclweb.org/anthology/D18-1134,D18-1,D18-1134,https://arxiv.org/abs/1809.00732,"('Ahmed Elgohary', 'Chen Zhao', 'Jordan Boyd-Graber')",10,A dataset and baselines for sequential open-domain question answering,EMNLP,2018
"Recently, string kernels have obtained state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification. In this paper, we apply two simple yet effective transductive learning approaches to further improve the results of string kernels. The first approach is based on interpreting the pairwise string kernel similarities between samples in the training set and samples in the test set as features. Our second approach is a simple self-training method based on two learning iterations. In the first iteration, a classifier is trained on the training set and tested on the test set, as usual. In the second iteration, a number of test samples (to which the classifier associated higher confidence scores) are added to the training set for another round of training. However, the ground-truth labels of the added test samples are not necessary. Instead, we use the labels predicted by the classifier in the first training iteration. By adapting string kernels to the test set, we report significantly better accuracy rates in English polarity classification and Arabic dialect identification.",http://aclweb.org/anthology/D18-1135,D18-1,D18-1135,https://arxiv.org/abs/1808.08409,"('Radu Tudor Ionescu', 'Andrei M. Butnaru')",10,Improving the results of string kernels in sentiment analysis and Arabic dialect identification by adapting them to your test set,EMNLP,2018
"With the development of the Internet, natural language processing (NLP), in which sentiment analysis is an important task, became vital in information processing.Sentiment analysis includes aspect sentiment classification. Aspect sentiment can provide complete and in-depth results with increased attention on aspect-level. Different context words in a sentence influence the sentiment polarity of a sentence variably, and polarity varies based on the different aspects in a sentence. Take the sentence, 'I bought a new camera. The picture quality is amazing but the battery life is too short.'as an example. If the aspect is picture quality, then the expected sentiment polarity is 'positive', if the battery life aspect is considered, then the sentiment polarity should be 'negative'; therefore, aspect is important to consider when we explore aspect sentiment in the sentence. Recurrent neural network (RNN) is regarded as a good model to deal with natural language processing, and RNNs has get good performance on aspect sentiment classification including Target-Dependent LSTM (TD-LSTM) ,Target-Connection LSTM (TC-LSTM) (Tang, 2015a, b), AE-LSTM, AT-LSTM, AEAT-LSTM (Wang et al., 2016).There are also extensive literatures on sentiment classification utilizing convolutional neural network, but there is little literature on aspect sentiment classification using convolutional neural network. In our paper, we develop attention-based input layers in which aspect information is considered by input layer. We then incorporate attention-based input layers into convolutional neural network (CNN) to introduce context words information. In our experiment, incorporating aspect information into CNN improves the latter's aspect sentiment classification performance without using syntactic parser or external sentiment lexicons in a benchmark dataset from Twitter but get better performance compared with other models.",http://aclweb.org/anthology/D18-1136,D18-1,D18-1136,https://arxiv.org/abs/1807.01704,"('Binxuan Huang', 'Kathleen Carley')",10,Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification,EMNLP,2018
"Deep learning approaches for sentiment classification do not fully exploit sentiment linguistic knowledge. In this paper, we propose a Multi-sentiment-resource Enhanced Attention Network (MEAN) to alleviate the problem by integrating three kinds of sentiment linguistic knowledge (e.g., sentiment lexicon, negation words, intensity words) into the deep neural network via attention mechanisms. By using various types of sentiment resources, MEAN utilizes sentiment-relevant information from different representation subspaces, which makes it more effective to capture the overall semantics of the sentiment, negation and intensity words for sentiment prediction. The experimental results demonstrate that MEAN has robust superiority over strong competitors.",http://aclweb.org/anthology/D18-1137,D18-1,D18-1137,https://arxiv.org/abs/1807.04990,"('Jianfei Yu', 'Luis Marujo', 'Jing Jiang', 'Pradeep Karuturi', 'William Brendel')",10,Improving Multi-label Emotion Classification via Sentiment Classification with Dual Attention Transfer Network,EMNLP,2018
"The task of sentiment modification requires reversing the sentiment of the input and preserving the sentiment-independent content. However, aligned sentences with the same content but different sentiments are usually unavailable. Due to the lack of such parallel data, it is hard to extract sentiment independent content and reverse the sentiment in an unsupervised way. Previous work usually can not reconcile sentiment transformation and content preservation. In this paper, motivated by the fact the non-emotional context (e.g., ""staff"") provides strong cues for the occurrence of emotional words (e.g., ""friendly""), we propose a novel method that automatically extracts appropriate sentiment information from learned sentiment memories according to specific context. Experiments show that our method substantially improves the content preservation degree and achieves the state-of-the-art performance.",http://aclweb.org/anthology/D18-1138,D18-1,D18-1138,https://arxiv.org/abs/1808.07311,"('Yi Zhang', 'Jingjing Xu', 'Pengcheng Yang', 'Xu Sun')",10,Learning Sentiment Memories for Sentiment Modification without Parallel Data,EMNLP,2018
"With the development of the Internet, natural language processing (NLP), in which sentiment analysis is an important task, became vital in information processing.Sentiment analysis includes aspect sentiment classification. Aspect sentiment can provide complete and in-depth results with increased attention on aspect-level. Different context words in a sentence influence the sentiment polarity of a sentence variably, and polarity varies based on the different aspects in a sentence. Take the sentence, 'I bought a new camera. The picture quality is amazing but the battery life is too short.'as an example. If the aspect is picture quality, then the expected sentiment polarity is 'positive', if the battery life aspect is considered, then the sentiment polarity should be 'negative'; therefore, aspect is important to consider when we explore aspect sentiment in the sentence. Recurrent neural network (RNN) is regarded as a good model to deal with natural language processing, and RNNs has get good performance on aspect sentiment classification including Target-Dependent LSTM (TD-LSTM) ,Target-Connection LSTM (TC-LSTM) (Tang, 2015a, b), AE-LSTM, AT-LSTM, AEAT-LSTM (Wang et al., 2016).There are also extensive literatures on sentiment classification utilizing convolutional neural network, but there is little literature on aspect sentiment classification using convolutional neural network. In our paper, we develop attention-based input layers in which aspect information is considered by input layer. We then incorporate attention-based input layers into convolutional neural network (CNN) to introduce context words information. In our experiment, incorporating aspect information into CNN improves the latter's aspect sentiment classification performance without using syntactic parser or external sentiment lexicons in a benchmark dataset from Twitter but get better performance compared with other models.",http://aclweb.org/anthology/D18-1139,D18-1,D18-1139,https://arxiv.org/abs/1807.01704,"('Martin Schmitt', 'Simon Steinheber', 'Konrad Schreiber', 'Benjamin Roth')",10,Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks,EMNLP,2018
"We explore two methods for representing authors in the context of textual sarcasm detection: a Bayesian approach that directly represents authors' propensities to be sarcastic, and a dense embedding approach that can learn interactions between the author and the text. Using the SARC dataset of Reddit comments, we show that augmenting a bidirectional RNN with these representations improves performance; the Bayesian approach suffices in homogeneous contexts, whereas the added power of the dense embeddings proves valuable in more diverse ones.",http://aclweb.org/anthology/D18-1140,D18-1,D18-1140,https://arxiv.org/abs/1808.08470,"('Y. Alex Kolchinski', 'Christopher Potts')",10,Representing Social Media Users for Sarcasm Detection,EMNLP,2018
"We present a statistical parsing framework for sentence-level sentiment classification in this article. Unlike previous works that employ syntactic parsing results for sentiment analysis, we develop a statistical parser to directly analyze the sentiment structure of a sentence. We show that complicated phenomena in sentiment analysis (e.g., negation, intensification, and contrast) can be handled the same as simple and straightforward sentiment expressions in a unified and probabilistic way. We formulate the sentiment grammar upon Context-Free Grammars (CFGs), and provide a formal description of the sentiment parsing framework. We develop the parsing model to obtain possible sentiment parse trees for a sentence, from which the polarity model is proposed to derive the sentiment strength and polarity, and the ranking model is dedicated to selecting the best sentiment tree. We train the parser directly from examples of sentences annotated only with sentiment polarity labels but without any syntactic annotations or polarity annotations of constituents within sentences. Therefore we can obtain training data easily. In particular, we train a sentiment parser, s.parser, from a large amount of review sentences with users' ratings as rough sentiment polarity labels. Extensive experiments on existing benchmark datasets show significant improvements over baseline sentiment classification approaches.",http://aclweb.org/anthology/D18-1141,D18-1,D18-1141,https://arxiv.org/abs/1401.6330,"('Rohil Verma', 'Samuel Kim', 'David Walter')",10,Syntactical Analysis of the Weaknesses of Sentiment Analyzers,EMNLP,2018
"Recent research has explored the possibility of automatically deducing information such as gender, age and race of an individual from their biometric data. While the face modality has been extensively studied in this regard, relatively less research has been conducted in the context of the iris modality. In this paper, we first review the medical literature to establish a biological basis for extracting gender and race cues from the iris. Then, we demonstrate that it is possible to use simple texture descriptors, like BSIF (Binarized Statistical Image Feature) and LBP (Local Binary Patterns), to extract gender and race attributes from a NIR ocular image used in a typical iris recognition system. The proposed method predicts race and gender from a single eye image with an accuracy of 86% and 90%, respectively. In addition, the following analysis are conducted: (a) the role of different parts of the ocular region on attribute prediction; (b) the influence of gender on race prediction, and vice-versa; (c) the impact of eye color on gender and race prediction; (d) the impact of image blur on gender and race prediction; (e) the generalizability of the method across different datasets, i.e., cross-dataset performance; and (f) the consistency of prediction performance across the left and right eyes.",http://aclweb.org/anthology/D18-1142,D18-1,D18-1142,https://arxiv.org/abs/1805.01912,"('Sridhar Moorthy', 'Ruth Pogacar', 'Samin Khan', 'Yang Xu')",10,Is Nike female? Exploring the role of sound symbolism in predicting brand name gender,EMNLP,2018
"Lexical selection in Machine Translation consists of several related components. Two that have received a lot of attention are lexical mapping from an underlying concept or lexical item, and choosing the correct subcategorization frame based on argument structure. Because most MT applications are small or relatively domain specific, a third component of lexical selection is generally overlooked - distinguishing between lexical items that are closely related conceptually. While some MT systems have proposed using a 'world knowledge' module to decide which word is more appropriate based on various pragmatic or stylistic constraints, we are interested in seeing how much we can accomplish using a combination of syntax and lexical semantics. By using separate ontologies for each language implemented in FB-LTAGs, we are able to elegantly model the more specific and language dependent syntactic and semantic distinctions necessary to further filter the choice of the lexical item.",http://aclweb.org/anthology/D18-1143,D18-1,D18-1143,https://arxiv.org/abs/cmp-lg/9411005,"('Nayeon Lee', 'Chien-Sheng Wu', 'Pascale Fung')",10,Improving Large-Scale Fact-Checking using Decomposable Attention Models and Lexical Tagging,EMNLP,2018
"Usage of online textual media is steadily increasing. Daily, more and more news stories, blog posts and scientific articles are added to the online volumes. These are all freely accessible and have been employed extensively in multiple research areas, e.g. automatic text summarization, information retrieval, information extraction, etc. Meanwhile, online debate forums have recently become popular, but have remained largely unexplored. For this reason, there are no sufficient resources of annotated debate data available for conducting research in this genre. In this paper, we collected and annotated debate data for an automatic summarization task. Similar to extractive gold standard summary generation our data contains sentences worthy to include into a summary. Five human annotators performed this task. Inter-annotator agreement, based on semantic similarity, is 36% for Cohen's kappa and 48% for Krippendorff's alpha. Moreover, we also implement an extractive summarization system for online debates and discuss prominent features for the task of summarizing online debate data automatically.",http://aclweb.org/anthology/D18-1144,D18-1,D18-1144,https://arxiv.org/abs/1708.04592,"('Ryuji Kano', 'Yasuhide Miura', 'Motoki Taniguchi', 'Yan-Ying Chen', 'Francine Chen', 'Tomoko Ohkuma')",10,Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations,EMNLP,2018
"Compromised social media accounts are legitimate user accounts that have been hijacked by a third (malicious) party and can cause various kinds of damage. Early detection of such compromised accounts is very important in order to control the damage. In this work we propose a novel general framework for discovering compromised accounts by utilizing statistical text analysis. The framework is built on the observation that users will use language that is measurably different from the language that a hacker (or spammer) would use, when the account is compromised. We use the framework to develop specific algorithms based on language modeling and use the similarity of language models of users and spammers as features in a supervised learning setup to identify compromised accounts. Evaluation results on a large Twitter corpus of over 129 million tweets show promising results of the proposed approach.",http://aclweb.org/anthology/D18-1145,D18-1,D18-1145,https://arxiv.org/abs/1804.07247,"('Masoud Rouhizadeh', 'Kokil Jaidka', 'Laura Smith', 'H. Andrew Schwartz', 'Anneke Buffone', 'Lyle Ungar')",10,Identifying Locus of Control in Social Media Language,EMNLP,2018
"This paper examines the limit properties of information criteria (such as AIC, BIC, HQIC) for distinguishing between the unit root model and the various kinds of explosive models. The explosive models include the local-to-unit-root model, the mildly explosive model and the regular explosive model. Initial conditions with different order of magnitude are considered. Both the OLS estimator and the indirect inference estimator are studied. It is found that BIC and HQIC, but not AIC, consistently select the unit root model when data come from the unit root model. When data come from the local-to-unit-root model, both BIC and HQIC select the wrong model with probability approaching 1 while AIC has a positive probability of selecting the right model in the limit. When data come from the regular explosive model or from the mildly explosive model in the form of $1+n^{\alpha }/n$ with $\alpha \in (0,1)$, all three information criteria consistently select the true model. Indirect inference estimation can increase or decrease the probability for information criteria to select the right model asymptotically relative to OLS, depending on the information criteria and the true model. Simulation results confirm our asymptotic results in finite sample.",http://aclweb.org/anthology/D18-1146,D18-1,D18-1146,https://arxiv.org/abs/1703.02720,['Shengli Hu'],10,Somm: Into the Model,EMNLP,2018
"Online social media users react to content in them based on context. Emotions or mood play a significant part of these reactions, which has filled these platforms with opinionated content. Different approaches and applications to make better use of this data are continuously being developed. However, due to the nature of the data, the variety of platforms, and dynamic online user behavior, there are still many issues to be dealt with. It remains a challenge to properly obtain a reliable emotional status from a user prior to posting a comment. This work introduces a methodology that explores semi-supervised multilingual emotion detection based on the overlap of Facebook reactions and textual data. With the resulting emotion detection system we evaluate the possibility of using emotions and user behavior features for the task of sarcasm detection. More than 1 million English and Chinese comments from over 62,000 public Facebook pages posts have been collected and processed, conducted experiments show acceptable performance metrics.",http://aclweb.org/anthology/D18-1147,D18-1,D18-1147,https://arxiv.org/abs/1805.06510,"('Hamed Khanpour', 'Cornelia Caragea')",10,Fine-Grained Emotion Detection in Health-Related Online Posts,EMNLP,2018
"Nowcasting based on social media text promises to provide unobtrusive and near real-time predictions of community-level outcomes. These outcomes are typically regarding people, but the data is often aggregated without regard to users in the Twitter populations of each community. This paper describes a simple yet effective method for building community-level models using Twitter language aggregated by user. Results on four different U.S. county-level tasks, spanning demographic, health, and psychological outcomes show large and consistent improvements in prediction accuracies (e.g. from Pearson r=.73 to .82 for median income prediction or r=.37 to .47 for life satisfaction prediction) over the standard approach of aggregating all tweets. We make our aggregated and anonymized community-level data, derived from 37 billion tweets -- over 1 billion of which were mapped to counties, available for research.",http://aclweb.org/anthology/D18-1148,D18-1,D18-1148,https://arxiv.org/abs/1808.09600,"('Salvatore Giorgi', 'Daniel Preoţiuc-Pietro', 'Anneke Buffone', 'Daniel Rieman', 'Lyle Ungar', 'H. Andrew Schwartz')",10,The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level Predictions,EMNLP,2018
"We propose a conditional non-autoregressive neural sequence model based on iterative refinement. The proposed model is designed based on the principles of latent variable models and denoising autoencoders, and is generally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En-De and En-Ro) and image caption generation, and observe that it significantly speeds up decoding while maintaining the generation quality comparable to the autoregressive counterpart.",http://aclweb.org/anthology/D18-1149,D18-1,D18-1149,https://arxiv.org/abs/1802.06901,"('Jason Lee', 'Elman Mansimov', 'Kyunghyun Cho')",10,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,EMNLP,2018
"We propose a large margin criterion for training neural language models. Conventionally, neural language models are trained by minimizing perplexity (PPL) on grammatical sentences. However, we demonstrate that PPL may not be the best metric to optimize in some tasks, and further propose a large margin formulation. The proposed method aims to enlarge the margin between the ""good"" and ""bad"" sentences in a task-specific sense. It is trained end-to-end and can be widely applied to tasks that involve re-scoring of generated text. Compared with minimum-PPL training, our method gains up to 1.1 WER reduction for speech recognition and 1.0 BLEU increase for machine translation.",http://aclweb.org/anthology/D18-1150,D18-1,D18-1150,https://arxiv.org/abs/1808.08987,"('Jiaji Huang', 'Yi Li', 'Wei Ping', 'Liang Huang')",10,Large Margin Neural Language Model,EMNLP,2018
"We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) ""fool"" pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.",http://aclweb.org/anthology/D18-1151,D18-1,D18-1151,https://arxiv.org/abs/1804.06059,"('Rebecca Marvin', 'Tal Linzen')",10,Targeted Syntactic Evaluation of Language Models,EMNLP,2018
"The strong recurrence is equivalent to the Riemann hypothesis. On the other hand, the generalized strong recurrence holds for any irrational number. In this paper, we show the generalized strong recurrence for all non-zero rational numbers. Moreover, we prove that the generalized strong recurrence in the region of absolute convergence holds for any real number.",http://aclweb.org/anthology/D18-1152,D18-1,D18-1152,https://arxiv.org/abs/1006.1778,"('Hao Peng', 'Roy Schwartz', 'Sam Thomson', 'Noah A. Smith')",10,Rational Recurrences,EMNLP,2018
"Many efforts have been made to facilitate natural language processing tasks with pre-trained language models (LMs), and brought significant improvements to various applications. To fully leverage the nearly unlimited corpora and capture linguistic information of multifarious levels, large-size LMs are required; but for a specific task, only parts of these information are useful. Such large-sized LMs, even in the inference stage, may cause heavy computation workloads, making them too time-consuming for large-scale applications. Here we propose to compress bulky LMs while preserving useful information with regard to a specific task. As different layers of the model keep different information, we develop a layer selection method for model pruning using sparsity-inducing regularization. By introducing the dense connectivity, we can detach any layer without affecting others, and stretch shallow and wide LMs to be deep and narrow. In model training, LMs are learned with layer-wise dropouts for better robustness. Experiments on two benchmark datasets demonstrate the effectiveness of our method.",http://aclweb.org/anthology/D18-1153,D18-1,D18-1153,https://arxiv.org/abs/1804.07827,"('Liyuan Liu', 'Xiang Ren', 'Jingbo Shang', 'Xiaotao Gu', 'Jian Peng', 'Jiawei Han')",10,Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling,EMNLP,2018
"Identifying the salience (i.e. importance) of discourse units is an important task in language understanding. While events play important roles in text documents, little research exists on analyzing their saliency status. This paper empirically studies the Event Salience task and proposes two salience detection models based on content similarities and discourse relations. The first is a feature based salience model that incorporates similarities among discourse units. The second is a neural model that captures more complex relations between discourse units. Tested on our new large-scale event salience corpus, both methods significantly outperform the strong frequency baseline, while our neural model further improves the feature based one by a large margin. Our analyses demonstrate that our neural model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures).",http://aclweb.org/anthology/D18-1154,D18-1,D18-1154,https://arxiv.org/abs/1809.00647,"('Zhengzhong Liu', 'Chenyan Xiong', 'Teruko Mitamura', 'Eduard Hovy')",10,Automatic Event Salience Identification,EMNLP,2018
"The current leading paradigm for temporal information extraction from text consists of three phases: (1) recognition of events and temporal expressions, (2) recognition of temporal relations among them, and (3) time-line construction from the temporal relations. In contrast to the first two phases, the last phase, time-line construction, received little attention and is the focus of this work. In this paper, we propose a new method to construct a linear time-line from a set of (extracted) temporal relations. But more importantly, we propose a novel paradigm in which we directly predict start and end-points for events from the text, constituting a time-line without going through the intermediate step of prediction of temporal relations as in earlier work. Within this paradigm, we propose two models that predict in linear complexity, and a new training loss using TimeML-style annotations, yielding promising results.",http://aclweb.org/anthology/D18-1155,D18-1,D18-1155,https://arxiv.org/abs/1808.09401,"('Artuur Leeuwenberg', 'Marie-Francine Moens')",10,Temporal Information Extraction by Predicting Relative Time-lines,EMNLP,2018
"Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.",http://aclweb.org/anthology/D18-1156,D18-1,D18-1156,https://arxiv.org/abs/1809.09078,"('Xiao Liu', 'Zhunchen Luo', 'Heyan Huang')",10,Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation,EMNLP,2018
"Computer vision tasks often have side information available that is helpful to solve the task. For example, for crowd counting, the camera perspective (e.g., camera angle and height) gives a clue about the appearance and scale of people in the scene. While side information has been shown to be useful for counting systems using traditional hand-crafted features, it has not been fully utilized in counting systems based on deep learning. In order to incorporate the available side information, we propose an adaptive convolutional neural network (ACNN), where the convolutional filter weights adapt to the current scene context via the side information. In particular, we model the filter weights as a low-dimensional manifold, parametrized by the side information, within the high-dimensional space of filter weights. With the help of side information and adaptive weights, the ACNN can disentangle the variations related to the side information, and extract discriminative features related to the current context. Since existing crowd counting datasets do not contain ground-truth side information, we collect a new dataset with the ground-truth camera angle and height as the side information. On experiments in crowd counting, the ACNN improves counting accuracy compared to a plain CNN with a similar number of parameters. We also apply ACNN to image deconvolution to show its potential effectiveness on other computer vision applications.",http://aclweb.org/anthology/D18-1157,D18-1,D18-1157,https://arxiv.org/abs/1611.06748,"('Shikhar Vashishth', 'Rishabh Joshi', 'Sai Suman Prayaga', 'Chiranjib Bhattacharyya', 'Partha Talukdar')",10,RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information,EMNLP,2018
"In this paper, we present a gated convolutional neural network and a temporal attention-based localization method for audio classification, which won the 1st place in the large-scale weakly supervised sound event detection task of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 challenge. The audio clips in this task, which are extracted from YouTube videos, are manually labeled with one or a few audio tags but without timestamps of the audio events, which is called as weakly labeled data. Two sub-tasks are defined in this challenge including audio tagging and sound event detection using this weakly labeled data. A convolutional recurrent neural network (CRNN) with learnable gated linear units (GLUs) non-linearity applied on the log Mel spectrogram is proposed. In addition, a temporal attention method is proposed along the frames to predicate the locations of each audio event in a chunk from the weakly labeled data. We ranked the 1st and the 2nd as a team in these two sub-tasks of DCASE 2017 challenge with F value 55.6\% and Equal error 0.73, respectively.",http://aclweb.org/anthology/D18-1158,D18-1,D18-1158,https://arxiv.org/abs/1710.00343,"('Yubo Chen', 'Hang Yang', 'Kang Liu', 'Jun Zhao', 'Yantao Jia')",10,Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms,EMNLP,2018
"We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank. We isolate a number of dependency relations which previous models neglect but which contribute to higher parse accuracy.",http://aclweb.org/anthology/D18-1159,D18-1,D18-1159,https://arxiv.org/abs/cs/0011040,"('Tianze Shi', 'Lillian Lee')",10,Valency-Augmented Dependency Parsing,EMNLP,2018
"Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.",http://aclweb.org/anthology/D18-1160,D18-1,D18-1160,https://arxiv.org/abs/1808.09111,"('Junxian He', 'Graham Neubig', 'Taylor Berg-Kirkpatrick')",10,Unsupervised Learning of Syntactic Structure with Invertible Neural Projections,EMNLP,2018
"Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however, neural models have not surpassed state-of-the-art approaches in constituency parsing. To remedy this, we introduce a new shift-reduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n^3) oracles for standard dependency parsing. Training with this oracle, we achieve the best F1 scores on both English and French of any parser that does not use reranking or external data.",http://aclweb.org/anthology/D18-1161,D18-1,D18-1161,https://arxiv.org/abs/1612.06475,"('Daniel Fernández-González', 'Carlos Gómez-Rodríguez')",10,Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing,EMNLP,2018
"We introduce a method to reduce constituent parsing to sequence labeling. For each word w_t, it generates a label that encodes: (1) the number of ancestors in the tree that the words w_t and w_{t+1} have in common, and (2) the nonterminal symbol at the lowest common ancestor. We first prove that the proposed encoding function is injective for any tree without unary branches. In practice, the approach is made extensible to all constituency trees by collapsing unary branches. We then use the PTB and CTB treebanks as testbeds and propose a set of fast baselines. We achieve 90% F-score on the PTB test set, outperforming the Vinyals et al. (2015) sequence-to-sequence parser. In addition, sacrificing some accuracy, our approach achieves the fastest constituent parsing speeds reported to date on PTB by a wide margin.",http://aclweb.org/anthology/D18-1162,D18-1,D18-1162,https://arxiv.org/abs/1810.08994,"('Carlos Gómez-Rodríguez', 'David Vilares')",10,Constituent Parsing as Sequence Labeling,EMNLP,2018
"Human body part parsing, or human semantic part segmentation, is fundamental to many computer vision tasks. In conventional semantic segmentation methods, the ground truth segmentations are provided, and fully convolutional networks (FCN) are trained in an end-to-end scheme. Although these methods have demonstrated impressive results, their performance highly depends on the quantity and quality of training data. In this paper, we present a novel method to generate synthetic human part segmentation data using easily-obtained human keypoint annotations. Our key idea is to exploit the anatomical similarity among human to transfer the parsing results of a person to another person with similar pose. Using these estimated results as additional training data, our semi-supervised model outperforms its strong-supervised counterpart by 6 mIOU on the PASCAL-Person-Part dataset, and we achieve state-of-the-art human parsing results. Our approach is general and can be readily extended to other object/animal parsing task assuming that their anatomical similarity can be annotated by keypoints. The proposed model and accompanying source code are available at https://github.com/MVIG-SJTU/WSHP",http://aclweb.org/anthology/D18-1163,D18-1,D18-1163,https://arxiv.org/abs/1805.04310,"('Dingquan Wang', 'Jason Eisner')",10,Synthetic Data Made to Order: The Case of Parsing,EMNLP,2018
"Visual Question Answering (VQA) has attracted attention from both computer vision and natural language processing communities. Most existing approaches adopt the pipeline of representing an image via pre-trained CNNs, and then using the uninterpretable CNN features in conjunction with the question to predict the answer. Although such end-to-end models might report promising performance, they rarely provide any insight, apart from the answer, into the VQA process. In this work, we propose to break up the end-to-end VQA into two steps: explaining and reasoning, in an attempt towards a more explainable VQA by shedding light on the intermediate results between these two steps. To that end, we first extract attributes and generate descriptions as explanations for an image using pre-trained attribute detectors and image captioning models, respectively. Next, a reasoning module utilizes these explanations in place of the image to infer an answer to the question. The advantages of such a breakdown include: (1) the attributes and captions can reflect what the system extracts from the image, thus can provide some explanations for the predicted answer; (2) these intermediate results can help us identify the inabilities of both the image understanding part and the answer inference part when the predicted answer is wrong. We conduct extensive experiments on a popular VQA dataset and dissect all results according to several measurements of the explanation quality. Our system achieves comparable performance with the state-of-the-art, yet with added benefits of explainability and the inherent ability to further improve with higher quality explanations.",http://aclweb.org/anthology/D18-1164,D18-1,D18-1164,https://arxiv.org/abs/1801.09041,"('Qing Li', 'Jianlong Fu', 'Dongfei Yu', 'Tao Mei', 'Jiebo Luo')",10,Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions,EMNLP,2018
"Active learning identifies data points to label that are expected to be the most useful in improving a supervised model. Opportunistic active learning incorporates active learning into interactive tasks that constrain possible queries during interactions. Prior work has shown that opportunistic active learning can be used to improve grounding of natural language descriptions in an interactive object retrieval task. In this work, we use reinforcement learning for such an object retrieval task, to learn a policy that effectively trades off task completion with model improvement that would benefit future tasks.",http://aclweb.org/anthology/D18-1165,D18-1,D18-1165,https://arxiv.org/abs/1808.10009,"('Aishwarya Padmakumar', 'Peter Stone', 'Raymond Mooney')",10,Learning a Policy for Opportunistic Active Learning,EMNLP,2018
"Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems. The data and leaderboard are available at http://hucvl.github.io/recipeqa.",http://aclweb.org/anthology/D18-1166,D18-1,D18-1166,https://arxiv.org/abs/1809.00812,"('Semih Yagcioglu', 'Aykut Erdem', 'Erkut Erdem', 'Nazli Ikizler-Cinbis')",10,RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes,EMNLP,2018
"Video Question Answering is a challenging problem in visual information retrieval, which provides the answer to the referenced video content according to the question. However, the existing visual question answering approaches mainly tackle the problem of static image question, which may be ineffectively for video question answering due to the insufficiency of modeling the temporal dynamics of video contents. In this paper, we study the problem of video question answering by modeling its temporal dynamics with frame-level attention mechanism. We propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering. We then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance. We construct a large-scale video question answering dataset. We conduct the experiments on both multiple-choice and open-ended video question answering tasks to show the effectiveness of the proposed method.",http://aclweb.org/anthology/D18-1167,D18-1,D18-1167,https://arxiv.org/abs/1707.06355,"('Jie Lei', 'Licheng Yu', 'Mohit Bansal', 'Tamara Berg')",10,"TVQA: Localized, Compositional Video Question Answering",EMNLP,2018
"Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset. Our dataset consists of two parts: a dataset with real videos and template sentences (TEMPO - Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO - Human Language).",http://aclweb.org/anthology/D18-1168,D18-1,D18-1168,https://arxiv.org/abs/1809.01337,"('Lisa Anne Hendricks', 'Oliver Wang', 'Eli Shechtman', 'Josef Sivic', 'Trevor Darrell', 'Bryan Russell')",10,Localizing Moments in Video with Temporal Language,EMNLP,2018
"This paper introduces the first dataset for evaluating English-Chinese Bilingual Contextual Word Similarity, namely BCWS (https://github.com/MiuLab/BCWS). The dataset consists of 2,091 English-Chinese word pairs with the corresponding sentential contexts and their similarity scores annotated by the human. Our annotated dataset has higher consistency compared to other similar datasets. We establish several baselines for the bilingual embedding task to benchmark the experiments. Modeling cross-lingual sense representations as provided in this dataset has the potential of moving artificial intelligence from monolingual understanding towards multilingual understanding.",http://aclweb.org/anthology/D18-1169,D18-1,D18-1169,https://arxiv.org/abs/1810.08951,"('Mohammad Taher Pilehvar', 'Dimitri Kartsaklis', 'Victor Prokhorov', 'Nigel Collier')",10,Card-660: Cambridge Rare Word Dataset - a Reliable Benchmark for Infrequent Word Representation Models,EMNLP,2018
"Word Sense Disambiguation (WSD) aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-theart systems on several English all-words WSD datasets.",http://aclweb.org/anthology/D18-1170,D18-1,D18-1170,https://arxiv.org/abs/1805.08028,"('Fuli Luo', 'Tianyu Liu', 'Zexue He', 'Qiaolin Xia', 'Zhifang Sui', 'Baobao Chang')",10,Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical Co-Attention,EMNLP,2018
"In modern agriculture, usually weeds control consists in spraying herbicides all over the agricultural field. This practice involves significant waste and cost of herbicide for farmers and environmental pollution. One way to reduce the cost and environmental impact is to allocate the right doses of herbicide at the right place and at the right time (Precision Agriculture). Nowadays, Unmanned Aerial Vehicle (UAV) is becoming an interesting acquisition system for weeds localization and management due to its ability to obtain the images of the entire agricultural field with a very high spatial resolution and at low cost. Despite the important advances in UAV acquisition systems, automatic weeds detection remains a challenging problem because of its strong similarity with the crops. Recently Deep Learning approach has shown impressive results in different complex classification problem. However, this approach needs a certain amount of training data but, creating large agricultural datasets with pixel-level annotations by expert is an extremely time consuming task. In this paper, we propose a novel fully automatic learning method using Convolutional Neuronal Networks (CNNs) with unsupervised training dataset collection for weeds detection from UAV images. The proposed method consists in three main phases. First we automatically detect the crop lines and using them to identify the interline weeds. In the second phase, interline weeds are used to constitute the training dataset. Finally, we performed CNNs on this dataset to build a model able to detect the crop and weeds in the images. The results obtained are comparable to the traditional supervised training data labeling. The accuracy gaps are 1.5% in the spinach field and 6% in the bean field.",http://aclweb.org/anthology/D18-1171,D18-1,D18-1171,https://arxiv.org/abs/1805.12395,"('Erik-Lân Do Dinh', 'Hannah Wieland', 'Iryna Gurevych')",10,Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations,EMNLP,2018
"We consider the problem of learning distributed representations for documents in data streams. The documents are represented as low-dimensional vectors and are jointly learned with distributed vector representations of word tokens using a hierarchical framework with two embedded neural language models. In particular, we exploit the context of documents in streams and use one of the language models to model the document sequences, and the other to model word sequences within them. The models learn continuous vector representations for both word tokens and documents such that semantically similar documents and words are close in a common vector space. We discuss extensions to our model, which can be applied to personalized recommendation and social relationship mining by adding further user layers to the hierarchy, thus learning user-specific vectors to represent individual preferences. We validated the learned representations on a public movie rating data set from MovieLens, as well as on a large-scale Yahoo News data comprising three months of user activity logs collected on Yahoo servers. The results indicate that the proposed model can learn useful representations of both documents and word tokens, outperforming the current state-of-the-art by a large margin.",http://aclweb.org/anthology/D18-1172,D18-1,D18-1172,https://arxiv.org/abs/1606.08689,"('Olof Görnerup', 'Daniel Gillblad')",10,Streaming word similarity mining on the cheap,EMNLP,2018
"Augmenting a neural network with memory that can grow without growing the number of trained parameters is a recent powerful concept with many exciting applications. We propose a design of memory augmented neural networks (MANNs) called Labeled Memory Networks (LMNs) suited for tasks requiring online adaptation in classification models. LMNs organize the memory with classes as the primary key.The memory acts as a second boosted stage following a regular neural network thereby allowing the memory and the primary network to play complementary roles. Unlike existing MANNs that write to memory for every instance and use LRU based memory replacement, LMNs write only for instances with non-zero loss and use label-based memory replacement. We demonstrate significant accuracy gains on various tasks including word-modelling and few-shot learning. In this paper, we establish their potential in online adapting a batch trained neural network to domain-relevant labeled data at deployment time. We show that LMNs are better than other MANNs designed for meta-learning. We also found them to be more accurate and faster than state-of-the-art methods of retuning model parameters for adapting to domain-specific labeled data.",http://aclweb.org/anthology/D18-1173,D18-1,D18-1173,https://arxiv.org/abs/1707.01461,"('Jingyuan Sun', 'Shaonan Wang', 'Chengqing Zong')",10,"Memory, Show the Way: Memory Based Few Shot Word Representation Learning",EMNLP,2018
"Agglutinative languages such as Turkish, Finnish and Hungarian require morphological disambiguation before further processing due to the complex morphology of words. A morphological disambiguator is used to select the correct morphological analysis of a word. Morphological disambiguation is important because it generally is one of the first steps of natural language processing and its performance affects subsequent analyses. In this paper, we propose a system that uses deep learning techniques for morphological disambiguation. Many of the state-of-the-art results in computer vision, speech recognition and natural language processing have been obtained through deep learning models. However, applying deep learning techniques to morphologically rich languages is not well studied. In this work, while we focus on Turkish morphological disambiguation we also present results for French and German in order to show that the proposed architecture achieves high accuracy with no language-specific feature engineering or additional resource. In the experiments, we achieve 84.12, 88.35 and 93.78 morphological disambiguation accuracy among the ambiguous words for Turkish, German and French respectively.",http://aclweb.org/anthology/D18-1174,D18-1,D18-1174,https://arxiv.org/abs/1702.03654,"('Karol Grzegorczyk', 'Marcin Kurdziel')",10,Disambiguated skip-gram model,EMNLP,2018
"During natural disasters and conflicts, information about what happened is often confusing, messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce Story Salads, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task and that it is necessary to take into account global context and coherence.",http://aclweb.org/anthology/D18-1175,D18-1,D18-1175,https://arxiv.org/abs/1810.13391,"('Su Wang', 'Eric Holgate', 'Greg Durrett', 'Katrin Erk')",10,Picking Apart Story Salads,EMNLP,2018
"Sentence representations can capture a wide range of information that cannot be captured by local features based on character or word N-grams. This paper examines the usefulness of universal sentence representations for evaluating the quality of machine translation. Although it is difficult to train sentence representations using small-scale translation datasets with manual evaluation, sentence representations trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. Experimental results of the WMT-2016 dataset show that the proposed method achieves state-of-the-art performance with sentence representation features only.",http://aclweb.org/anthology/D18-1176,D18-1,D18-1176,https://arxiv.org/abs/1805.07469,"('Douwe Kiela', 'Changhan Wang', 'Kyunghyun Cho')",10,Dynamic Meta-Embeddings for Improved Sentence Representations,EMNLP,2018
"Inferring topics from the overwhelming amount of short texts becomes a critical but challenging task for many content analysis tasks, such as content charactering, user interest profiling, and emerging topic detecting. Existing methods such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA) cannot solve this prob- lem very well since only very limited word co-occurrence information is available in short texts. This paper studies how to incorporate the external word correlation knowledge into short texts to improve the coherence of topic modeling. Based on recent results in word embeddings that learn se- mantically representations for words from a large corpus, we introduce a novel method, Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudo- texts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic. The experiments on real-world datasets validate the effectiveness of our model comparing with the state-of-the-art models.",http://aclweb.org/anthology/D18-1177,D18-1,D18-1177,https://arxiv.org/abs/1609.08496,"('Melissa Ailem', 'Bowen Zhang', 'Aurélien Bellet', 'Pascal Denis', 'Fei Sha')",10,A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images,EMNLP,2018
"Most natural language processing systems based on machine learning are not robust to domain shift. For example, a state-of-the-art syntactic dependency parser trained on Wall Street Journal sentences has an absolute drop in performance of more than ten points when tested on textual data from the Web. An efficient solution to make these methods more robust to domain shift is to first learn a word representation using large amounts of unlabeled data from both domains, and then use this representation as features in a supervised learning algorithm. In this paper, we propose to use hidden Markov models to learn word representations for part-of-speech tagging. In particular, we study the influence of using data from the source, the target or both domains to learn the representation and the different ways to represent words using an HMM.",http://aclweb.org/anthology/D18-1178,D18-1,D18-1178,https://arxiv.org/abs/1312.4092,"('Murhaf Fares', 'Stephan Oepen', 'Erik Velldal')",10,Transfer and Multi-Task Learning for Noun–Noun Compound Interpretation,EMNLP,2018
"Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.",http://aclweb.org/anthology/D18-1179,D18-1,D18-1179,https://arxiv.org/abs/1808.08949,"('Matthew Peters', 'Mark Neumann', 'Luke Zettlemoyer', 'Wen-tau Yih')",10,Dissecting Contextual Word Embeddings: Architecture and Representation,EMNLP,2018
We describe an inventory of semantic relations that are expressed by prepositions. We define these relations by building on the word sense disambiguation task for prepositions and propose a mapping from preposition senses to the relation labels by collapsing semantically related senses across prepositions.,http://aclweb.org/anthology/D18-1180,D18-1,D18-1180,https://arxiv.org/abs/1305.5785,"('Hongyu Gong', 'Jiaqi Mu', 'Suma Bhat', 'Pramod Viswanath')",10,Preposition Sense Disambiguation and Representation,EMNLP,2018
"Distributed representations of words have been shown to capture lexical semantics, as demonstrated by their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present several definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer designed to leverage morphology can complement word-level embeddings. Finally, an error analysis suggests that the errors made by a definition model may provide insight into the shortcomings of word embeddings.",http://aclweb.org/anthology/D18-1181,D18-1,D18-1181,https://arxiv.org/abs/1612.00394,"('Tom Bosc', 'Pascal Vincent')",10,Auto-Encoding Dictionary Definitions into Consistent Word Embeddings,EMNLP,2018
"We propose a range of deep lexical acquisition methods which make use of morphological, syntactic and ontological language resources to model word similarity and bootstrap from a seed lexicon. The different methods are deployed in learning lexical items for a precision grammar, and shown to each have strengths and weaknesses over different word classes. A particular focus of this paper is the relative accessibility of different language resource types, and predicted ``bang for the buck'' associated with each in deep lexical acquisition applications.",http://aclweb.org/anthology/D18-1182,D18-1,D18-1182,https://arxiv.org/abs/0709.2401,"('Gabriel Stanovsky', 'Mark Hopkins')",10,Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources,EMNLP,2018
"Previous work has shown that neural encoder-decoder speech recognition can be improved with hierarchical multitask learning, where auxiliary tasks are added at intermediate layers of a deep encoder. We explore the effect of hierarchical multitask learning in the context of connectionist temporal classification (CTC)-based speech recognition, and investigate several aspects of this approach. Consistent with previous work, we observe performance improvements on telephone conversational speech recognition (specifically the Eval2000 test sets) when training a subword-level CTC model with an auxiliary phone loss at an intermediate layer. We analyze the effects of a number of experimental variables (like interpolation constant and position of the auxiliary loss function), performance in lower-resource settings, and the relationship between pretraining and multitask learning. We observe that the hierarchical multitask approach improves over standard multitask training in our higher-data experiments, while in the low-resource settings standard multitask training works well. The best results are obtained by combining hierarchical multitask learning and pretraining, which improves word error rates by 3.4% absolute on the Eval2000 test sets.",http://aclweb.org/anthology/D18-1183,D18-1,D18-1183,https://arxiv.org/abs/1807.06234,"('Lizhen Liu', 'Xiao Hu', 'Wei Song', 'Ruiji Fu', 'Ting Liu', 'Guoping Hu')",10,Neural Multitask Learning for Simile Recognition,EMNLP,2018
"This work improves monolingual sentence alignment for text simplification, specifically for text in standard and simple Wikipedia. We introduce a convolutional neural network structure to model similarity between two sentences. Due to the limitation of available parallel corpora, the model is trained in a semi-supervised way, by using the output of a knowledge-based high performance aligning system. We apply the resulting similarity score to rescore the knowledge-based output, and adapt the model by a small hand-aligned dataset. Experiments show that both rescoring and adaptation improve the performance of knowledge-based method.",http://aclweb.org/anthology/D18-1184,D18-1,D18-1184,https://arxiv.org/abs/1809.08703,"('Yang Liu', 'Matt Gardner', 'Mirella Lapata')",10,Structured Alignment Networks for Matching Sentences,EMNLP,2018
"This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new architecture where alignment pairs are compared, compressed and then propagated to upper layers for enhanced representation learning. Secondly, we adopt factorization layers for efficient and expressive compression of alignment vectors into scalar features, which are then used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving competitive performance on all. A lightweight parameterization of our model also enjoys a $\approx 3$ times reduction in parameter size compared to the existing state-of-the-art models, e.g., ESIM and DIIN, while maintaining competitive performance. Additionally, visual analysis shows that our propagated features are highly interpretable.",http://aclweb.org/anthology/D18-1185,D18-1,D18-1185,https://arxiv.org/abs/1801.00102,"('Yi Tay', 'Anh Tuan Luu', 'Siu Cheung Hui')",10,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",EMNLP,2018
"Natural language inference (NLI) is a central problem in language understanding. End-to-end artificial neural networks have reached state-of-the-art performance in NLI field recently.   In this paper, we propose Character-level Intra Attention Network (CIAN) for the NLI task. In our model, we use the character-level convolutional network to replace the standard word embedding layer, and we use the intra attention to capture the intra-sentence semantics. The proposed CIAN model provides improved results based on a newly published MNLI corpus.",http://aclweb.org/anthology/D18-1186,D18-1,D18-1186,https://arxiv.org/abs/1707.07469,"('Jingjing Gong', 'Xipeng Qiu', 'Xinchi Chen', 'Dong Liang', 'Xuanjing Huang')",10,Convolutional Interaction Network for Natural Language Inference,EMNLP,2018
"State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowledge intensive domains, where training data is limited. To address this gap, we introduce MedNLI - a dataset annotated by doctors, performing a natural language inference task (NLI), grounded in the medical history of patients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. SNLI) and 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies.",http://aclweb.org/anthology/D18-1187,D18-1,D18-1187,https://arxiv.org/abs/1808.06752,"('Alexey Romanov', 'Chaitanya Shivade')",10,Lessons from Natural Language Inference in the Clinical Domain,EMNLP,2018
"We present a generative model to map natural language questions into SQL queries. Existing neural network based approaches typically generate a SQL query word-by-word, however, a large portion of the generated results are incorrect or not executable due to the mismatch between question words and table contents. Our approach addresses this problem by considering the structure of table and the syntax of SQL language. The quality of the generated SQL query is significantly improved through (1) learning to replicate content from column names, cells or SQL keywords; and (2) improving the generation of WHERE clause by leveraging the column-cell relation. Experiments are conducted on WikiSQL, a recently released dataset with the largest question-SQL pairs. Our approach significantly improves the state-of-the-art execution accuracy from 69.0% to 74.4%.",http://aclweb.org/anthology/D18-1188,D18-1,D18-1188,https://arxiv.org/abs/1804.08338,"('Daya Guo', 'Yibo Sun', 'Duyu Tang', 'Nan Duan', 'Jian Yin', 'Hong Chi', 'James Cao', 'Peng Chen', 'Ming Zhou')",10,Question Generation from SQL Queries Improves Neural Semantic Parsing,EMNLP,2018
"This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.",http://aclweb.org/anthology/D18-1189,D18-1,D18-1189,https://arxiv.org/abs/1608.03000,"('Zexuan Zhong', 'Jiaqi Guo', 'Wei Yang', 'Jian Peng', 'Tao Xie', 'Jian-Guang Lou', 'Ting Liu', 'Dongmei Zhang')",10,SemRegex: A Semantics-Based Approach for Generating Regular Expressions from Natural Language Specifications,EMNLP,2018
"Building a semantic parser quickly in a new domain is a fundamental challenge for conversational interfaces, as current semantic parsers require expensive supervision and lack the ability to generalize to new domains. In this paper, we introduce a zero-shot approach to semantic parsing that can parse utterances in unseen domains while only being trained on examples in other source domains. First, we map an utterance to an abstract, domain-independent, logical form that represents the structure of the logical form, but contains slots instead of KB constants. Then, we replace slots with KB constants via lexical alignment scores and global inference. Our model reaches an average accuracy of 53.4% on 7 domains in the Overnight dataset, substantially better than other zero-shot baselines, and performs as good as a parser trained on over 30% of the target domain examples.",http://aclweb.org/anthology/D18-1190,D18-1,D18-1190,https://arxiv.org/abs/1804.07918,"('Jonathan Herzig', 'Jonathan Berant')",10,Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing,EMNLP,2018
"We present a simple and accurate span-based model for semantic role labeling (SRL). Our model directly takes into account all possible argument spans and scores them for each label. At decoding time, we greedily select higher scoring labeled spans. One advantage of our model is to allow us to design and use span-level features, that are difficult to use in token-based BIO tagging approaches. Experimental results demonstrate that our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively.",http://aclweb.org/anthology/D18-1191,D18-1,D18-1191,https://arxiv.org/abs/1810.02245,"('Hiroki Ouchi', 'Hiroyuki Shindo', 'Yuji Matsumoto')",10,A Span Selection Model for Semantic Role Labeling,EMNLP,2018
"Source code is rarely written in isolation. It depends significantly on the programmatic context, such as the class that the code would reside in. To study this phenomenon, we introduce the task of generating class member functions given English documentation and the programmatic context provided by the rest of the class. This task is challenging because the desired code can vary greatly depending on the functionality the class provides (e.g., a sort function may or may not be available when we are asked to ""return the smallest element"" in a particular member variable list). We introduce CONCODE, a new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment. We also present a detailed error analysis suggesting that there is significant room for future work on this task.",http://aclweb.org/anthology/D18-1192,D18-1,D18-1192,https://arxiv.org/abs/1808.09588,"('Srinivasan Iyer', 'Ioannis Konstas', 'Alvin Cheung', 'Luke Zettlemoyer')",10,Mapping Language to Code in Programmatic Context,EMNLP,2018
"We consider task and motion planning in complex dynamic environments for problems expressed in terms of a set of Linear Temporal Logic (LTL) constraints, and a reward function. We propose a methodology based on reinforcement learning that employs deep neural networks to learn low-level control policies as well as task-level option policies. A major challenge in this setting, both for neural network approaches and classical planning, is the need to explore future worlds of a complex and interactive environment. To this end, we integrate Monte Carlo Tree Search with hierarchical neural net control policies trained on expressive LTL specifications. This paper investigates the ability of neural networks to learn both LTL constraints and control policies in order to generate task plans in complex environments. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying given rules of the road.",http://aclweb.org/anthology/D18-1193,D18-1,D18-1193,https://arxiv.org/abs/1703.07887,"('Tao Yu', 'Michihiro Yasunaga', 'Kai Yang', 'Rui Zhang', 'Dongxu Wang', 'Zifan Li', 'Dragomir Radev')",10,SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task,EMNLP,2018
"In this dissertation we study regular expression based parsing and the use of grammatical specifications for the synthesis of fast, streaming string-processing programs.   In the first part we develop two linear-time algorithms for regular expression based parsing with Perl-style greedy disambiguation. The first algorithm operates in two passes in a semi-streaming fashion, using a constant amount of working memory and an auxiliary tape storage which is written in the first pass and consumed by the second. The second algorithm is a single-pass and optimally streaming algorithm which outputs as much of the parse tree as is semantically possible based on the input prefix read so far, and resorts to buffering as many symbols as is required to resolve the next choice. Optimality is obtained by performing a PSPACE-complete pre-analysis on the regular expression.   In the second part we present Kleenex, a language for expressing high-performance streaming string processing programs as regular grammars with embedded semantic actions, and its compilation to streaming string transducers with worst-case linear-time performance. Its underlying theory is based on transducer decomposition into oracle and action machines, and a finite-state specialization of the streaming parsing algorithm presented in the first part. In the second part we also develop a new linear-time streaming parsing algorithm for parsing expression grammars (PEG) which generalizes the regular grammars of Kleenex. The algorithm is based on a bottom-up tabulation algorithm reformulated using least fixed points and evaluated using an instance of the chaotic iteration scheme by Cousot and Cousot.",http://aclweb.org/anthology/D18-1194,D18-1,D18-1194,https://arxiv.org/abs/1704.08820,"('Sheng Zhang', 'Xutai Ma', 'Rachel Rudinger', 'Kevin Duh', 'Benjamin Van Durme')",10,Cross-lingual Decompositional Semantic Parsing,EMNLP,2018
"For building question answering systems and natural language interfaces, semantic parsing has emerged as an important and powerful paradigm. Semantic parsers map natural language into logical forms, the classic representation for many important linguistic phenomena. The modern twist is that we are interested in learning semantic parsers from data, which introduces a new layer of statistical and computational issues. This article lays out the components of a statistical semantic parser, highlighting the key challenges. We will see that semantic parsing is a rich fusion of the logical and the statistical world, and that this fusion will play an integral role in the future of natural language understanding systems.",http://aclweb.org/anthology/D18-1195,D18-1,D18-1195,https://arxiv.org/abs/1603.06677,"('Igor Labutov', 'Bishan Yang', 'Tom Mitchell')",10,Learning to Learn Semantic Parsers from Natural Language Supervision,EMNLP,2018
"To parse images into fine-grained semantic parts, the complex fine-grained elements will put it in trouble when using off-the-shelf semantic segmentation networks. In this paper, for image parsing task, we propose to parse images from coarse to fine with progressively refined semantic classes. It is achieved by stacking the segmentation layers in a segmentation network several times. The former segmentation module parses images at a coarser-grained level, and the result will be feed to the following one to provide effective contextual clues for the finer-grained parsing. To recover the details of small structures, we add skip connections from shallow layers of the network to fine-grained parsing modules. As for the network training, we merge classes in groundtruth to get coarse-to-fine label maps, and train the stacked network with these hierarchical supervision end-to-end. Our coarse-to-fine stacked framework can be injected into many advanced neural networks to improve the parsing results. Extensive evaluations on several public datasets including face parsing and human parsing well demonstrate the superiority of our method.",http://aclweb.org/anthology/D18-1196,D18-1,D18-1196,https://arxiv.org/abs/1804.08256,"('Jesse Dunietz', 'Jaime Carbonell', 'Lori Levin')",10,DeepCx: A transition-based approach for shallow semantic parsing with complex constructional triggers,EMNLP,2018
"We consider the problem of neural semantic parsing, which translates natural language questions into executable SQL queries. We introduce a new mechanism, execution guidance, to leverage the semantics of SQL. It detects and excludes faulty programs during the decoding procedure by conditioning on the execution of partially generated program. The mechanism can be used with any autoregressive generative model, which we demonstrate on four state-of-the-art recurrent or template-based semantic parsing models. We demonstrate that execution guidance universally improves model performance on various text-to-SQL datasets with different scales and query complexity: WikiSQL, ATIS, and GeoQuery. As a result, we achieve new state-of-the-art execution accuracy of 83.8% on WikiSQL.",http://aclweb.org/anthology/D18-1197,D18-1,D18-1197,https://arxiv.org/abs/1807.03100,"('Semih Yavuz', 'Izzeddin Gur', 'Yu Su', 'Xifeng Yan')",10,What It Takes to Achieve 100 Percent Condition Accuracy on WikiSQL,EMNLP,2018
"The Abstract Meaning Representation (AMR) is a representation for open-domain rich semantics, with potential use in fields like event extraction and machine translation. Node generation, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs by transformations on spans of text, which allows for more robust learning of this stage. Our set of construction actions generalize better than the previous approach, and can be learned with a simple classifier. We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end performance by 3 F$_1$ on both the LDC2013E117 and LDC2014T12 datasets.",http://aclweb.org/anthology/D18-1198,D18-1,D18-1198,https://arxiv.org/abs/1506.03139,"('Zhijiang Guo', 'Wei Lu')",10,Better Transition-Based AMR Parsing with a Refined Search Space,EMNLP,2018
"In recent years, deep hashing methods have been proved to be efficient since it employs convolutional neural network to learn features and hashing codes simultaneously. However, these methods are mostly supervised. In real-world application, it is a time-consuming and overloaded task for annotating a large number of images. In this paper, we propose a novel unsupervised deep hashing method for large-scale image retrieval. Our method, namely unsupervised semantic deep hashing (\textbf{USDH}), uses semantic information preserved in the CNN feature layer to guide the training of network. We enforce four criteria on hashing codes learning based on VGG-19 model: 1) preserving relevant information of feature space in hashing space; 2) minimizing quantization loss between binary-like codes and hashing codes; 3) improving the usage of each bit in hashing codes by using maximum information entropy, and 4) invariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have demonstrated that \textbf{USDH} outperforms several state-of-the-art unsupervised hashing methods for image retrieval. We also conduct experiments on Oxford 17 datasets for fine-grained classification to verify its efficiency for other computer vision tasks.",http://aclweb.org/anthology/D18-1199,D18-1,D18-1199,https://arxiv.org/abs/1803.06911,"('Changsheng Liu', 'Rebecca Hwa')",10,Heuristically Informed Unsupervised Idiom Usage Recognition,EMNLP,2018
"Here we study polysemy as a potential learning bias in vocabulary learning in children. We employ a massive set of transcriptions of conversations between children and adults in English, to analyze the evolution of mean polysemy in the words produced by children whose ages range between 10 and 60 months. Our results show that mean polysemy in children increases over time in two phases, i.e. a fast growth till the 31st month followed by a slower tendency towards adult speech. In contrast, no dependency with time is found in adults. This suggests that children have a preference for non-polysemous words in their early stages of vocabulary acquisition. Our hypothesis is twofold: (a) polysemy is a standalone bias or (b) polysemy is a side-effect of other biases. Interestingly, the bias for low polysemy described above weakens when controlling for syntactic category (noun, verb, adjective or adverb). The pattern of the evolution of polysemy suggests that both hypotheses may apply to some extent, and that (b) would originate from a combination of the well-known preference for nouns and the lower polysemy of nouns with respect to other syntactic categories.",http://aclweb.org/anthology/D18-1200,D18-1,D18-1200,https://arxiv.org/abs/1611.08807,"('Haim Dubossarsky', 'Eitan Grossman', 'Daphna Weinshall')",10,Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research,EMNLP,2018
"Semantic graphs, such as WordNet, are resources which curate natural language on two distinguishable layers. On the local level, individual relations between synsets (semantic building blocks) such as hypernymy and meronymy enhance our understanding of the words used to express their meanings. Globally, analysis of graph-theoretic properties of the entire net sheds light on the structure of human language as a whole. In this paper, we combine global and local properties of semantic graphs through the framework of Max-Margin Markov Graph Models (M3GM), a novel extension of Exponential Random Graph Model (ERGM) that scales to large multi-relational graphs. We demonstrate how such global modeling improves performance on the local task of predicting semantic relations between synsets, yielding new state-of-the-art results on the WN18RR dataset, a challenging version of WordNet link prediction in which ""easy"" reciprocal cases are removed. In addition, the M3GM model identifies multirelational motifs that are characteristic of well-formed lexical semantic ontologies.",http://aclweb.org/anthology/D18-1201,D18-1,D18-1201,https://arxiv.org/abs/1808.08644,"('Yuval Pinter', 'Jacob Eisenstein')",10,Predicting Semantic Relations using Global Graph Properties,EMNLP,2018
"This paper studies the potential of identifying lexical paraphrases within a single corpus, focusing on the extraction of verb paraphrases. Most previous approaches detect individual paraphrase instances within a pair (or set) of comparable corpora, each of them containing roughly the same information, and rely on the substantial level of correspondence of such corpora. We present a novel method that successfully detects isolated paraphrase instances within a single corpus without relying on any a-priori structure and information. A comparison suggests that an instance-based approach may be combined with a vector based approach in order to assess better the paraphrase likelihood for many verb pairs.",http://aclweb.org/anthology/D18-1202,D18-1,D18-1202,https://arxiv.org/abs/cs/0312058,"('Anne Cocos', 'Veronica Wharton', 'Ellie Pavlick', 'Marianna Apidianaki', 'Chris Callison-Burch')",10,Learning Scalar Adjective Intensity from Paraphrases,EMNLP,2018
"In this paper, we propose a new kernel-based co-occurrence measure that can be applied to sparse linguistic expressions (e.g., sentences) with a very short learning time, as an alternative to pointwise mutual information (PMI). As well as deriving PMI from mutual information, we derive this new measure from the Hilbert--Schmidt independence criterion (HSIC); thus, we call the new measure the pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of PMI that allows various similarity metrics (e.g., sentence embeddings) to be plugged in as kernels. Moreover, PHSIC can be estimated by simple and fast (linear in the size of the data) matrix calculations regardless of whether we use linear or nonlinear kernels. Empirically, in a dialogue response selection task, PHSIC is learned thousands of times faster than an RNN-based PMI while outperforming PMI in accuracy. In addition, we also demonstrate that PHSIC is beneficial as a criterion of a data selection task for machine translation owing to its ability to give high (low) scores to a consistent (inconsistent) pair with other pairs.",http://aclweb.org/anthology/D18-1203,D18-1,D18-1203,https://arxiv.org/abs/1809.00800,"('Sho Yokoi', 'Sosuke Kobayashi', 'Kenji Fukumizu', 'Jun Suzuki', 'Kentaro Inui')",10,Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions,EMNLP,2018
"Query relevance ranking and sentence saliency ranking are the two main tasks in extractive query-focused summarization. Previous supervised summarization systems often perform the two tasks in isolation. However, since reference summaries are the trade-off between relevance and saliency, using them as supervision, neither of the two rankers could be trained well. This paper proposes a novel summarization system called AttSum, which tackles the two tasks jointly. It automatically learns distributed representations for sentences as well as the document cluster. Meanwhile, it applies the attention mechanism to simulate the attentive reading of human behavior when a query is given. Extensive experiments are conducted on DUC query-focused summarization benchmark datasets. Without using any hand-crafted features, AttSum achieves competitive performance. It is also observed that the sentences recognized to focus on the query indeed meet the query need.",http://aclweb.org/anthology/D18-1204,D18-1,D18-1204,https://arxiv.org/abs/1604.00125,"('Yongzhen Wang', 'Xiaozhong Liu', 'Zheng Gao')",10,Neural Related Work Summarization with a Joint Context-driven Attention Mechanism,EMNLP,2018
"Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance using the latter is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset. Code is available at \url{https://github.com/sheffieldnlp/AMR2Text-summ}",http://aclweb.org/anthology/D18-1205,D18-1,D18-1205,https://arxiv.org/abs/1808.09160,"('Wei Li', 'Xinyan Xiao', 'Yajuan Lyu', 'Yuanzhuo Wang')",10,Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling,EMNLP,2018
"We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question ""What is the article about?"". We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.",http://aclweb.org/anthology/D18-1206,D18-1,D18-1206,https://arxiv.org/abs/1808.08745,"('Shashi Narayan', 'Shay B. Cohen', 'Mirella Lapata')",10,"Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",EMNLP,2018
"Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. Our preliminary study reveals nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from this problem. While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. To avoid generating fake facts in a summary, we leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text. The dual-attention sequence-to-sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions. Experiments on the Gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80%. Notably, the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text.",http://aclweb.org/anthology/D18-1207,D18-1,D18-1207,https://arxiv.org/abs/1711.04434,"('Wojciech Kryściński', 'Romain Paulus', 'Caiming Xiong', 'Richard Socher')",10,Improving Abstraction in Text Summarization,EMNLP,2018
"We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the summarization task.",http://aclweb.org/anthology/D18-1208,D18-1,D18-1208,https://arxiv.org/abs/1810.12343,"('Chris Kedzie', 'Kathleen McKeown', 'Hal Daume III')",10,Content Selection in Deep Learning Models of Summarization,EMNLP,2018
"Network embeddings, which learn low-dimensional representations for each vertex in a large-scale network, have received considerable attention in recent years. For a wide range of applications, vertices in a network are typically accompanied by rich textual information such as user profiles, paper abstracts, etc. We propose to incorporate semantic features into network embeddings by matching important words between text sequences for all pairs of vertices. We introduce a word-by-word alignment framework that measures the compatibility of embeddings between word pairs, and then adaptively accumulates these alignment features with a simple yet effective aggregation function. In experiments, we evaluate the proposed framework on three real-world benchmarks for downstream tasks, including link prediction and multi-label vertex classification. Results demonstrate that our model outperforms state-of-the-art network embedding methods by a large margin.",http://aclweb.org/anthology/D18-1209,D18-1,D18-1209,https://arxiv.org/abs/1808.09633,"('Dinghan Shen', 'Xinyuan Zhang', 'Ricardo Henao', 'Lawrence Carin')",10,Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment,EMNLP,2018
"Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences. In this paper, we consider an approach of using a small meta network to learn context-sensitive convolutional filters for text processing. The role of meta network is to abstract the contextual information of a sentence or document into a set of input-aware filters. We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations. In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-sensitive filters, consistently outperforms the standard CNN and attention-based CNN baselines. By visualizing the learned context-sensitive filters, we further validate and rationalize the effectiveness of proposed framework.",http://aclweb.org/anthology/D18-1210,D18-1,D18-1210,https://arxiv.org/abs/1709.08294,"('Dinghan Shen', 'Martin Renqiang Min', 'Yitong Li', 'Lawrence Carin')",10,Learning Context-Sensitive Convolutional Filters for Text Processing,EMNLP,2018
"We explore several new models for document relevance ranking, building upon the Deep Relevance Matching Model (DRMM) of Guo et al. (2016). Unlike DRMM, which uses context-insensitive encodings of terms and query-document term interactions, we inject rich context-sensitive encodings throughout our models, inspired by PACRR's (Hui et al., 2017) convolutional n-gram matching features, but extended in several ways including multiple views of query and document inputs. We test our models on datasets from the BIOASQ question answering challenge (Tsatsaronis et al., 2015) and TREC ROBUST 2004 (Voorhees, 2005), showing they outperform BM25-based baselines, DRMM, and PACRR.",http://aclweb.org/anthology/D18-1211,D18-1,D18-1211,https://arxiv.org/abs/1809.01682,"('Ryan McDonald', 'George Brokos', 'Ion Androutsopoulos')",10,Deep Relevance Ranking Using Enhanced Document-Query Interactions,EMNLP,2018
"We propose a fully unsupervised framework for ad-hoc cross-lingual information retrieval (CLIR) which requires no bilingual data at all. The framework leverages shared cross-lingual word embedding spaces in which terms, queries, and documents can be represented, irrespective of their actual language. The shared embedding spaces are induced solely on the basis of monolingual corpora in two languages through an iterative process based on adversarial neural networks. Our experiments on the standard CLEF CLIR collections for three language pairs of varying degrees of language similarity (English-Dutch/Italian/Finnish) demonstrate the usefulness of the proposed fully unsupervised approach. Our CLIR models with unsupervised cross-lingual embeddings outperform baselines that utilize cross-lingual embeddings induced relying on word-level and document-level alignments. We then demonstrate that further improvements can be achieved by unsupervised ensemble CLIR models. We believe that the proposed framework is the first step towards development of effective CLIR models for language pairs and domains where parallel data are scarce or non-existent.",http://aclweb.org/anthology/D18-1212,D18-1,D18-1212,https://arxiv.org/abs/1805.00879,"('Bo Li', 'Ping Cheng')",10,Learning Neural Representation for CLIR with Adversarial Framework,EMNLP,2018
"While in a classification or a regression setting a label or a value is assigned to each individual document, in a ranking setting we determine the relevance ordering of the entire input document list. This difference leads to the notion of relative relevance between documents in ranking. The majority of the existing learning-to-rank algorithms model such relativity at the loss level using pairwise or listwise loss functions. However, they are restricted to pointwise scoring functions, i.e., the relevance score of a document is computed based on the document itself, regardless of the other documents in the list. In this paper, we overcome this limitation by proposing generalized groupwise scoring functions (GSFs), in which the relevance score of a document is determined jointly by groups of documents in the list. We learn GSFs with a deep neural network architecture, and demonstrate that several representative learning-to-rank algorithms can be modeled as special cases in our framework. We conduct evaluation using the public MSLR-WEB30K dataset, and our experiments show that GSFs lead to significant performance improvements both in a standalone deep learning architecture, or when combined with a state-of-the-art tree-based learning-to-rank algorithm.",http://aclweb.org/anthology/D18-1213,D18-1,D18-1213,https://arxiv.org/abs/1811.04415,"('Swayambhu Nath Ray', 'Shib Sankar Dasgupta', 'Partha Talukdar')",10,AD3: Attentive Deep Document Dater,EMNLP,2018
"As an ubiquitous method in natural language processing, word embeddings are extensively employed to map semantic properties of words into a dense vector representation. They capture semantic and syntactic relations among words but the vector corresponding to the words are only meaningful relative to each other. Neither the vector nor its dimensions have any absolute, interpretable meaning. We introduce an additive modification to the objective function of the embedding learning algorithm that encourages the embedding vectors of words that are semantically related a predefined concept to take larger values along a specified dimension, while leaving the original semantic learning mechanism mostly unaffected. In other words, we align words that are already determined to be related, along predefined concepts. Therefore, we impart interpretability to the word embedding by assigning meaning to its vector dimensions. The predefined concepts are derived from an external lexical resource, which in this paper is chosen as Roget's Thesaurus. We observe that alignment along the chosen concepts is not limited to words in the Thesaurus and extends to other related words as well. We quantify the extent of interpretability and assignment of meaning from our experimental results. We also demonstrate the preservation of semantic coherence of the resulting vector space by using word-analogy and word-similarity tests. These tests show that the interpretability-imparted word embeddings that are obtained by the proposed framework do not sacrifice performances in common benchmark tests.",http://aclweb.org/anthology/D18-1214,D18-1,D18-1214,https://arxiv.org/abs/1807.07279,"('David Alvarez-Melis', 'Tommi Jaakkola')",10,Gromov-Wasserstein Alignment of Word Embedding Spaces,EMNLP,2018
"Deep learning has emerged as a versatile tool for a wide range of NLP tasks, due to its superior capacity in representation learning. But its applicability is limited by the reliance on annotated examples, which are difficult to produce at scale. Indirect supervision has emerged as a promising direction to address this bottleneck, either by introducing labeling functions to automatically generate noisy examples from unlabeled text, or by imposing constraints over interdependent label decisions. A plethora of methods have been proposed, each with respective strengths and limitations. Probabilistic logic offers a unifying language to represent indirect supervision, but end-to-end modeling with probabilistic logic is often infeasible due to intractable inference and learning. In this paper, we propose deep probabilistic logic (DPL) as a general framework for indirect supervision, by composing probabilistic logic with deep learning. DPL models label decisions as latent variables, represents prior knowledge on their relations using weighted first-order logical formulas, and alternates between learning a deep neural network for the end task and refining uncertain formula weights for indirect supervision, using variational EM. This framework subsumes prior indirect supervision methods as special cases, and enables novel combination via infusion of rich domain and linguistic knowledge. Experiments on biomedical machine reading demonstrate the promise of this approach.",http://aclweb.org/anthology/D18-1215,D18-1,D18-1215,https://arxiv.org/abs/1808.08485,"('Hai Wang', 'Hoifung Poon')",10,Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision,EMNLP,2018
"Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, attention can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this mapping is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and attention. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15% average error reduction on benchmark datasets.",http://aclweb.org/anthology/D18-1216,D18-1,D18-1216,https://arxiv.org/abs/1808.09367,"('Yujia Bao', 'Shiyu Chang', 'Mo Yu', 'Regina Barzilay')",10,Deriving Machine Attention from Human Rationales,EMNLP,2018
"We simplify sentences with an attentive neural network sequence to sequence model, dubbed S4. The model includes a novel word-copy mechanism and loss function to exploit linguistic similarities between the original and simplified sentences. It also jointly uses pre-trained and fine-tuned word embeddings to capture the semantics of complex sentences and to mitigate the effects of limited data. When trained and evaluated on pairs of sentences from thousands of news articles, we observe a 8.8 point improvement in BLEU score over a sequence to sequence baseline; however, learning word substitutions remains difficult. Such sequence to sequence models are promising for other text generation tasks such as style transfer.",http://aclweb.org/anthology/D18-1217,D18-1,D18-1217,https://arxiv.org/abs/1805.05557,"('Kevin Clark', 'Minh-Thang Luong', 'Christopher D. Manning', 'Quoc Le')",10,Semi-Supervised Sequence Modeling with Cross-View Training,EMNLP,2018
"A popular approach for large scale data annotation tasks is crowdsourcing, wherein each data point is labeled by multiple noisy annotators. We consider the problem of inferring ground truth from noisy ordinal labels obtained from multiple annotators of varying and unknown expertise levels. Annotation models for ordinal data have been proposed mostly as extensions of their binary/categorical counterparts and have received little attention in the crowdsourcing literature. We propose a new model for crowdsourced ordinal data that accounts for instance difficulty as well as annotator expertise, and derive a variational Bayesian inference algorithm for parameter estimation. We analyze the ordinal extensions of several state-of-the-art annotator models for binary/categorical labels and evaluate the performance of all the models on two real world datasets containing ordinal query-URL relevance scores, collected through Amazon's Mechanical Turk. Our results indicate that the proposed model performs better or as well as existing state-of-the-art methods and is more resistant to `spammy' annotators (i.e., annotators who assign labels randomly without actually looking at the instance) than popular baselines such as mean, median, and majority vote which do not account for annotator expertise.",http://aclweb.org/anthology/D18-1218,D18-1,D18-1218,https://arxiv.org/abs/1305.0015,"('Silviu Paun', 'Jon Chamberlain', 'Udo Kruschwitz', 'Juntao Yu', 'Massimo Poesio')",10,A Probabilistic Annotation Model for Crowdsourcing Coreference,EMNLP,2018
"Previous work on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b) use syntactic preposition patterns to calculate word relatedness. However, such patterns only consider NPs' head nouns and hence do not fully capture the semantics of NPs. Recently, Hou (2018) created word embeddings (embeddings_PP) to capture associative similarity (ie, relatedness) between nouns by exploring the syntactic structure of noun phrases. But embeddings_PP only contains word representations for nouns. In this paper, we create new word vectors by combining embeddings_PP with GloVe. This new word embeddings (embeddings_bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily. We therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an NP based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best system in Hou et al.(2013b) which explores Markov Logic Networks to model the problem. Additionally, we further improve the results for bridging anaphora resolution reported in Hou (2018) by combining our simple deterministic approach with Hou et al.(2013b)'s best system MLN II.",http://aclweb.org/anthology/D18-1219,D18-1,D18-1219,https://arxiv.org/abs/1811.05721,['Yufang Hou'],10,A Deterministic Algorithm for Bridging Anaphora Resolution,EMNLP,2018
"We introduce an automatic system that achieves state-of-the-art results on the Winograd Schema Challenge (WSC), a common sense reasoning task that requires diverse, complex forms of inference and knowledge. Our method uses a knowledge hunting module to gather text from the web, which serves as evidence for candidate problem resolutions. Given an input problem, our system generates relevant queries to send to a search engine, then extracts and classifies knowledge from the returned results and weighs them to make a resolution. Our approach improves F1 performance on the full WSC by 0.21 over the previous best and represents the first system to exceed 0.5 F1. We further demonstrate that the approach is competitive on the Choice of Plausible Alternatives (COPA) task, which suggests that it is generally applicable.",http://aclweb.org/anthology/D18-1220,D18-1,D18-1220,https://arxiv.org/abs/1810.01375,"('Ali Emami', 'Noelia De La Cruz', 'Adam Trischler', 'Kaheer Suleman', 'Jackie Chi Kit Cheung')",10,A Knowledge Hunting Framework for Common Sense Reasoning,EMNLP,2018
"This paper addresses the problem of mapping natural language text to knowledge base entities. The mapping process is approached as a composition of a phrase or a sentence into a point in a multi-dimensional entity space obtained from a knowledge graph. The compositional model is an LSTM equipped with a dynamic disambiguation mechanism on the input word embeddings (a Multi-Sense LSTM), addressing polysemy issues. Further, the knowledge base space is prepared by collecting random walks from a graph enhanced with textual features, which act as a set of semantic bridges between text and knowledge base entities. The ideas of this work are demonstrated on large-scale text-to-entity mapping and entity classification tasks, with state of the art results.",http://aclweb.org/anthology/D18-1221,D18-1,D18-1221,https://arxiv.org/abs/1808.07724,"('Dimitri Kartsaklis', 'Mohammad Taher Pilehvar', 'Nigel Collier')",10,Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs,EMNLP,2018
"Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. Most conventional knowledge embedding methods encode both entities (concepts and instances) and relations as vectors in a low dimensional semantic space equally, ignoring the difference between concepts and instances. In this paper, we propose a novel knowledge graph embedding model named TransC by differentiating concepts and instances. Specifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. We use the relative positions to model the relations between concepts and instances (i.e., instanceOf), and the relations between concepts and sub-concepts (i.e., subClassOf). We evaluate our model on both link prediction and triple classification tasks on the dataset based on YAGO. Experimental results show that TransC outperforms state-of-the-art methods, and captures the semantic transitivity for instanceOf and subClassOf relation. Our codes and datasets can be obtained from https:// github.com/davidlvxin/TransC.",http://aclweb.org/anthology/D18-1222,D18-1,D18-1222,https://arxiv.org/abs/1811.04588,"('Xin Lv', 'Lei Hou', 'Juanzi Li', 'Zhiyuan Liu')",10,Differentiating Concepts and Instances for Knowledge Graph Embedding,EMNLP,2018
"Joint representation learning of text and knowledge within a unified semantic space enables us to perform knowledge graph completion more accurately. In this work, we propose a novel framework to embed words, entities and relations into the same continuous vector space. In this model, both entity and relation embeddings are learned by taking knowledge graph and plain text into consideration. In experiments, we evaluate the joint learning model on three tasks including entity prediction, relation prediction and relation classification from text. The experiment results show that our model can significantly and consistently improve the performance on the three tasks as compared with other baselines.",http://aclweb.org/anthology/D18-1223,D18-1,D18-1223,https://arxiv.org/abs/1611.04125,"('Wenhan Xiong', 'Mo Yu', 'Shiyu Chang', 'Xiaoxiao Guo', 'William Yang Wang')",10,One-Shot Relational Learning for Knowledge Graphs,EMNLP,2018
"We focus on two research issues in entity search: scoring a document or snippet that potentially supports a candidate entity, and aggregating scores from different snippets into an entity score. Proximity scoring has been studied in IR outside the scope of entity search. However, aggregation has been hardwired except in a few cases where probabilistic language models are used. We instead explore simple, robust, discriminative ranking algorithms, with informative snippet features and broad families of aggregation functions. Our first contribution is a study of proximity-cognizant snippet features. In contrast with prior work which uses hardwired ""proximity kernels"" that implement a fixed decay with distance, we present a ""universal"" feature encoding which jointly expresses the perplexity (informativeness) of a query term match and the proximity of the match to the entity mention. Our second contribution is a study of aggregation functions. Rather than train the ranking algorithm on snippets and then aggregate scores, we directly train on entities such that the ranking algorithm takes into account the aggregation function being used. Our third contribution is an extensive Web-scale evaluation of the above algorithms on two data sets having quite different properties and behavior. The first one is the W3C dataset used in TREC-scale enterprise search, with pre-annotated entity mentions. The second is a Web-scale open-domain entity search dataset consisting of 500 million Web pages, which contain about 8 billion token spans annotated automatically with two million entities from 200,000 entity types in Wikipedia. On the TREC dataset, the performance of our system is comparable to the currently prevalent systems. On the much larger and noisier Web dataset, our system delivers significantly better performance than all other systems, with 8% MAP improvement over the closest competitor.",http://aclweb.org/anthology/D18-1224,D18-1,D18-1224,https://arxiv.org/abs/1303.3164,"('Shanshan Zhang', 'Lihong He', 'Slobodan Vucetic', 'Eduard Dragut')",10,Regular Expression Guided Entity Mention Mining from Noisy Web Data,EMNLP,2018
"We aim for zero-shot localization and classification of human actions in video. Where traditional approaches rely on global attribute or object classification scores for their zero-shot knowledge transfer, our main contribution is a spatial-aware object embedding. To arrive at spatial awareness, we build our embedding on top of freely available actor and object detectors. Relevance of objects is determined in a word embedding space and further enforced with estimated spatial preferences. Besides local object awareness, we also embed global object awareness into our embedding to maximize actor and object interaction. Finally, we exploit the object positions and sizes in the spatial-aware embedding to demonstrate a new spatio-temporal action retrieval scenario with composite queries. Action localization and classification experiments on four contemporary action video datasets support our proposal. Apart from state-of-the-art results in the zero-shot localization and classification settings, our spatial-aware embedding is even competitive with recent supervised action localization alternatives.",http://aclweb.org/anthology/D18-1225,D18-1,D18-1225,https://arxiv.org/abs/1707.09145,"('Shib Sankar Dasgupta', 'Swayambhu Nath Ray', 'Partha Talukdar')",10,HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding,EMNLP,2018
"Neural architecture for named entity recognition has achieved great success in the field of natural language processing. Currently, the dominating architecture consists of a bi-directional recurrent neural network (RNN) as the encoder and a conditional random field (CRF) as the decoder. In this paper, we propose a deformable stacked structure for named entity recognition, in which the connections between two adjacent layers are dynamically established. We evaluate the deformable stacked structure by adapting it to different layers. Our model achieves the state-of-the-art performances on the OntoNotes dataset.",http://aclweb.org/anthology/D18-1226,D18-1,D18-1226,https://arxiv.org/abs/1809.08730,"('Bill Yuchen Lin', 'Wei Lu')",10,Neural Adaptation Layers for Cross-domain Named Entity Recognition,EMNLP,2018
"Hashtags are semantico-syntactic constructs used across various social networking and microblogging platforms to enable users to start a topic specific discussion or classify a post into a desired category. Segmenting and linking the entities present within the hashtags could therefore help in better understanding and extraction of information shared across the social media. However, due to lack of space delimiters in the hashtags (e.g #nsavssnowden), the segmentation of hashtags into constituent entities (""NSA"" and ""Edward Snowden"" in this case) is not a trivial task. Most of the current state-of-the-art social media analytics systems like Sentiment Analysis and Entity Linking tend to either ignore hashtags, or treat them as a single word. In this paper, we present a context aware approach to segment and link entities in the hashtags to a knowledge base (KB) entry, based on the context within the tweet. Our approach segments and links the entities in hashtags such that the coherence between hashtag semantics and the tweet is maximized. To the best of our knowledge, no existing study addresses the issue of linking entities in hashtags for extracting semantic information. We evaluate our method on two different datasets, and demonstrate the effectiveness of our technique in improving the overall entity linking in tweets via additional semantic information provided by segmenting and linking entities in a hashtag.",http://aclweb.org/anthology/D18-1227,D18-1,D18-1227,https://arxiv.org/abs/1501.03210,"('Hongliang Dai', 'Yangqiu Song', 'Liwei Qiu', 'Rijia Liu')",10,Entity Linking within a Social Media Platform: A Case Study on Yelp,EMNLP,2018
"Objective: To build a comprehensive corpus covering syntactic and semantic annotations of Chinese clinical texts with corresponding annotation guidelines and methods as well as to develop tools trained on the annotated corpus, which supplies baselines for research on Chinese texts in the clinical domain.   Materials and methods: An iterative annotation method was proposed to train annotators and to develop annotation guidelines. Then, by using annotation quality assurance measures, a comprehensive corpus was built, containing annotations of part-of-speech (POS) tags, syntactic tags, entities, assertions, and relations. Inter-annotator agreement (IAA) was calculated to evaluate the annotation quality and a Chinese clinical text processing and information extraction system (CCTPIES) was developed based on our annotated corpus.   Results: The syntactic corpus consists of 138 Chinese clinical documents with 47,424 tokens and 2553 full parsing trees, while the semantic corpus includes 992 documents that annotated 39,511 entities with their assertions and 7695 relations. IAA evaluation shows that this comprehensive corpus is of good quality, and the system modules are effective.   Discussion: The annotated corpus makes a considerable contribution to natural language processing (NLP) research into Chinese texts in the clinical domain. However, this corpus has a number of limitations. Some additional types of clinical text should be introduced to improve corpus coverage and active learning methods should be utilized to promote annotation efficiency.   Conclusions: In this study, several annotation guidelines and an annotation method for Chinese clinical texts were proposed, and a comprehensive corpus with its NLP modules were constructed, providing a foundation for further study of applying NLP techniques to Chinese texts in the clinical domain.",http://aclweb.org/anthology/D18-1228,D18-1,D18-1228,https://arxiv.org/abs/1611.02091,"('Pinal Patel', 'Disha Davey', 'Vishal Panchal', 'Parth Pathak')",10,Annotation of a Large Clinical Entity Corpus,EMNLP,2018
"We propose a lightly-supervised approach for information extraction, in particular named entity classification, which combines the benefits of traditional bootstrapping, i.e., use of limited annotations and interpretability of extraction patterns, with the robust learning approaches proposed in representation learning. Our algorithm iteratively learns custom embeddings for both the multi-word entities to be extracted and the patterns that match them from a few example entities per category. We demonstrate that this representation-based approach outperforms three other state-of-the-art bootstrapping approaches on two datasets: CoNLL-2003 and OntoNotes. Additionally, using these embeddings, our approach outputs a globally-interpretable model consisting of a decision list, by ranking patterns based on their proximity to the average entity embedding in a given class. We show that this interpretable model performs close to our complete bootstrapping model, proving that representation learning can be used to produce interpretable models with small loss in performance.",http://aclweb.org/anthology/D18-1229,D18-1,D18-1229,https://arxiv.org/abs/1805.11545,"('Matthew Berger', 'Ajay Nagesh', 'Joshua Levine', 'Mihai Surdeanu', 'Helen Zhang')",10,Visual Supervision in Bootstrapped Information Extraction,EMNLP,2018
This paper describes an approach for automatic construction of dictionaries for Named Entity Recognition (NER) using large amounts of unlabeled data and a few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower dimensional embeddings (representations) for candidate phrases and classify these phrases using a small number of labeled examples. Our method achieves 16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER respectively. We also show that by adding candidate phrase embeddings as features in a sequence tagger gives better performance compared to using word embeddings.,http://aclweb.org/anthology/D18-1230,D18-1,D18-1230,https://arxiv.org/abs/1504.06650,"('Jingbo Shang', 'Liyuan Liu', 'Xiaotao Gu', 'Xiang Ren', 'Teng Ren', 'Jiawei Han')",10,Learning Named Entity Tagger using Domain-Specific Dictionary,EMNLP,2018
"While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type. Unfortunately, many knowledge graph entities lack such textual descriptions. In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words. We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines.",http://aclweb.org/anthology/D18-1231,D18-1,D18-1231,https://arxiv.org/abs/1805.10564,"('Ben Zhou', 'Daniel Khashabi', 'Chen-Tse Tsai', 'Dan Roth')",10,Zero-Shot Open Entity Typing as Type-Compatible Grounding,EMNLP,2018
"Despite that current reading comprehension systems have achieved significant advancements, their promising performances are often obtained at the cost of making an ensemble of numerous models. Besides, existing approaches are also vulnerable to adversarial attacks. This paper tackles these problems by leveraging knowledge distillation, which aims to transfer knowledge from an ensemble model to a single model. We first demonstrate that vanilla knowledge distillation applied to answer span prediction is effective for reading comprehension systems. We then propose two novel approaches that not only penalize the prediction on confusing answers but also guide the training with alignment information distilled from the ensemble. Experiments show that our best student model has only a slight drop of 0.4% F1 on the SQuAD test set compared to the ensemble teacher, while running 12x faster during inference. It even outperforms the teacher on adversarial SQuAD datasets and NarrativeQA benchmark.",http://aclweb.org/anthology/D18-1232,D18-1,D18-1232,https://arxiv.org/abs/1808.07644,"('Minghao Hu', 'Yuxing Peng', 'Furu Wei', 'Zhen Huang', 'Dongsheng Li', 'Nan Yang', 'Ming Zhou')",10,Attention-Guided Answer Distillation for Machine Reading Comprehension,EMNLP,2018
"Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader's background knowledge. One example is the task of interpreting regulations to answer ""Can I...?"" or ""Do I have to...?"" questions such as ""I am working in Canada. Do I have to carry on paying UK National Insurance?"" after reading a UK government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as ""How long have you been working abroad?"" when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 32k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.",http://aclweb.org/anthology/D18-1233,D18-1,D18-1233,https://arxiv.org/abs/1809.01494,"('Marzieh Saeidi', 'Max Bartolo', 'Patrick Lewis', 'Sameer Singh', 'Tim Rocktäschel', 'Mike Sheldon', 'Guillaume Bouchard', 'Sebastian Riedel')",10,Interpretation of Natural Language Rules in Conversational Machine Reading,EMNLP,2018
"Visual Question Answering (VQA) has attracted much attention since it offers insight into the relationships between the multi-modal analysis of images and natural language. Most of the current algorithms are incapable of answering open-domain questions that require to perform reasoning beyond the image contents. To address this issue, we propose a novel framework which endows the model capabilities in answering more complex questions by leveraging massive external knowledge with dynamic memory networks. Specifically, the questions along with the corresponding images trigger a process to retrieve the relevant information in external knowledge bases, which are embedded into a continuous vector space by preserving the entity-relation structures. Afterwards, we employ dynamic memory networks to attend to the large body of facts in the knowledge graph and images, and then perform reasoning over these facts to generate corresponding answers. Extensive experiments demonstrate that our model not only achieves the state-of-the-art performance in the visual question answering task, but can also answer open-domain questions effectively by leveraging the external knowledge.",http://aclweb.org/anthology/D18-1234,D18-1,D18-1234,https://arxiv.org/abs/1712.00733,"('Sen Hu', 'Lei Zou', 'Xinbo Zhang')",10,A State-transition Framework to Answer Complex Questions over Knowledge Base,EMNLP,2018
"Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading comprehension datasets, in this paper we propose a new Chinese reading comprehension dataset for accelerating related research in the community. The proposed dataset contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set. Along with this dataset, we also hosted the first Evaluation on Chinese Machine Reading Comprehension (CMRC-2017) and successfully attracted tens of participants, which suggest the potential impact of this dataset.",http://aclweb.org/anthology/D18-1235,D18-1,D18-1235,https://arxiv.org/abs/1709.08299,"('Jiahua Liu', 'Wan Wei', 'Maosong Sun', 'Hao Chen', 'Yantao Du', 'Dekang Lin')",10,A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension,EMNLP,2018
"Lindenbaum method is named after the Polish logician Adolf Lindenbaum who prematurely and without a clear trace disappeared in the turmoil of the Second World War at the age of about 37. The method is based on the symbolic nature of formalized languages of deductive systems and opens a gate for applications of algebra to logic and, thereby, to Abstract algebraic logic.",http://aclweb.org/anthology/D18-1236,D18-1,D18-1236,https://arxiv.org/abs/1609.07379,"('Mingming Sun', 'Xu Li', 'Ping Li')",10,Logician and Orator: Learning from the Duality between Language and Knowledge in Open Domain,EMNLP,2018
"Recurrent neural networks (RNNs) such as long short-term memory and gated recurrent units are pivotal building blocks across a broad spectrum of sequence modeling problems. This paper proposes a recurrently controlled recurrent network (RCRN) for expressive and powerful sequence encoding. More concretely, the key idea behind our approach is to learn the recurrent gating functions using recurrent networks. Our architecture is split into two components - a controller cell and a listener cell whereby the recurrent controller actively influences the compositionality of the listener cell. We conduct extensive experiments on a myriad of tasks in the NLP domain such as sentiment analysis (SST, IMDb, Amazon reviews, etc.), question classification (TREC), entailment classification (SNLI, SciTail), answer selection (WikiQA, TrecQA) and reading comprehension (NarrativeQA). Across all 26 datasets, our results demonstrate that RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs, suggesting that our controller architecture might be a suitable replacement for the widely adopted stacked architecture.",http://aclweb.org/anthology/D18-1237,D18-1,D18-1237,https://arxiv.org/abs/1811.09786,"('Seohyun Back', 'Seunghak Yu', 'Sathish Reddy Indurthi', 'Jihie Kim', 'Jaegul Choo')",10,MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller,EMNLP,2018
"We propose MRU (Multi-Range Reasoning Units), a new fast compositional encoder for machine comprehension (MC). Our proposed MRU encoders are characterized by multi-ranged gating, executing a series of parameterized contract-and-expand layers for learning gating vectors that benefit from long and short-term dependencies. The aims of our approach are as follows: (1) learning representations that are concurrently aware of long and short-term context, (2) modeling relationships between intra-document blocks and (3) fast and efficient sequence encoding. We show that our proposed encoder demonstrates promising results both as a standalone encoder and as well as a complementary building block. We conduct extensive experiments on three challenging MC datasets, namely RACE, SearchQA and NarrativeQA, achieving highly competitive performance on all. On the RACE benchmark, our model outperforms DFN (Dynamic Fusion Networks) by 1.5%-6% without using any recurrent or convolution layers. Similarly, we achieve competitive performance relative to AMANDA on the SearchQA benchmark and BiDAF on the NarrativeQA benchmark without using any LSTM/GRU layers. Finally, incorporating MRU encoders with standard BiLSTM architectures further improves performance, achieving state-of-the-art results.",http://aclweb.org/anthology/D18-1238,D18-1,D18-1238,https://arxiv.org/abs/1803.09074,"('Yi Tay', 'Anh Tuan Luu', 'Siu Cheung Hui')",10,Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,EMNLP,2018
"Answering compositional questions requiring multi-step reasoning is challenging. We introduce an end-to-end differentiable model for interpreting questions about a knowledge graph (KG), which is inspired by formal approaches to semantics. Each span of text is represented by a denotation in a KG and a vector that captures ungrounded aspects of meaning. Learned composition modules recursively combine constituent spans, culminating in a grounding for the complete sentence which answers the question. For example, to interpret ""not green"", the model represents ""green"" as a set of KG entities and ""not"" as a trainable ungrounded vector---and then uses this vector to parameterize a composition function that performs a complement operation. For each sentence, we build a parse chart subsuming all possible parses, allowing the model to jointly learn both the composition operators and output structure by gradient descent from end-task supervision. The model learns a variety of challenging semantic operators, such as quantifiers, disjunctions and composed relations, and infers latent syntactic structure. It also generalizes well to longer questions than seen in its training data, in contrast to RNN, its tree-based variants, and semantic parsing baselines.",http://aclweb.org/anthology/D18-1239,D18-1,D18-1239,https://arxiv.org/abs/1808.09942,"('Nitish Gupta', 'Mike Lewis')",10,Neural Compositional Denotational Semantics for Question Answering,EMNLP,2018
"Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences. In this paper, we consider an approach of using a small meta network to learn context-sensitive convolutional filters for text processing. The role of meta network is to abstract the contextual information of a sentence or document into a set of input-aware filters. We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations. In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-sensitive filters, consistently outperforms the standard CNN and attention-based CNN baselines. By visualizing the learned context-sensitive filters, we further validate and rationalize the effectiveness of proposed framework.",http://aclweb.org/anthology/D18-1240,D18-1,D18-1240,https://arxiv.org/abs/1709.08294,"('Kateryna Tymoshenko', 'Alessandro Moschitti')",10,Cross-Pair Text Representations for Answer Sentence Selection,EMNLP,2018
"With the rapid development of knowledge base,question answering based on knowledge base has been a hot research issue. In this paper, we focus on answering singlerelation factoid questions based on knowledge base. We build a question answering system and study the effect of context information on fact selection, such as entity's notable type,outdegree. Experimental results show that context information can improve the result of simple question answering.",http://aclweb.org/anthology/D18-1241,D18-1,D18-1241,https://arxiv.org/abs/1810.04000,"('Eunsol Choi', 'He He', 'Mohit Iyyer', 'Mark Yatskar', 'Wen-tau Yih', 'Yejin Choi', 'Percy Liang', 'Luke Zettlemoyer')",10,QuAC: Question Answering in Context,EMNLP,2018
"We witness an unprecedented proliferation of knowledge graphs that record millions of entities and their relationships. While knowledge graphs are structure-flexible and content rich, they are difficult to use. The challenge lies in the gap between their overwhelming complexity and the limited database knowledge of non-professional users. If writing structured queries over simple tables is difficult, complex graphs are only harder to query. As an initial step toward improving the usability of knowledge graphs, we propose to query such data by example entity tuples, without requiring users to form complex graph queries. Our system, GQBE (Graph Query By Example), automatically derives a weighted hidden maximal query graph based on input query tuples, to capture a user's query intent. It efficiently finds and ranks the top approximate answer tuples. For fast query processing, GQBE only partially evaluates query graphs. We conducted experiments and user studies on the large Freebase and DBpedia datasets and observed appealing accuracy and efficiency. Our system provides a complementary approach to the existing keyword-based methods, facilitating user-friendly graph querying. To the best of our knowledge, there was no such proposal in the past in the context of graphs.",http://aclweb.org/anthology/D18-1242,D18-1,D18-1242,https://arxiv.org/abs/1311.2100,"('Kangqi Luo', 'Fengli Lin', 'Xusheng Luo', 'Kenny Zhu')",10,Knowledge Base Question Answering via Encoding of Complex Query Graphs,EMNLP,2018
"Extracting relations is critical for knowledge base completion and construction in which distant supervised methods are widely used to extract relational facts automatically with the existing knowledge bases. However, the automatically constructed datasets comprise amounts of low-quality sentences containing noisy words, which is neglected by current distant supervised methods resulting in unacceptable precisions. To mitigate this problem, we propose a novel word-level distant supervised approach for relation extraction. We first build Sub-Tree Parse(STP) to remove noisy words that are irrelevant to relations. Then we construct a neural network inputting the sub-tree while applying the entity-wise attention to identify the important semantic features of relational words in each instance. To make our model more robust against noisy words, we initialize our network with a priori knowledge learned from the relevant task of entity classification by transfer learning. We conduct extensive experiments using the corpora of New York Times(NYT) and Freebase. Experiments show that our approach is effective and improves the area of Precision/Recall(PR) from 0.35 to 0.39 over the state-of-the-art work.",http://aclweb.org/anthology/D18-1243,D18-1,D18-1243,https://arxiv.org/abs/1808.06738,"('Tianyi Liu', 'Xinsong Zhang', 'Wanhao Zhou', 'Weijia Jia')",10,Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning,EMNLP,2018
"Dependency trees help relation extraction models capture long-range relations between words. However, existing dependency-based models either neglect crucial information (e.g., negation) by pruning the dependency trees too aggressively, or are computationally inefficient because it is difficult to parallelize over different tree structures. We propose an extension of graph convolutional networks that is tailored for relation extraction, which pools information over arbitrary dependency structures efficiently in parallel. To incorporate relevant information while maximally removing irrelevant content, we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold. The resulting model achieves state-of-the-art performance on the large-scale TACRED dataset, outperforming existing sequence and dependency-based neural models. We also show through detailed analysis that this model has complementary strengths to sequence models, and combining them further improves the state of the art.",http://aclweb.org/anthology/D18-1244,D18-1,D18-1244,https://arxiv.org/abs/1809.10185,"('Yuhao Zhang', 'Peng Qi', 'Christopher D. Manning')",10,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,EMNLP,2018
"We propose a framework to improve performance of distantly-supervised relation extraction, by jointly learning to solve two related tasks: concept-instance extraction and relation extraction. We combine this with a novel use of document structure: in some small, well-structured corpora, sections can be identified that correspond to relation arguments, and distantly-labeled examples from such sections tend to have good precision. Using these as seeds we extract additional relation examples by applying label propagation on a graph composed of noisy examples extracted from a large unstructured testing corpus. Combined with the soft constraint that concept examples should have the same type as the second argument of the relation, we get significant improvements over several state-of-the-art approaches to distantly-supervised relation extraction.",http://aclweb.org/anthology/D18-1245,D18-1,D18-1245,https://arxiv.org/abs/1606.03398,"('Jinhua Du', 'Jingguang Han', 'Andy Way', 'Dadong Wan')",10,Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction,EMNLP,2018
"We propose in this paper a combined model of Long Short Term Memory and Convolutional Neural Networks (LSTM-CNN) that exploits word embeddings and positional embeddings for cross-sentence n-ary relation extraction. The proposed model brings together the properties of both LSTMs and CNNs, to simultaneously exploit long-range sequential information and capture most informative features, essential for cross-sentence n-ary relation extraction. The LSTM-CNN model is evaluated on standard dataset on cross-sentence n-ary relation extraction, where it significantly outperforms baselines such as CNNs, LSTMs and also a combined CNN-LSTM model. The paper also shows that the LSTM-CNN model outperforms the current state-of-the-art methods on cross-sentence n-ary relation extraction.",http://aclweb.org/anthology/D18-1246,D18-1,D18-1246,https://arxiv.org/abs/1811.00845,"('Linfeng Song', 'Yue Zhang', 'Zhiguo Wang', 'Daniel Gildea')",10,N-ary Relation Extraction using Graph-State LSTM,EMNLP,2018
"Time series prediction has been studied in a variety of domains. However, it is still challenging to predict future series given historical observations and past exogenous data. Existing methods either fail to consider the interactions among different components of exogenous variables which may affect the prediction accuracy, or cannot model the correlations between exogenous data and target data. Besides, the inherent temporal dynamics of exogenous data are also related to the target series prediction, and thus should be considered as well. To address these issues, we propose an end-to-end deep learning model, i.e., Hierarchical attention-based Recurrent Highway Network (HRHN), which incorporates spatio-temporal feature extraction of exogenous variables and temporal dynamics modeling of target variables into a single framework. Moreover, by introducing the hierarchical attention mechanism, HRHN can adaptively select the relevant exogenous features in different semantic levels. We carry out comprehensive empirical evaluations with various methods over several datasets, and show that HRHN outperforms the state of the arts in time series prediction, especially in capturing sudden changes and sudden oscillations of time series.",http://aclweb.org/anthology/D18-1247,D18-1,D18-1247,https://arxiv.org/abs/1806.00685,"('Xu Han', 'Pengfei Yu', 'Zhiyuan Liu', 'Maosong Sun', 'Peng Li')",10,Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention,EMNLP,2018
"The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross- sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.",http://aclweb.org/anthology/D18-1248,D18-1,D18-1248,https://arxiv.org/abs/1609.04873,"('Guanying Wang', 'Wen Zhang', 'Ruoxu Wang', 'Yalin Zhou', 'Xi Chen', 'Wei Zhang', 'Hai Zhu', 'Huajun Chen')",10,Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding,EMNLP,2018
"The task of end-to-end relation extraction consists of two sub-tasks: i) identifying entity mentions along with their types and ii) recognizing semantic relations among the entity mention pairs. %Identifying entity mentions along with their types and recognizing semantic relations among the entity mentions, are two very important problems in Information Extraction. It has been shown that for better performance, it is necessary to address these two sub-tasks jointly. We propose an approach for simultaneous extraction of entity mentions and relations in a sentence, by using inference in Markov Logic Networks (MLN). We learn three different classifiers : i) local entity classifier, ii) local relation classifier and iii) ""pipeline"" relation classifier which uses predictions of the local entity classifier. Predictions of these classifiers may be inconsistent with each other. We represent these predictions along with some domain knowledge using weighted first-order logic rules in an MLN and perform joint inference over the MLN to obtain a global output with minimum inconsistencies. Experiments on the ACE (Automatic Content Extraction) 2004 dataset demonstrate that our approach of joint extraction using MLNs outperforms the baselines of individual classifiers. Our end-to-end relation extraction performance is better than 2 out of 3 previous results reported on the ACE 2004 dataset.",http://aclweb.org/anthology/D18-1249,D18-1,D18-1249,https://arxiv.org/abs/1712.00988,"('Changzhi Sun', 'Yuanbin Wu', 'Man Lan', 'Shiliang Sun', 'Wenting Wang', 'Kuang-Chih Lee', 'Kewen Wu')",10,Extracting Entities and Relations with Joint Minimum Risk Training,EMNLP,2018
"Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolutional neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve an F1-score of 86.1%, outperforming previous state-of-the-art recorded results.",http://aclweb.org/anthology/D18-1250,D18-1,D18-1250,https://arxiv.org/abs/1601.03651,"('Hoang-Quynh Le', 'Duy-Cat Can', 'Sinh T. Vu', 'Thanh Hai Dang', 'Mohammad Taher Pilehvar', 'Nigel Collier')",10,Large-scale Exploration of Neural Relation Classification Architectures,EMNLP,2018
"A significant number of oil paintings produced by Georgia O'Keeffe (1887-1986) show surface protrusions of varying width, up to several hundreds of microns. These protrusions are similar to those described in the art conservation literature as metallic soaps. Since the presence of these protrusions raises questions about the state of conservation and long-term prospects for deterioration of these artworks, a 3D-imaging technique, photometric stereo using ultraviolet illumination, was developed for the long-term monitoring of the surface-shape of the protrusions and the surrounding paint. Because the UV fluorescence response of painting materials is isotropic, errors typically caused by non-Lambertian (anisotropic) specularities when using visible reflected light can be avoided providing a more accurate estimation of shape. As an added benefit, fluorescence provides additional contrast information contributing to materials characterization. The developed methodology aims to detect, characterize, and quantify the distribution of micro-protrusions and their development over the surface of entire artworks. Combined with a set of analytical in-situ techniques, and computational tools, this approach constitutes a novel methodology to investigate the selective distribution of protrusions in correlation with the composition of painting materials at the macro-scale. While focused on O'Keeffe's paintings as a case study, we expect the proposed approach to have broader significance by providing a non-invasive protocol to the conservation community to probe topological changes for any relatively flat painted surface of an artwork, and more specifically to monitor the dynamic formation of protrusions, in relation to paint composition and modifications of environmental conditions, loans, exhibitions and storage over the long-term.",http://aclweb.org/anthology/D18-1251,D18-1,D18-1251,https://arxiv.org/abs/1711.08103,"('Dhivya Chinnappa', 'Eduardo Blanco')",10,Possessors Change Over Time: A Case Study with Artworks,EMNLP,2018
"This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.",http://aclweb.org/anthology/D18-1252,D18-1,D18-1252,https://arxiv.org/abs/1603.09631,"('Todd Shore', 'Gabriel Skantze')",10,Using Lexical Alignment and Referring Ability to Address Data Sparsity in Situated Dialog Reference Resolution,EMNLP,2018
"Developing agents to engage in complex goal-oriented dialogues is challenging partly because the main learning signals are very sparse in long conversations. In this paper, we propose a divide-and-conquer approach that discovers and exploits the hidden structure of the task to enable efficient policy learning. First, given successful example dialogues, we propose the Subgoal Discovery Network (SDN) to divide a complex goal-oriented task into a set of simpler subgoals in an unsupervised fashion. We then use these subgoals to learn a multi-level policy by hierarchical reinforcement learning. We demonstrate our method by building a dialogue agent for the composite task of travel planning. Experiments with simulated and real users show that our approach performs competitively against a state-of-the-art method that requires human-defined subgoals. Moreover, we show that the learned subgoals are often human comprehensible.",http://aclweb.org/anthology/D18-1253,D18-1,D18-1253,https://arxiv.org/abs/1804.07855,"('Da Tang', 'Xiujun Li', 'Jianfeng Gao', 'Chong Wang', 'Lihong Li', 'Tony Jebara')",10,Subgoal Discovery for Hierarchical Dialogue Policy Learning,EMNLP,2018
"There are several dialog frameworks which allow manual specification of intents and rule based dialog flow. The rule based framework provides good control to dialog designers at the expense of being more time consuming and laborious. The job of a dialog designer can be reduced if we could identify pairs of user intents and corresponding responses automatically from prior conversations between users and agents. In this paper we propose an approach to find these frequent user utterances (which serve as examples for intents) and corresponding agent responses. We propose a novel SimCluster algorithm that extends standard K-means algorithm to simultaneously cluster user utterances and agent utterances by taking their adjacency information into account. The method also aligns these clusters to provide pairs of intents and response groups. We compare our results with those produced by using simple Kmeans clustering on a real dataset and observe upto 10% absolute improvement in F1-scores. Through our experiments on synthetic dataset, we show that our algorithm gains more advantage over K-means algorithm when the data has large variance.",http://aclweb.org/anthology/D18-1254,D18-1,D18-1254,https://arxiv.org/abs/1710.10609,"('Iryna Haponchyk', 'Antonio Uva', 'Seunghak Yu', 'Olga Uryupina', 'Alessandro Moschitti')",10,Supervised Clustering of Questions into Intents for Dialog System Applications,EMNLP,2018
"Existing dialog datasets contain a sequence of utterances and responses without any explicit background knowledge associated with them. This has resulted in the development of models which treat conversation as a sequence-to-sequence generation task i.e, given a sequence of utterances generate the response sequence). This is not only an overly simplistic view of conversation but it is also emphatically different from the way humans converse by heavily relying on their background knowledge about the topic (as opposed to simply relying on the previous sequence of utterances). For example, it is common for humans to (involuntarily) produce utterances which are copied or suitably modified from background articles they have read about the topic. To facilitate the development of such natural conversation models which mimic the human process of conversing, we create a new dataset containing movie chats wherein each response is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie. We establish baseline results on this dataset (90K utterances from 9K conversations) using three different models: (i) pure generation based models which ignore the background knowledge (ii) generation based models which learn to copy information from the background knowledge when required and (iii) span prediction based models which predict the appropriate response span in the background knowledge.",http://aclweb.org/anthology/D18-1255,D18-1,D18-1255,https://arxiv.org/abs/1809.08205,"('Nikita Moghe', 'Siddhartha Arora', 'Suman Banerjee', 'Mitesh M. Khapra')",10,Towards Exploiting Background Knowledge for Building Conversation Systems,EMNLP,2018
"We consider negotiation settings in which two agents use natural language to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing \$50) and the execution of that strategy (e.g., generating ""The bike is brand new. Selling for just \$50.""). Recent work on negotiation trains neural models, but their end-to-end nature makes it hard to control their strategy, and reinforcement learning tends to lead to degenerate solutions. In this paper, we propose a modular approach based on coarse di- alogue acts (e.g., propose(price=50)) that decouples strategy and generation. We show that we can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on Craigslist. Human evaluation shows that our systems achieve higher task success rate and more human-like negotiation behavior than previous approaches.",http://aclweb.org/anthology/D18-1256,D18-1,D18-1256,https://arxiv.org/abs/1808.09637,"('He He', 'Derek Chen', 'Anusha Balakrishnan', 'Percy Liang')",10,Decoupling Strategy and Generation in Negotiation Dialogues,EMNLP,2018
"Cloze tests are widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-created cloze test dataset CLOTH, containing questions used in middle-school and high-school language exams. With missing blanks carefully created by teachers and candidate choices purposely designed to be nuanced, CLOTH requires a deeper language understanding and a wider attention span than previously automatically-generated cloze datasets. We test the performance of dedicatedly designed baseline models including a language model trained on the One Billion Word Corpus and show humans outperform them by a significant margin. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending the long-term context to be the key bottleneck.",http://aclweb.org/anthology/D18-1257,D18-1,D18-1257,https://arxiv.org/abs/1711.03225,"('Qizhe Xie', 'Guokun Lai', 'Zihang Dai', 'Eduard Hovy')",10,Large-scale Cloze Test Dataset Created by Teachers,EMNLP,2018
"We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.",http://aclweb.org/anthology/D18-1258,D18-1,D18-1258,https://arxiv.org/abs/1809.00732,"('Anusri Pampari', 'Preethi Raghavan', 'Jennifer Liang', 'Jian Peng')",10,emrQA: A Large Corpus for Question Answering on Electronic Medical Records,EMNLP,2018
"Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",http://aclweb.org/anthology/D18-1259,D18-1,D18-1259,https://arxiv.org/abs/1809.09600,"('Zhilin Yang', 'Peng Qi', 'Saizheng Zhang', 'Yoshua Bengio', 'William Cohen', 'Ruslan Salakhutdinov', 'Christopher D. Manning')",10,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",EMNLP,2018
"We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1329 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic---in the context of common knowledge---and the language it is expressed in. Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.",http://aclweb.org/anthology/D18-1260,D18-1,D18-1260,https://arxiv.org/abs/1809.02789,"('Todor Mihaylov', 'Peter Clark', 'Tushar Khot', 'Ashish Sabharwal')",10,Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering,EMNLP,2018
"We propose a new dataset for evaluating question answering models with respect to their capacity to reason about beliefs. Our tasks are inspired by theory-of-mind experiments that examine whether children are able to reason about the beliefs of others, in particular when those beliefs differ from reality. We evaluate a number of recent neural models with memory augmentation. We find that all fail on our tasks, which require keeping track of inconsistent states of the world; moreover, the models' accuracy decreases notably when random sentences are introduced to the tasks at test.",http://aclweb.org/anthology/D18-1261,D18-1,D18-1261,https://arxiv.org/abs/1808.09352,"('Aida Nematzadeh', 'Kaylee Burns', 'Erin Grant', 'Alison Gopnik', 'Tom Griffiths')",10,Evaluating Theory of Mind in Question Answering,EMNLP,2018
"Label embedding plays an important role in zero-shot learning. Side information such as attributes, semantic text representations, and label hierarchy are commonly used as the label embedding in zero-shot classification tasks. However, the label embedding used in former works considers either only one single context of the label, or multiple contexts without dependency. Therefore, different contexts of the label may not be well aligned in the embedding space to preserve the relatedness between labels, which will result in poor interpretability of the label embedding. In this paper, we propose a Multi-Context Label Embedding (MCLE) approach to incorporate multiple label contexts, e.g., label hierarchy and attributes, within a unified matrix factorization framework. To be specific, we model each single context by a matrix factorization formula and introduce a shared variable to capture the dependency among different contexts. Furthermore, we enforce sparsity constraint on our multi-context framework to strengthen the interpretability of the learned label embedding. Extensive experiments on two real-world datasets demonstrate the superiority of our MCLE in label description and zero-shot image classification.",http://aclweb.org/anthology/D18-1262,D18-1,D18-1262,https://arxiv.org/abs/1805.01199,"('Zuchao Li', 'Shexia He', 'Jiaxun Cai', 'Zhuosheng Zhang', 'Hai Zhao', 'Gongshen Liu', 'Linlin Li', 'Luo Si')",10,A Unified Syntax-aware Framework for Semantic Role Labeling,EMNLP,2018
"Using paraphrases, the expression of the same semantic meaning in different words, to improve generalization and translation performance is often useful. However, prior works only explore the use of paraphrases at the word or phrase level, not at the sentence or document level. Unlike previous works, we use different translations of the whole training data that are consistent in structure as paraphrases at the corpus level. Our corpus contains parallel paraphrases in multiple languages from various sources. We treat paraphrases as foreign languages, tag source sentences with paraphrase labels, and train in the style of multilingual Neural Machine Translation (NMT). Experimental results indicate that adding paraphrases improves the rare word translation, increases entropy and diversity in lexical choice. Moreover, adding the source paraphrases improves translation performance more effectively than adding the target paraphrases. Combining both the source and the target paraphrases boosts performance further; combining paraphrases with multilingual data also helps but has mixed performance. We achieve a BLEU score of 57.2 for French-to-English translation, training on 24 paraphrases of the Bible, which is ~+27 above the WMT'14 baseline.",http://aclweb.org/anthology/D18-1263,D18-1,D18-1263,https://arxiv.org/abs/1808.08438,"('Gabriel Stanovsky', 'Ido Dagan')",10,Semantics as a Foreign Language,EMNLP,2018
"In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highest-scored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score.",http://aclweb.org/anthology/D18-1264,D18-1,D18-1264,https://arxiv.org/abs/1810.03541,"('Yijia Liu', 'Wanxiang Che', 'Bo Zheng', 'Bing Qin', 'Ting Liu')",10,An AMR Aligner Tuned by Transition-based Parser,EMNLP,2018
"We propose a novel dependency-based hybrid tree model for semantic parsing, which converts natural language utterance into machine interpretable meaning representations. Unlike previous state-of-the-art models, the semantic information is interpreted as the latent dependency between the natural language words in our joint representation. Such dependency information can capture the interactions between the semantics and natural language words. We integrate a neural component into our model and propose an efficient dynamic-programming algorithm to perform tractable inference. Through extensive experiments on the standard multilingual GeoQuery dataset with eight languages, we demonstrate that our proposed approach is able to achieve state-of-the-art performance across several languages. Analysis also justifies the effectiveness of using our new dependency-based representation.",http://aclweb.org/anthology/D18-1265,D18-1,D18-1265,https://arxiv.org/abs/1809.00107,"('Zhanming Jie', 'Wei Lu')",10,Dependency-based Hybrid Trees for Semantic Parsing,EMNLP,2018
"Semantic parsing from denotations faces two key challenges in model training: (1) given only the denotations (e.g., answers), search for good candidate semantic parses, and (2) choose the best model update algorithm. We propose effective and general solutions to each of them. Using policy shaping, we bias the search procedure towards semantic parses that are more compatible to the text, which provide better supervision signals for training. In addition, we propose an update equation that generalizes three different families of learning algorithms, which enables fast model exploration. When experimented on a recently proposed sequential question answering dataset, our framework leads to a new state-of-the-art model that outperforms previous work by 5.0% absolute on exact match accuracy.",http://aclweb.org/anthology/D18-1266,D18-1,D18-1266,https://arxiv.org/abs/1809.01299,"('Dipendra Misra', 'Ming-Wei Chang', 'Xiaodong He', 'Wen-tau Yih')",10,Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations,EMNLP,2018
"Multilinguality is gradually becoming ubiquitous in the sense that more and more researchers have successfully shown that using additional languages help improve the results in many Natural Language Processing tasks. Multilingual Multiway Corpora (MMC) contain the same sentence in multiple languages. Such corpora have been primarily used for Multi-Source and Pivot Language Machine Translation but are also useful for developing multilingual sequence taggers by transfer learning. While these corpora are available, they are not organized for multilingual experiments and researchers need to write boilerplate code every time they want to use said corpora. Moreover, because there is no official MMC collection it becomes difficult to compare against existing approaches. As such we present our work on creating a unified and systematically organized repository of MMC spanning a large number of languages. We also provide training, development and test splits for corpora where official splits are unavailable. We hope that this will help speed up the pace of multilingual NLP research and ensure that NLP researchers obtain results that are more trustable since they can be compared easily. We indicate corpora sources, extraction procedures if any and relevant statistics. We also make our collection public for research purposes.",http://aclweb.org/anthology/D18-1267,D18-1,D18-1267,https://arxiv.org/abs/1710.01025,"('Jonathan Mallinson', 'Rico Sennrich', 'Mirella Lapata')",10,Sentence Compression for Arbitrary Languages via Multilingual Pivoting,EMNLP,2018
"Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision, either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.",http://aclweb.org/anthology/D18-1268,D18-1,D18-1268,https://arxiv.org/abs/1809.03633,"('Ruochen Xu', 'Yiming Yang', 'Naoki Otani', 'Yuexin Wu')",10,Unsupervised Cross-lingual Transfer of Word Embedding Spaces,EMNLP,2018
"Matching natural language sentences is central for many applications such as information retrieval and question answering. Existing deep models rely on a single sentence representation or multiple granularity representations for matching. However, such methods cannot well capture the contextualized local information in the matching process. To tackle this problem, we present a new deep architecture to match two sentences with multiple positional sentence representations. Specifically, each positional sentence representation is a sentence representation at this position, generated by a bidirectional long short term memory (Bi-LSTM). The matching score is finally produced by aggregating interactions between these different positional sentence representations, through $k$-Max pooling and a multi-layer perceptron. Our model has several advantages: (1) By using Bi-LSTM, rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation; (2) By matching with multiple positional sentence representations, it is flexible to aggregate different important contextualized local information in a sentence to support the matching; (3) Experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model.",http://aclweb.org/anthology/D18-1269,D18-1,D18-1269,https://arxiv.org/abs/1511.08277,"('Alexis Conneau', 'Ruty Rinott', 'Guillaume Lample', 'Adina Williams', 'Samuel Bowman', 'Holger Schwenk', 'Veselin Stoyanov')",10,XNLI: Evaluating Cross-lingual Sentence Representations,EMNLP,2018
"Cross-lingual Entity Linking (XEL) aims to ground entity mentions written in any language to an English Knowledge Base (KB), such as Wikipedia. XEL for most languages is challenging, owing to limited availability of resources as supervision. We address this challenge by developing the first XEL approach that combines supervision from multiple languages jointly. This enables our approach to: (a) augment the limited supervision in the target language with additional supervision from a high-resource language (like English), and (b) train a single entity linking model for multiple languages, improving upon individually trained models for each language. Extensive evaluation on three benchmark datasets across 8 languages shows that our approach significantly improves over the current state-of-the-art. We also provide analyses in two limited resource settings: (a) zero-shot setting, when no supervision in the target language is available, and in (b) low-resource setting, when some supervision in the target language is available. Our analysis provides insights into the limitations of zero-shot XEL approaches in realistic scenarios, and shows the value of joint supervision in low-resource settings.",http://aclweb.org/anthology/D18-1270,D18-1,D18-1270,https://arxiv.org/abs/1809.07657,"('Shyam Upadhyay', 'Nitish Gupta', 'Dan Roth')",10,Joint Multilingual Supervision for Cross-lingual Entity Linking,EMNLP,2018
"Aligning coordinated text streams from multiple sources and multiple languages has opened many new research venues on cross-lingual knowledge discovery. In this paper we aim to advance state-of-the-art by: (1). extending coarse-grained topic-level knowledge mining to fine-grained information units such as entities and events; (2). following a novel Data-to-Network-to-Knowledge (D2N2K) paradigm to construct and utilize network structures to capture and propagate reliable evidence. We introduce a novel Burst Information Network (BINet) representation that can display the most important information and illustrate the connections among bursty entities, events and keywords in the corpus. We propose an effective approach to construct and decipher BINets, incorporating novel criteria based on multi-dimensional clues from pronunciation, translation, burst, neighbor and graph topological structure. The experimental results on Chinese and English coordinated text streams show that our approach can accurately decipher the nodes with high confidence in the BINets and that the algorithm can be efficiently run in parallel, which makes it possible to apply it to huge amounts of streaming data for never-ending language and information decipherment.",http://aclweb.org/anthology/D18-1271,D18-1,D18-1271,https://arxiv.org/abs/1609.08237,"('Tao Ge', 'Qing Dou', 'Heng Ji', 'Lei Cui', 'Baobao Chang', 'Zhifang Sui', 'Furu Wei', 'Ming Zhou')",10,Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition,EMNLP,2018
"The article describes a model of automatic interpretation of English puns, based on Roget's Thesaurus, and its implementation, PunFields. In a pun, the algorithm discovers two groups of words that belong to two main semantic fields. The fields become a semantic vector based on which an SVM classifier learns to recognize puns. A rule-based model is then applied for recognition of intentionally ambiguous (target) words and their definitions. In SemEval Task 7 PunFields shows a considerably good result in pun classification, but requires improvement in searching for the target word and its definition.",http://aclweb.org/anthology/D18-1272,D18-1,D18-1272,https://arxiv.org/abs/1707.05479,"('Yufeng Diao', 'Hongfei Lin', 'Di Wu', 'Liang Yang', 'Kan Xu', 'Zhihao Yang', 'Jian Wang', 'Shaowu Zhang', 'Bo Xu', 'Dongyu Zhang')",10,WECA：A WordNet-Encoded Collocation-Attention Network for Homographic Pun Recognition,EMNLP,2018
"Ancient Chinese brings the wisdom and spirit culture of the Chinese nation. Automatically translation from ancient Chinese to modern Chinese helps to inherit and carry forward the quintessence of the ancients. In this paper, we propose an Ancient-Modern Chinese clause alignment approach and apply it to create a large scale Ancient-Modern Chinese parallel corpus which contains about 1.24M bilingual pairs. To our best knowledge, this is the first large high-quality Ancient-Modern Chinese dataset. Furthermore, we train the SMT and various NMT based models on this dataset and provide a strong baseline for this task",http://aclweb.org/anthology/D18-1273,D18-1,D18-1273,https://arxiv.org/abs/1808.03738,"('Dingmin Wang', 'Yan Song', 'Jing Li', 'Jialong Han', 'Haisong Zhang')",10,A Hybrid Approach to Automatic Corpus Generation for Chinese Spelling Check,EMNLP,2018
"We propose a neural encoder-decoder model with reinforcement learning (NRL) for grammatical error correction (GEC). Unlike conventional maximum likelihood estimation (MLE), the model directly optimizes towards an objective that considers a sentence-level, task-specific evaluation metric, avoiding the exposure bias issue in MLE. We demonstrate that NRL outperforms MLE both in human and automated evaluation metrics, achieving the state-of-the-art on a fluency-oriented GEC corpus.",http://aclweb.org/anthology/D18-1274,D18-1,D18-1274,https://arxiv.org/abs/1707.00299,"('Shamil Chollampatt', 'Hwee Tou Ng')",10,Neural Quality Estimation of Grammatical Error Correction,EMNLP,2018
"Use of social media has grown dramatically during the last few years. Users follow informal languages in communicating through social media. The language of communication is often mixed in nature, where people transcribe their regional language with English and this technique is found to be extremely popular. Natural language processing (NLP) aims to infer the information from these text where Part-of-Speech (PoS) tagging plays an important role in getting the prosody of the written text. For the task of PoS tagging on Code-Mixed Indian Social Media Text, we develop a supervised system based on Conditional Random Field classifier. In order to tackle the problem effectively, we have focused on extracting rich linguistic features. We participate in three different language pairs, ie. English-Hindi, English-Bengali and English-Telugu on three different social media platforms, Twitter, Facebook & WhatsApp. The proposed system is able to successfully assign coarse as well as fine-grained PoS tag labels for a given a code-mixed sentence. Experiments show that our system is quite generic that shows encouraging performance levels on all the three language pairs in all the domains.",http://aclweb.org/anthology/D18-1275,D18-1,D18-1275,https://arxiv.org/abs/1702.00167,"('Tao Gui', 'Qi Zhang', 'Jingjing Gong', 'Minlong Peng', 'di liang', 'Keyu Ding', 'Xuanjing Huang')",10,Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging,EMNLP,2018
"The configurational information in sentences of a free word order language such as Sanskrit is of limited use. Thus, the context of the entire sentence will be desirable even for basic processing tasks such as word segmentation. We propose a structured prediction framework that jointly solves the word segmentation and morphological tagging tasks in Sanskrit. We build an energy based model where we adopt approaches generally employed in graph based parsing techniques (McDonald et al., 2005a; Carreras, 2007). Our model outperforms the state of the art with an F-Score of 96.92 (percentage improvement of 7.06%) while using less than one-tenth of the task-specific training data. We find that the use of a graph based ap- proach instead of a traditional lattice-based sequential labelling approach leads to a percentage gain of 12.6% in F-Score for the segmentation task.",http://aclweb.org/anthology/D18-1276,D18-1,D18-1276,https://arxiv.org/abs/1809.01446,"('Amrith Krishna', 'Bishal Santra', 'Sasi Prasanth Bandaru', 'Gaurav Sahu', 'Vishnu Dutt Sharma', 'Pavankumar Satuluri', 'Pawan Goyal')",10,Free as in Free Word Order: An Energy Based Model for Word Segmentation and Morphological Tagging in Sanskrit,EMNLP,2018
"Learning a classifier from ambiguously labeled face images is challenging since training images are not always explicitly-labeled. For instance, face images of two persons in a news photo are not explicitly labeled by their names in the caption. We propose a Matrix Completion for Ambiguity Resolution (MCar) method for predicting the actual labels from ambiguously labeled images. This step is followed by learning a standard supervised classifier from the disambiguated labels to classify new images. To prevent the majority labels from dominating the result of MCar, we generalize MCar to a weighted MCar (WMCar) that handles label imbalance. Since WMCar outputs a soft labeling vector of reduced ambiguity for each instance, we can iteratively refine it by feeding it as the input to WMCar. Nevertheless, such an iterative implementation can be affected by the noisy soft labeling vectors, and thus the performance may degrade. Our proposed Iterative Candidate Elimination (ICE) procedure makes the iterative ambiguity resolution possible by gradually eliminating a portion of least likely candidates in ambiguously labeled face. We further extend MCar to incorporate the labeling constraints between instances when such prior knowledge is available. Compared to existing methods, our approach demonstrates improvement on several ambiguously labeled datasets.",http://aclweb.org/anthology/D18-1277,D18-1,D18-1277,https://arxiv.org/abs/1702.04455,"('Ali Elkahky', 'Kellie Webster', 'Daniel Andor', 'Emily Pitler')",10,A Challenge Set and Methods for Noun-Verb Ambiguity,EMNLP,2018
"When parsing morphologically-rich languages with neural models, it is beneficial to model input at the character level, and it has been claimed that this is because character-level models learn morphology. We test these claims by comparing character-level models to an oracle with access to explicit morphological analysis on twelve languages with varying morphological typologies. Our results highlight many strengths of character-level models, but also show that they are poor at disambiguating some words, particularly in the face of case syncretism. We then demonstrate that explicitly modeling morphological case improves our best model, showing that character-level models can benefit from targeted forms of explicit morphological modeling.",http://aclweb.org/anthology/D18-1278,D18-1,D18-1278,https://arxiv.org/abs/1808.09180,"('Clara Vania', 'Andreas Grivas', 'Adam Lopez')",10,What do character-level models learn about morphology? The case of dependency parsing,EMNLP,2018
"Character-based neural models have recently proven very useful for many NLP tasks. However, there is a gap of sophistication between methods for learning representations of sentences and words. While most character models for learning representations of sentences are deep and complex, models for learning representations of words are shallow and simple. Also, in spite of considerable research on learning character embeddings, it is still not clear which kind of architecture is the best for capturing character-to-word representations. To address these questions, we first investigate the gaps between methods for learning word and sentence representations. We conduct detailed experiments and comparisons of different state-of-the-art convolutional models, and also investigate the advantages and disadvantages of their constituents. Furthermore, we propose IntNet, a funnel-shaped wide convolutional neural architecture with no down-sampling for learning representations of the internal structure of words by composing their characters from limited, supervised training corpora. We evaluate our proposed model on six sequence labeling datasets, including named entity recognition, part-of-speech tagging, and syntactic chunking. Our in-depth analysis shows that IntNet significantly outperforms other character embedding models and obtains new state-of-the-art performance without relying on any external knowledge or resources.",http://aclweb.org/anthology/D18-1279,D18-1,D18-1279,https://arxiv.org/abs/1810.12443,"('Yingwei Xin', 'Ethan Hart', 'Vibhuti Mahajan', 'Jean David Ruvini')",10,Learning Better Internal Structure of Words for Sequence Labeling,EMNLP,2018
"Emotion recognition has become an important field of research in Human Computer Interactions as we improve upon the techniques for modelling the various aspects of behaviour. With the advancement of technology our understanding of emotions are advancing, there is a growing need for automatic emotion recognition systems. One of the directions the research is heading is the use of Neural Networks which are adept at estimating complex functions that depend on a large number and diverse source of input data. In this paper we attempt to exploit this effectiveness of Neural networks to enable us to perform multimodal Emotion recognition on IEMOCAP dataset using data from Speech, Text, and Motion capture data from face expressions, rotation and hand movements. Prior research has concentrated on Emotion detection from Speech on the IEMOCAP dataset, but our approach is the first that uses the multiple modes of data offered by IEMOCAP for a more robust and accurate emotion detection.",http://aclweb.org/anthology/D18-1280,D18-1,D18-1280,https://arxiv.org/abs/1804.05788,"('Devamanyu Hazarika', 'Soujanya Poria', 'Rada Mihalcea', 'Erik Cambria', 'Roger Zimmermann')",10,ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection,EMNLP,2018
"Thanks to the success of object detection technology, we can retrieve objects of the specified classes even from huge image collections. However, the current state-of-the-art object detectors (such as Faster R-CNN) can only handle pre-specified classes. In addition, large amounts of positive and negative visual samples are required for training. In this paper, we address the problem of open-vocabulary object retrieval and localization, where the target object is specified by a textual query (e.g., a word or phrase). We first propose Query-Adaptive R-CNN, a simple extension of Faster R-CNN adapted to open-vocabulary queries, by transforming the text embedding vector into an object classifier and localization regressor. Then, for discriminative training, we then propose negative phrase augmentation (NPA) to mine hard negative samples which are visually similar to the query and at the same time semantically mutually exclusive of the query. The proposed method can retrieve and localize objects specified by a textual query from one million images in only 0.5 seconds with high precision.",http://aclweb.org/anthology/D18-1281,D18-1,D18-1281,https://arxiv.org/abs/1711.09509,"('Ryota Hinami', ""Shin'ichi Satoh"")",10,Discriminative Learning of Open-Vocabulary Object Retrieval and Localization by Negative Phrase Augmentation,EMNLP,2018
"Semantic object parts can be useful for several visual recognition tasks. Lately, these tasks have been addressed using Convolutional Neural Networks (CNN), achieving outstanding results. In this work we study whether CNNs learn semantic parts in their internal representation. We investigate the responses of convolutional filters and try to associate their stimuli with semantic parts. We perform two extensive quantitative analyses. First, we use ground-truth part bounding-boxes from the PASCAL-Part dataset to determine how many of those semantic parts emerge in the CNN. We explore this emergence for different layers, network depths, and supervision levels. Second, we collect human judgements in order to study what fraction of all filters systematically fire on any semantic part, even if not annotated in PASCAL-Part. Moreover, we explore several connections between discriminative power and semantics. We find out which are the most discriminative filters for object recognition, and analyze whether they respond to semantic parts or to other image patches. We also investigate the other direction: we determine which semantic parts are the most discriminative and whether they correspond to those parts emerging in the network. This enables to gain an even deeper understanding of the role of semantic parts in the network.",http://aclweb.org/anthology/D18-1282,D18-1,D18-1282,https://arxiv.org/abs/1607.03738,"('Carina Silberer', 'Manfred Pinkal')",10,Grounding Semantic Roles in Images,EMNLP,2018
"The purpose of this paper is twofold: (i) we argue that the structure of commonsense knowledge must be discovered, rather than invented; and (ii) we argue that natural language, which is the best known theory of our (shared) commonsense knowledge, should itself be used as a guide to discovering the structure of commonsense knowledge. In addition to suggesting a systematic method to the discovery of the structure of commonsense knowledge, the method we propose seems to also provide an explanation for a number of phenomena in natural language, such as metaphor, intensionality, and the semantics of nominal compounds. Admittedly, our ultimate goal is quite ambitious, and it is no less than the systematic 'discovery' of a well-typed ontology of commonsense knowledge, and the subsequent formulation of the long-awaited goal of a meaning algebra.",http://aclweb.org/anthology/D18-1283,D18-1,D18-1283,https://arxiv.org/abs/cs/0610067,"('Shaohua Yang', 'Qiaozi Gao', 'Sari Sadiya', 'Joyce Chai')",10,Commonsense Justification for Action Explanation,EMNLP,2018
"The ability to infer persona from dialogue can have applications in areas ranging from computational narrative analysis to personalized dialogue generation. We introduce neural models to learn persona embeddings in a supervised character trope classification task. The models encode dialogue snippets from IMDB into representations that can capture the various categories of film characters. The best-performing models use a multi-level attention mechanism over a set of utterances. We also utilize prior knowledge in the form of textual descriptions of the different tropes. We apply the learned embeddings to find similar characters across different movies, and cluster movies according to the distribution of the embeddings. The use of short conversational text as input, and the ability to learn from prior knowledge using memory, suggests these methods could be applied to other domains.",http://aclweb.org/anthology/D18-1284,D18-1,D18-1284,https://arxiv.org/abs/1810.08717,"('Eric Chu', 'Prashanth Vijayaraghavan', 'Deb Roy')",10,Learning Personas from Dialogue with Attentive Memory Networks,EMNLP,2018
"In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) with a novel encoder-decoder-reconstructor architecture, which leverages both the forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder makes use of the forward flow to produce the sentence description based on the encoded video semantic features. Two types of reconstructors are customized to employ the backward flow and reproduce the video features based on the hidden state sequence generated by the decoder. The generation loss yielded by the encoder-decoder and the reconstruction loss introduced by the reconstructor are jointly drawn into training the proposed RecNet in an end-to-end fashion. Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the encoder-decoder models and leads to significant gains in video caption accuracy.",http://aclweb.org/anthology/D18-1285,D18-1,D18-1285,https://arxiv.org/abs/1803.11438,"('Candace Ross', 'Andrei Barbu', 'Yevgeni Berzak', 'Battushig Myanganbayar', 'Boris Katz')",10,Grounding language acquisition by training semantic parsers using captioned videos,EMNLP,2018
"We propose an end-to-end deep learning model for translating free-form natural language instructions to a high-level plan for behavioral robot navigation. We use attention models to connect information from both the user instructions and a topological representation of the environment. We evaluate our model's performance on a new dataset containing 10,050 pairs of navigation instructions. Our model significantly outperforms baseline approaches. Furthermore, our results suggest that it is possible to leverage the environment map as a relevant knowledge base to facilitate the translation of free-form navigational instruction.",http://aclweb.org/anthology/D18-1286,D18-1,D18-1286,https://arxiv.org/abs/1810.00663,"('Xiaoxue Zang', 'Ashwini Pokle', 'Marynel Vázquez', 'Kevin Chen', 'Juan Carlos Niebles', 'Alvaro Soto', 'Silvio Savarese')",10,Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation,EMNLP,2018
"We propose to decompose instruction execution to goal prediction and action generation. We design a model that maps raw visual observations to goals using LINGUNET, a language-conditioned image generation network, and then generates the actions required to complete them. Our model is trained from demonstration only without external resources.   To evaluate our approach, we introduce two benchmarks for instruction following: LANI, a navigation task; and CHAI, where an agent executes household instructions. Our evaluation demonstrates the advantages of our model decomposition, and illustrates the challenges posed by our new benchmarks.",http://aclweb.org/anthology/D18-1287,D18-1,D18-1287,https://arxiv.org/abs/1809.00786,"('Dipendra Misra', 'Andrew Bennett', 'Valts Blukis', 'Eyvind Niklasson', 'Max Shatkhin', 'Yoav Artzi')",10,Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction,EMNLP,2018
"A great improvement to the insight on brain function that we can get from fMRI data can come from effective connectivity analysis, in which the flow of information between even remote brain regions is inferred by the parameters of a predictive dynamical model. As opposed to biologically inspired models, some techniques as Granger causality (GC) are purely data-driven and rely on statistical prediction and temporal precedence. While powerful and widely applicable, this approach could suffer from two main limitations when applied to BOLD fMRI data: confounding effect of hemodynamic response function (HRF) and conditioning to a large number of variables in presence of short time series. For task-related fMRI, neural population dynamics can be captured by modeling signal dynamics with explicit exogenous inputs; for resting-state fMRI on the other hand, the absence of explicit inputs makes this task more difficult, unless relying on some specific prior physiological hypothesis. In order to overcome these issues and to allow a more general approach, here we present a simple and novel blind-deconvolution technique for BOLD-fMRI signal. Coming to the second limitation, a fully multivariate conditioning with short and noisy data leads to computational problems due to overfitting. Furthermore, conceptual issues arise in presence of redundancy. We thus apply partial conditioning to a limited subset of variables in the framework of information theory, as recently proposed. Mixing these two improvements we compare the differences between BOLD and deconvolved BOLD level effective networks and draw some conclusions.",http://aclweb.org/anthology/D18-1288,D18-1,D18-1288,https://arxiv.org/abs/1208.3766,"('Cory Shain', 'William Schuler')",10,Deconvolutional Time Series Regression: A Technique for Modeling Temporally Diffuse Effects,EMNLP,2018
"Run-on sentences are common grammatical mistakes but little research has tackled this problem to date. This work introduces two machine learning models to correct run-on sentences that outperform leading methods for related tasks, punctuation restoration and whole-sentence grammatical error correction. Due to the limited annotated data for this error, we experiment with artificially generating training data from clean newswire text. Our findings suggest artificial training data is viable for this task. We discuss implications for correcting run-ons and other types of mistakes that have low coverage in error-annotated corpora.",http://aclweb.org/anthology/D18-1289,D18-1,D18-1289,https://arxiv.org/abs/1809.08298,"('Dominique Brunato', 'Lorenzo De Mattei', ""Felice Dell'Orletta"", 'Benedetta Iavarone', 'Giulia Venturi')",10,Is this Sentence Difficult? Do you Agree?,EMNLP,2018
"The most approaches to Knowledge Base Question Answering are based on semantic parsing. In this paper, we address the problem of learning vector representations for complex semantic parses that consist of multiple entities and relations. Previous work largely focused on selecting the correct semantic relations for a question and disregarded the structure of the semantic parse: the connections between entities and the directions of the relations. We propose to use Gated Graph Neural Networks to encode the graph structure of the semantic parse. We show on two data sets that the graph networks outperform all baseline models that do not explicitly model the structure. The error analysis confirms that our approach can successfully process complex semantic parses.",http://aclweb.org/anthology/D18-1290,D18-1,D18-1290,https://arxiv.org/abs/1808.04126,"('Rivka Malca', 'Roi Reichart')",10,Neural Transition Based Parsing of Web Queries: An Entity Based Approach,EMNLP,2018
"We provide a comprehensive analysis of the interactions between pre-trained word embeddings, character models and POS tags in a transition-based dependency parser. While previous studies have shown POS information to be less important in the presence of character models, we show that in fact there are complex interactions between all three techniques. In isolation each produces large improvements over a baseline system using randomly initialised word embeddings only, but combining them quickly leads to diminishing returns. We categorise words by frequency, POS tag and language in order to systematically investigate how each of the techniques affects parsing quality. For many word categories, applying any two of the three techniques is almost as good as the full combined system. Character models tend to be more important for low-frequency open-class words, especially in morphologically rich languages, while POS tags can help disambiguate high-frequency function words. We also show that large character embedding sizes help even for languages with small character sets, especially in morphologically rich languages.",http://aclweb.org/anthology/D18-1291,D18-1,D18-1291,https://arxiv.org/abs/1808.09060,"('Aaron Smith', 'Miryam de Lhoneux', 'Sara Stymne', 'Joakim Nivre')",10,"An Investigation of the Interactions Between Pre-Trained Word Embeddings, Character Models and POS Tags in Dependency Parsing",EMNLP,2018
"There have been several recent attempts to improve the accuracy of grammar induction systems by bounding the recursive complexity of the induction model (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018). Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depth-bounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer (Johnson et al., 2007b), where bounding can be switched on and off, and then samples trees with and without bounding. Results show that depth-bounding is indeed significantly effective in limiting the search space of the inducer and thereby increasing the accuracy of the resulting parsing model. Moreover, parsing results on English, Chinese and German show that this bounded model with a new inference technique is able to produce parse trees more accurately than or competitively with state-of-the-art constituency-based grammar induction models.",http://aclweb.org/anthology/D18-1292,D18-1,D18-1292,https://arxiv.org/abs/1809.03112,"('Lifeng Jin', 'Finale Doshi-Velez', 'Timothy Miller', 'William Schuler', 'Lane Schwartz')",10,Depth-bounding is effective: Improvements and evaluation of unsupervised PCFG induction,EMNLP,2018
"Probabilistic timed automata are classical timed automata extended with discrete probability distributions over edges. We introduce clock-dependent probabilistic timed automata, a variant of probabilistic timed automata in which transition probabilities can depend linearly on clock values. Clock-dependent probabilistic timed automata allow the modelling of a continuous relationship between time passage and the likelihood of system events. We show that the problem of deciding whether the maximum probability of reaching a certain location is above a threshold is undecidable for clock-dependent probabilistic timed automata. On the other hand, we show that the maximum and minimum probability of reaching a certain location in clock-dependent probabilistic timed automata can be approximated using a region-graph-based approach.",http://aclweb.org/anthology/D18-1293,D18-1,D18-1293,https://arxiv.org/abs/1707.04507,"('Marco Cognetta', 'Yo-Sub Han', 'Soon Chan Kwon')",10,Incremental Computation of Infix Probabilities for Probabilistic Finite Automata,EMNLP,2018
"There are many occasions in which the security community is interested to discover the authorship of malware binaries, either for digital forensics analysis of malware corpora or for thwarting live threats of malware invasion. Such a discovery of authorship might be possible due to stylistic features inherent to software codes written by human programmers. Existing studies of authorship attribution of general purpose software mainly focus on source code, which is typically based on the style of programs and environment. However, those features critically depend on the availability of the program source code, which is usually not the case when dealing with malware binaries. Such program binaries often do not retain many semantic or stylistic features due to the compilation process. Therefore, authorship attribution in the domain of malware binaries based on features and styles that will survive the compilation process is challenging. This paper provides the state of the art in this literature. Further, we analyze the features involved in those techniques. By using a case study, we identify features that can survive the compilation process. Finally, we analyze existing works on binary authorship attribution and study their applicability to real malware binaries.",http://aclweb.org/anthology/D18-1294,D18-1,D18-1294,https://arxiv.org/abs/1701.02711,"('Richong Zhang', 'Zhiyuan Hu', 'Hongyu Guo', 'Yongyi Mao')",10,Syntax Encoding with Application in Authorship Attribution,EMNLP,2018
"Semantic segmentation has recently witnessed major progress, where fully convolutional neural networks have shown to perform well. However, most of the previous work focused on improving single image segmentation. To our knowledge, no prior work has made use of temporal video information in a recurrent network. In this paper, we introduce a novel approach to implicitly utilize temporal data in videos for online semantic segmentation. The method relies on a fully convolutional network that is embedded into a gated recurrent architecture. This design receives a sequence of consecutive video frames and outputs the segmentation of the last frame. Convolutional gated recurrent networks are used for the recurrent part to preserve spatial connectivities in the image. Our proposed method can be applied in both online and batch segmentation. This architecture is tested for both binary and semantic video segmentation tasks. Experiments are conducted on the recent benchmarks in SegTrack V2, Davis, CityScapes, and Synthia. Using recurrent fully convolutional networks improved the baseline network performance in all of our experiments. Namely, 5% and 3% improvement of F-measure in SegTrack2 and Davis respectively, 5.7% improvement in mean IoU in Synthia and 3.5% improvement in categorical mean IoU in CityScapes. The performance of the RFCN network depends on its baseline fully convolutional network. Thus RFCN architecture can be seen as a method to improve its baseline segmentation network by exploiting spatiotemporal information in videos.",http://aclweb.org/anthology/D18-1295,D18-1,D18-1295,https://arxiv.org/abs/1611.05435,"('Oliver Hellwig', 'Sebastian Nehrdich')",10,Sanskrit Word Segmentation Using Character-level Recurrent and Convolutional Neural Networks,EMNLP,2018
"For conversational large-vocabulary continuous speech recognition (LVCSR) tasks, up to about two thousand hours of audio is commonly used to train state of the art models. Collection of labeled conversational audio however, is prohibitively expensive, laborious and error-prone. Furthermore, academic corpora like Fisher English (2004) or Switchboard (1992) are inadequate to train models with sufficient accuracy in the unbounded space of conversational speech. These corpora are also timeworn due to dated acoustic telephony features and the rapid advancement of colloquial vocabulary and idiomatic speech over the last decades. Utilizing the colossal scale of our unlabeled telephony dataset, we propose a technique to construct a modern, high quality conversational speech training corpus on the order of hundreds of millions of utterances (or tens of thousands of hours) for both acoustic and language model training. We describe the data collection, selection and training, evaluating the results of our updated speech recognition system on a test corpus of 7K manually transcribed utterances. We show relative word error rate (WER) reductions of {35%, 19%} on {agent, caller} utterances over our seed model and 5% absolute WER improvements over IBM Watson STT on this conversational speech task.",http://aclweb.org/anthology/D18-1296,D18-1,D18-1296,https://arxiv.org/abs/1705.09724,"('Wayne Xiong', 'Lingfeng Wu', 'Jun Zhang', 'Andreas Stolcke')",10,Session-level Language Modeling for Conversational Speech,EMNLP,2018
"Dialog response selection is an important step towards natural response generation in conversational agents. Existing work on neural conversational models mainly focuses on offline supervised learning using a large set of context-response pairs. In this paper, we focus on online learning of response selection in retrieval-based dialog systems. We propose a contextual multi-armed bandit model with a nonlinear reward function that uses distributed representation of text for online response selection. A bidirectional LSTM is used to produce the distributed representations of dialog context and responses, which serve as the input to a contextual bandit. In learning the bandit, we propose a customized Thompson sampling method that is applied to a polynomial feature space in approximating the reward. Experimental results on the Ubuntu Dialogue Corpus demonstrate significant performance gains of the proposed method over conventional linear contextual bandits. Moreover, we report encouraging response selection performance of the proposed neural bandit model using the Recall@k metric for a small set of online training samples.",http://aclweb.org/anthology/D18-1297,D18-1,D18-1297,https://arxiv.org/abs/1711.08493,"('Yahui Liu', 'Wei Bi', 'Jun Gao', 'Xiaojiang Liu', 'Jian Yao', 'Shuming Shi')",10,Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method,EMNLP,2018
"Current dialogue systems are not very engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in Zhang et al. (2018) is synthetic and of limited size as it contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from Zhang et al. (2018) and achieving state-of-the-art results.",http://aclweb.org/anthology/D18-1298,D18-1,D18-1298,https://arxiv.org/abs/1809.01984,"('Pierre-Emmanuel Mazare', 'Samuel Humeau', 'Martin Raison', 'Antoine Bordes')",10,Training Millions of Personalized Dialogue Agents,EMNLP,2018
"Dialogue state tracking is the core part of a spoken dialogue system. It estimates the beliefs of possible user's goals at every dialogue turn. However, for most current approaches, it's difficult to scale to large dialogue domains. They have one or more of following limitations: (a) Some models don't work in the situation where slot values in ontology changes dynamically; (b) The number of model parameters is proportional to the number of slots; (c) Some models extract features based on hand-crafted lexicons. To tackle these challenges, we propose StateNet, a universal dialogue state tracker. It is independent of the number of values, shares parameters across all slots, and uses pre-trained word vectors instead of explicit semantic dictionaries. Our experiments on two datasets show that our approach not only overcomes the limitations, but also significantly outperforms the performance of state-of-the-art approaches.",http://aclweb.org/anthology/D18-1299,D18-1,D18-1299,https://arxiv.org/abs/1810.09587,"('Liliang Ren', 'Kaige Xie', 'Lu Chen', 'Kai Yu')",10,Towards Universal Dialogue State Tracking,EMNLP,2018
"Task oriented dialog systems typically first parse user utterances to semantic frames comprised of intents and slots. Previous work on task oriented intent and slot-filling work has been restricted to one intent per query and one slot label per token, and thus cannot model complex compositional requests. Alternative semantic parsing systems have represented queries as logical forms, but these are challenging to annotate and parse. We propose a hierarchical annotation scheme for semantic parsing that allows the representation of compositional queries, and can be efficiently and accurately parsed by standard constituency parsing models. We release a dataset of 44k annotated queries (fb.me/semanticparsingdialog), and show that parsing models outperform sequence-to-sequence approaches on this dataset.",http://aclweb.org/anthology/D18-1300,D18-1,D18-1300,https://arxiv.org/abs/1810.07942,"('Sonal Gupta', 'Rushin Shah', 'Mrinal Mohit', 'Anuj Kumar', 'Mike Lewis')",10,Semantic Parsing for Task Oriented Dialog using Hierarchical Representations,EMNLP,2018
"Liquids relax extremely slowly upon approaching the glass state. One explanation is that an entropy crisis, due to the rarefaction of available states, makes it increasingly arduous to reach equilibrium in that regime. Validating this scenario is challenging, because experiments offer limited resolution, while numerical studies lag more than eight orders of magnitude behind experimentally-relevant timescales. In this work we not only close the colossal gap between experiments and simulations but manage to create in-silico configurations that have no experimental analog yet. Deploying a range of computational tools, we obtain four estimates of their configurational entropy. These measurements consistently confirm that the steep entropy decrease observed in experiments is also found in simulations, even beyond the experimental glass transition. Our numerical results thus extend the new observational window into the physics of glasses and reinforce the relevance of an entropy crisis for understanding their formation.",http://aclweb.org/anthology/D18-1301,D18-1,D18-1301,https://arxiv.org/abs/1704.08257,['Natalie Schluter'],10,The glass ceiling in NLP,EMNLP,2018
"Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, ""You are a good woman"" was considered ""sexist"" when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure gender biases on models trained with different abusive language datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three bias mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce gender bias by 90-98% and can be extended to correct model bias in other scenarios.",http://aclweb.org/anthology/D18-1302,D18-1,D18-1302,https://arxiv.org/abs/1808.07231,"('Ji Ho Park', 'Jamin Shin', 'Pascale Fung')",10,Reducing Gender Bias in Abusive Language Detection,EMNLP,2018
"With the recent rise of #MeToo, an increasing number of personal stories about sexual harassment and sexual abuse have been shared online. In order to push forward the fight against such harassment and abuse, we present the task of automatically categorizing and analyzing various forms of sexual harassment, based on stories shared on the online forum SafeCity. For the labels of groping, ogling, and commenting, our single-label CNN-RNN model achieves an accuracy of 86.5%, and our multi-label model achieves a Hamming score of 82.5%. Furthermore, we present analysis using LIME, first-derivative saliency heatmaps, activation clustering, and embedding visualization to interpret neural model predictions and demonstrate how this extracts features that can help automatically fill out incident reports, identify unsafe areas, avoid unsafe practices, and 'pin the creeps'.",http://aclweb.org/anthology/D18-1303,D18-1,D18-1303,https://arxiv.org/abs/1809.04739,"('Sweta Karlekar', 'Mohit Bansal')",10,SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories,EMNLP,2018
"Interactive cognitive assessment tools may be valuable for doctors and therapists to reduce costs and improve quality in healthcare systems. Use cases and scenarios include the assessment of dementia. In this paper, we present our approach to the semi-automatic assessment of dementia. We describe a case study with digital pens for the patients including background, problem description and possible solutions. We conclude with lessons learned when implementing digital tests, and a generalisation for use outside the cognitive impairments field.",http://aclweb.org/anthology/D18-1304,D18-1,D18-1304,https://arxiv.org/abs/1810.04943,"('Chloé Pou-Prom', 'Frank Rudzicz')",10,Learning multiview embeddings for assessing dementia,EMNLP,2018
"We present a corpus that encompasses the complete history of conversations between contributors to Wikipedia, one of the largest online collaborative communities. By recording the intermediate states of conversations---including not only comments and replies, but also their modifications, deletions and restorations---this data offers an unprecedented view of online conversation. This level of detail supports new research questions pertaining to the process (and challenges) of large-scale online collaboration. We illustrate the corpus' potential with two case studies that highlight new perspectives on earlier work. First, we explore how a person's conversational behavior depends on how they relate to the discussion's venue. Second, we show that community moderation of toxic behavior happens at a higher rate than previously estimated. Finally the reconstruction framework is designed to be language agnostic, and we show that it can extract high quality conversational data in both Chinese and English.",http://aclweb.org/anthology/D18-1305,D18-1,D18-1305,https://arxiv.org/abs/1810.13181,"('Yiqing Hua', 'Cristian Danescu-Niculescu-Mizil', 'Dario Taraborelli', 'Nithum Thain', 'Jeffery Sorensen', 'Lucas Dixon')",10,WikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community,EMNLP,2018
"Named entity recognition often fails in idiosyncratic domains. That causes a problem for depending tasks, such as entity linking and relation extraction. We propose a generic and robust approach for high-recall named entity recognition. Our approach is easy to train and offers strong generalization over diverse domain-specific language, such as news documents (e.g. Reuters) or biomedical text (e.g. Medline). Our approach is based on deep contextual sequence learning and utilizes stacked bidirectional LSTM networks. Our model is trained with only few hundred labeled sentences and does not rely on further external knowledge. We report from our results F1 scores in the range of 84-94% on standard datasets.",http://aclweb.org/anthology/D18-1306,D18-1,D18-1306,https://arxiv.org/abs/1608.06757,"('Nathan Greenberg', 'Trapit Bansal', 'Patrick Verga', 'Andrew McCallum')",10,Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets,EMNLP,2018
"Adversarial training (AT) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data. We show how to use AT for the tasks of entity recognition and relation extraction. In particular, we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch).",http://aclweb.org/anthology/D18-1307,D18-1,D18-1307,https://arxiv.org/abs/1808.06876,"('Giannis Bekoulis', 'Johannes Deleu', 'Thomas Demeester', 'Chris Develder')",10,Adversarial training for multi-context joint entity and relation extraction,EMNLP,2018
"We propose a model for tagging unstructured texts with an arbitrary number of terms drawn from a tree-structured vocabulary (i.e., an ontology). We treat this as a special case of sequence-to-sequence learning in which the decoder begins at the root node of an ontological tree and recursively elects to expand child nodes as a function of the input text, the current node, and the latent decoder state. In our experiments the proposed method outperforms state-of-the-art approaches on the important task of automatically assigning MeSH terms to biomedical abstracts.",http://aclweb.org/anthology/D18-1308,D18-1,D18-1308,https://arxiv.org/abs/1810.01468,"('Gaurav Singh', 'James Thomas', 'Iain Marshall', 'John Shawe-Taylor', 'Byron C. Wallace')",10,Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree Decoding,EMNLP,2018
"In this report, we describe our participant named-entity recognition system at VLSP 2018 evaluation campaign. We formalized the task as a sequence labeling problem using BIO encoding scheme. We applied a feature-based model which combines word, word-shape features, Brown-cluster-based features, and word-embedding-based features. We compare several methods to deal with nested entities in the dataset. We showed that combining tags of entities at all levels for training a sequence labeling model (joint-tag model) improved the accuracy of nested named-entity recognition.",http://aclweb.org/anthology/D18-1309,D18-1,D18-1309,https://arxiv.org/abs/1803.08463,"('Mohammad Golam Sohrab', 'Makoto Miwa')",10,Deep Exhaustive Model for Nested Named Entity Recognition,EMNLP,2018
"In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations. The proposed model obtains (near) state-of-the art performance for both part-of-speech tagging and named entity recognition for a variety of languages. Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks. The proposed model has favorable generalization properties as it retains over 89.8% of its average POS tagging accuracy when trained at 1.2% of the total available training data, i.e.~150 sentences per language.",http://aclweb.org/anthology/D18-1310,D18-1,D18-1310,https://arxiv.org/abs/1612.07130,"('Minghao Wu', 'Fei Liu', 'Trevor Cohn')",10,Evaluating the Utility of Hand-crafted Features in Sequence Labelling,EMNLP,2018
"We consider the supervised training setting in which we learn task-specific word embeddings. We assume that we start with initial embeddings learned from unlabelled data and update them to learn task-specific embeddings for words in the supervised training data. However, for new words in the test set, we must use either their initial embeddings or a single unknown embedding, which often leads to errors. We address this by learning a neural network to map from initial embeddings to the task-specific embedding space, via a multi-loss objective function. The technique is general, but here we demonstrate its use for improved dependency parsing (especially for sentences with out-of-vocabulary words), as well as for downstream improvements on sentiment analysis.",http://aclweb.org/anthology/D18-1311,D18-1,D18-1311,https://arxiv.org/abs/1510.02387,"('Wenhui Wang', 'Baobao Chang', 'Mairgup Mansur')",10,Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data,EMNLP,2018
"This paper presents a semantic parsing approach for unrestricted texts. Semantic parsing is one of the major bottlenecks of Natural Language Understanding (NLU) systems and usually requires building expensive resources not easily portable to other domains. Our approach obtains a case-role analysis, in which the semantic roles of the verb are identified. In order to cover all the possible syntactic realisations of a verb, our system combines their argument structure with a set of general semantic labelled diatheses models. Combining them, the system builds a set of syntactic-semantic patterns with their own role-case representation. Once the patterns are build, we use an approximate tree pattern-matching algorithm to identify the most reliable pattern for a sentence. The pattern matching is performed between the syntactic-semantic patterns and the feature-structure tree representing the morphological, syntactical and semantic information of the analysed sentence. For sentences assigned to the correct model, the semantic parsing system we are presenting identifies correctly more than 73% of possible semantic case-roles.",http://aclweb.org/anthology/D18-1312,D18-1,D18-1312,https://arxiv.org/abs/cs/0006041,"('Mathieu Dehouck', 'Pascal Denis')",10,A Framework for Understanding the Role of Morphology in Universal Dependency Parsing,EMNLP,2018
"The advent of the attention mechanism in neural machine translation models has improved the performance of machine translation systems by enabling selective lookup into the source sentence. In this paper, the efficiencies of translation using bidirectional encoder attention decoder models were studied with respect to translation involving morphologically rich languages. The English - Tamil language pair was selected for this analysis. First, the use of Word2Vec embedding for both the English and Tamil words improved the translation results by 0.73 BLEU points over the baseline RNNSearch model with 4.84 BLEU score. The use of morphological segmentation before word vectorization to split the morphologically rich Tamil words into their respective morphemes before the translation, caused a reduction in the target vocabulary size by a factor of 8. Also, this model (RNNMorph) improved the performance of neural machine translation by 7.05 BLEU points over the RNNSearch model used over the same corpus. Since the BLEU evaluation of the RNNMorph model might be unreliable due to an increase in the number of matching tokens per sentence, the performances of the translations were also compared by means of human evaluation metrics of adequacy, fluency and relative ranking. Further, the use of morphological segmentation also improved the efficacy of the attention mechanism.",http://aclweb.org/anthology/D18-1313,D18-1,D18-1313,https://arxiv.org/abs/1612.02482,"('Arianna Bisazza', 'Clara Tump')",10,The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation,EMNLP,2018
"We employ imitation learning to train a neural transition-based string transducer for morphological tasks such as inflection generation and lemmatization. Previous approaches to training this type of model either rely on an external character aligner for the production of gold action sequences, which results in a suboptimal model due to the unwarranted dependence on a single gold action sequence despite spurious ambiguity, or require warm starting with an MLE model. Our approach only requires a simple expert policy, eliminating the need for a character aligner or warm start. It also addresses familiar MLE training biases and leads to strong and state-of-the-art performance on several benchmarks.",http://aclweb.org/anthology/D18-1314,D18-1,D18-1314,https://arxiv.org/abs/1808.10701,"('Peter Makarov', 'Simon Clematide')",10,Imitation Learning for Neural Morphological String Transduction,EMNLP,2018
"Current efforts in the biomedical sciences and related interdisciplinary fields are focused on gaining a molecular understanding of health and disease, which is a problem of daunting complexity that spans many orders of magnitude in characteristic length scales, from small molecules that regulate cell function to cell ensembles that form tissues and organs working together as an organism. In order to uncover the molecular nature of the emergent properties of a cell, it is essential to measure multiple cell components simultaneously in the same cell. In turn, cell heterogeneity requires multiple cells to be measured in order to understand health and disease in the organism. This review summarizes current efforts towards a data-driven framework that leverages single-cell technologies to build robust signatures of healthy and diseased phenotypes. While some approaches focus on multicolor flow cytometry data and other methods are designed to analyze high-content image-based screens, we emphasize the so-called Supercell/SVM paradigm (recently developed by the authors of this review and collaborators) as a unified framework that captures mesoscopic-scale emergence to build reliable phenotypes. Beyond their specific contributions to basic and translational biomedical research, these efforts illustrate, from a larger perspective, the powerful synergy that might be achieved from bringing together methods and ideas from statistical physics, data mining, and mathematics to solve the most pressing problems currently facing the life sciences.",http://aclweb.org/anthology/D18-1315,D18-1,D18-1315,https://arxiv.org/abs/1311.1496,"('Miikka Silfverberg', 'Mans Hulden')",10,An Encoder-Decoder Approach to the Paradigm Cell Filling Problem,EMNLP,2018
"Adversarial examples are inputs to machine learning models designed to cause the model to make a mistake. They are useful for understanding the shortcomings of machine learning models, interpreting their results, and for regularisation. In NLP, however, most example generation strategies produce input text by using known, pre-specified semantic transformations, requiring significant manual effort and in-depth understanding of the problem and domain. In this paper, we investigate the problem of automatically generating adversarial examples that violate a set of given First-Order Logic constraints in Natural Language Inference (NLI). We reduce the problem of identifying such adversarial examples to a combinatorial optimisation problem, by maximising a quantity measuring the degree of violation of such constraints and by using a language model for generating linguistically-plausible examples. Furthermore, we propose a method for adversarially regularising neural NLI models for incorporating background knowledge. Our results show that, while the proposed method does not always improve results on the SNLI and MultiNLI datasets, it significantly and consistently increases the predictive accuracy on adversarially-crafted datasets -- up to a 79.6% relative improvement -- while drastically reducing the number of background knowledge violations. Furthermore, we show that adversarial examples transfer among model architectures, and that the proposed adversarial training procedure improves the robustness of NLI models to adversarial examples.",http://aclweb.org/anthology/D18-1316,D18-1,D18-1316,https://arxiv.org/abs/1808.08609,"('Moustafa Alzantot', 'Yash Sharma', 'Ahmed Elgohary', 'Bo-Jhang Ho', 'Mani Srivastava', 'Kai-Wei Chang')",10,Generating Natural Language Adversarial Examples,EMNLP,2018
"Multi-head attention is appealing for the ability to jointly attend to information from different representation subspaces at different positions. In this work, we introduce a disagreement regularization to explicitly encourage the diversity among multiple attention heads. Specifically, we propose three types of disagreement regularization, which respectively encourage the subspace, the attended positions, and the output representation associated with each attention head to be different from other heads. Experimental results on widely-used WMT14 English-German and WMT17 Chinese-English translation tasks demonstrate the effectiveness and universality of the proposed approach.",http://aclweb.org/anthology/D18-1317,D18-1,D18-1317,https://arxiv.org/abs/1810.10183,"('Jian Li', 'Zhaopeng Tu', 'Baosong Yang', 'Michael R. Lyu', 'Tong Zhang')",10,Multi-Head Attention with Disagreement Regularization,EMNLP,2018
"Several recent papers investigate Active Learning (AL) for mitigating the data dependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, an active learner has no opportunity to compare models and acquisition functions. This paper provides a large scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or Bayes-by Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.",http://aclweb.org/anthology/D18-1318,D18-1,D18-1318,https://arxiv.org/abs/1808.05697,"('Aditya Siddhant', 'Zachary C. Lipton')",10,Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study,EMNLP,2018
"In natural language processing, a lot of the tasks are successfully solved with recurrent neural networks, but such models have a huge number of parameters. The majority of these parameters are often concentrated in the embedding layer, which size grows proportionally to the vocabulary length. We propose a Bayesian sparsification technique for RNNs which allows compressing the RNN dozens or hundreds of times without time-consuming hyperparameters tuning. We also generalize the model for vocabulary sparsification to filter out unnecessary words and compress the RNN even further. We show that the choice of the kept words is interpretable.",http://aclweb.org/anthology/D18-1319,D18-1,D18-1319,https://arxiv.org/abs/1810.10927,"('Nadezhda Chirkova', 'Ekaterina Lobacheva', 'Dmitry Vetrov')",10,Bayesian Compression for Natural Language Processing,EMNLP,2018
"Graphemes of most languages encode pronunciation, though some are more explicit than others. Languages like Spanish have a straightforward mapping between its graphemes and phonemes, while this mapping is more convoluted for languages like English. Spoken languages such as Cantonese present even more challenges in pronunciation modeling: (1) they do not have a standard written form, (2) the closest graphemic origins are logographic Han characters, of which only a subset of these logographic characters implicitly encodes pronunciation. In this work, we propose a multimodal approach to predict the pronunciation of Cantonese logographic characters, using neural networks with a geometric representation of logographs and pronunciation of cognates in historically related languages. The proposed framework improves performance by 18.1% and 25.0% respective to unimodal and multimodal baselines.",http://aclweb.org/anthology/D18-1320,D18-1,D18-1320,https://arxiv.org/abs/1809.04203,"('Minh Nguyen', 'Gia H Ngo', 'Nancy Chen')",10,Multimodal neural pronunciation modeling for spoken languages with logographic origin,EMNLP,2018
"Chinese pinyin input method engine (IME) converts pinyin into character so that Chinese characters can be conveniently inputted into computer through common keyboard. IMEs work relying on its core component, pinyin-to-character conversion (P2C). Usually Chinese IMEs simply predict a list of character sequences for user choice only according to user pinyin input at each turn. However, Chinese inputting is a multi-turn online procedure, which can be supposed to be exploited for further user experience promoting. This paper thus for the first time introduces a sequence-to-sequence model with gated-attention mechanism for the core task in IMEs. The proposed neural P2C model is learned by encoding previous input utterance as extra context to enable our IME capable of predicting character sequence with incomplete pinyin input. Our model is evaluated in different benchmark datasets showing great user experience improvement compared to traditional models, which demonstrates the first engineering practice of building Chinese aided IME.",http://aclweb.org/anthology/D18-1321,D18-1,D18-1321,https://arxiv.org/abs/1809.00329,"('Yafang Huang', 'Hai Zhao')",10,"Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet",EMNLP,2018
"In this paper, we investigate the use of prediction-adaptation-correction recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A PAC-RNN is comprised of a pair of neural networks in which a {\it correction} network uses auxiliary information given by a {\it prediction} network to help estimate the state probability. The information from the correction network is also used by the prediction network in a recurrent loop. Our model outperforms other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks. Moreover, transfer learning from a language that is similar to the target language can help improve performance further.",http://aclweb.org/anthology/D18-1322,D18-1,D18-1322,https://arxiv.org/abs/1510.08985,"('Thanapon Noraset', 'Doug Downey', 'Lidong Bing')",10,Estimating Marginal Probabilities of n-grams for Recurrent Neural Language Models,EMNLP,2018
"With a simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today. However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings. Additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word's average context vector, out-of-vocabulary (OOV) embeddings and representations for a word with multiple meanings can be created based on the word's local contexts. The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task.",http://aclweb.org/anthology/D18-1323,D18-1,D18-1323,https://arxiv.org/abs/1706.02496,"('Kristina Gulordava', 'Laura Aina', 'Gemma Boleda')",10,"How to represent a word and predict it, too: Improving tied architectures for language modelling",EMNLP,2018
"Language segmentation consists in finding the boundaries where one language ends and another language begins in a text written in more than one language. This is important for all natural language processing tasks. The problem can be solved by training language models on language data. However, in the case of low- or no-resource languages, this is problematic. I therefore investigate whether unsupervised methods perform better than supervised methods when it is difficult or impossible to train supervised approaches. A special focus is given to difficult texts, i.e. texts that are rather short (one sentence), containing abbreviations, low-resource languages and non-standard language. I compare three approaches: supervised n-gram language models, unsupervised clustering and weakly supervised n-gram language model induction. I devised the weakly supervised approach in order to deal with difficult text specifically. In order to test the approach, I compiled a small corpus of different text types, ranging from one-sentence texts to texts of about 300 words. The weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. The results look promising, but there is room for improvement and a more thorough investigation should be undertaken.",http://aclweb.org/anthology/D18-1324,D18-1,D18-1324,https://arxiv.org/abs/1510.01717,"('Nicolas Ford', 'Daniel Duckworth', 'Mohammad Norouzi', 'George Dahl')",10,The Importance of Generation Order in Language Modeling,EMNLP,2018
"Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.",http://aclweb.org/anthology/D18-1325,D18-1,D18-1325,https://arxiv.org/abs/1809.01576,"('Lesly Miculicich', 'Dhananjay Ram', 'Nikolaos Pappas', 'James Henderson')",10,Document-Level Neural Machine Translation with Hierarchical Attention Networks,EMNLP,2018
"Transferring representations from large supervised tasks to downstream tasks has shown promising results in AI fields such as Computer Vision and Natural Language Processing (NLP). In parallel, the recent progress in Machine Translation (MT) has enabled one to train multilingual Neural MT (NMT) systems that can translate between multiple languages and are also capable of performing zero-shot translation. However, little attention has been paid to leveraging representations learned by a multilingual NMT system to enable zero-shot multilinguality in other NLP tasks. In this paper, we demonstrate a simple framework, a multilingual Encoder-Classifier, for cross-lingual transfer learning by reusing the encoder from a multilingual NMT system and stitching it with a task-specific classifier component. Our proposed model achieves significant improvements in the English setup on three benchmark tasks - Amazon Reviews, SST and SNLI. Further, our system can perform classification in a new language for which no classification data was seen during training, showing that zero-shot classification is possible and remarkably competitive. In order to understand the underlying factors contributing to this finding, we conducted a series of analyses on the effect of the shared vocabulary, the training data type for NMT, classifier complexity, encoder representation power, and model generalization on zero-shot performance. Our results provide strong evidence that the representations learned from multilingual NMT systems are widely applicable across languages and tasks.",http://aclweb.org/anthology/D18-1326,D18-1,D18-1326,https://arxiv.org/abs/1809.04686,"('Yining Wang', 'Jiajun Zhang', 'Feifei Zhai', 'Jingfang Xu', 'Chengqing Zong')",10,Three Strategies to Improve One-to-Many Multilingual Translation,EMNLP,2018
"Incorporating syntactic information in Neural Machine Translation models is a method to compensate their requirement for a large amount of parallel training text, especially for low-resource language pairs. Previous works on using syntactic information provided by (inevitably error-prone) parsers has been promising. In this paper, we propose a forest-to-sequence Attentional Neural Machine Translation model to make use of exponentially many parse trees of the source sentence to compensate for the parser errors. Our method represents the collection of parse trees as a packed forest, and learns a neural attentional transduction model from the forest to the target sentence. Experiments on English to German, Chinese and Persian translation show the superiority of our method over the tree-to-sequence and vanilla sequence-to-sequence neural translation models.",http://aclweb.org/anthology/D18-1327,D18-1,D18-1327,https://arxiv.org/abs/1711.07019,"('Anna Currey', 'Kenneth Heafield')",10,Multi-Source Syntactic Neural Machine Translation,EMNLP,2018
"Although there are increasing and significant ties between China and Portuguese-speaking countries, there is not much parallel corpora in the Chinese-Portuguese language pair. Both languages are very populous, with 1.2 billion native Chinese speakers and 279 million native Portuguese speakers, the language pair, however, could be considered as low-resource in terms of available parallel corpora. In this paper, we describe our methods to curate Chinese-Portuguese parallel corpora and evaluate their quality. We extracted bilingual data from Macao government websites and proposed a hierarchical strategy to build a large parallel corpus. Experiments are conducted on existing and our corpora using both Phrased-Based Machine Translation (PBMT) and the state-of-the-art Neural Machine Translation (NMT) models. The results of this work can be used as a benchmark for future Chinese-Portuguese MT systems. The approach we used in this paper also shows a good example on how to boost performance of MT systems for low-resource language pairs.",http://aclweb.org/anthology/D18-1328,D18-1,D18-1328,https://arxiv.org/abs/1804.01768,"('Minh Quang Pham', 'Josep Crego', 'Jean Senellart', 'François Yvon')",10,Fixing Translation Divergences in Parallel Corpora for Neural MT,EMNLP,2018
"Continuous multimodal representations suitable for multimodal information retrieval are usually obtained with methods that heavily rely on multimodal autoencoders. In video hyperlinking, a task that aims at retrieving video segments, the state of the art is a variation of two interlocked networks working in opposing directions. These systems provide good multimodal embeddings and are also capable of translating from one representation space to the other. Operating on representation spaces, these networks lack the ability to operate in the original spaces (text or image), which makes it difficult to visualize the crossmodal function, and do not generalize well to unseen data. Recently, generative adversarial networks have gained popularity and have been used for generating realistic synthetic data and for obtaining high-level, single-modal latent representation spaces. In this work, we evaluate the feasibility of using GANs to obtain multimodal representations. We show that GANs can be used for multimodal representation learning and that they provide multimodal representations that are superior to representations obtained with multimodal autoencoders. Additionally, we illustrate the ability of visualizing crossmodal translations that can provide human-interpretable insights on learned GAN-based video hyperlinking models.",http://aclweb.org/anthology/D18-1329,D18-1,D18-1329,https://arxiv.org/abs/1705.05103,['Desmond Elliott'],10,Adversarial Evaluation of Multimodal Machine Translation,EMNLP,2018
"Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a least-square regression problem to learn a rotation aligning a small bilingual lexicon, and use a retrieval criterion for inference. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard benchmarks show that our approach outperforms the state of the art on word translation, with the biggest improvements observed for distant language pairs such as English-Chinese.",http://aclweb.org/anthology/D18-1330,D18-1,D18-1330,https://arxiv.org/abs/1804.07745,"('Armand Joulin', 'Piotr Bojanowski', 'Tomas Mikolov', 'Hervé Jégou', 'Edouard Grave')",10,Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion,EMNLP,2018
"Most of the Neural Machine Translation (NMT) models are based on the sequence-to-sequence (Seq2Seq) model with an encoder-decoder framework equipped with the attention mechanism. However, the conventional attention mechanism treats the decoding at each time step equally with the same matrix, which is problematic since the softness of the attention for different types of words (e.g. content words and function words) should differ. Therefore, we propose a new model with a mechanism called Self-Adaptive Control of Temperature (SACT) to control the softness of attention by means of an attention temperature. Experimental results on the Chinese-English translation and English-Vietnamese translation demonstrate that our model outperforms the baseline models, and the analysis and the case study show that our model can attend to the most relevant elements in the source-side contexts and generate the translation of high quality.",http://aclweb.org/anthology/D18-1331,D18-1,D18-1331,https://arxiv.org/abs/1808.07374,"('Junyang Lin', 'Xu Sun', 'Xuancheng Ren', 'Muyu Li', 'Qi Su')",10,Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation,EMNLP,2018
In order to extract the best possible performance from asynchronous stochastic gradient descent one must increase the mini-batch size and scale the learning rate accordingly. In order to achieve further speedup we introduce a technique that delays gradient updates effectively increasing the mini-batch size. Unfortunately with the increase of mini-batch size we worsen the stale gradient problem in asynchronous stochastic gradient descent (SGD) which makes the model convergence poor. We introduce local optimizers which mitigate the stale gradient problem and together with fine tuning our momentum we are able to train a shallow machine translation system 27% faster than an optimized baseline with negligible penalty in BLEU.,http://aclweb.org/anthology/D18-1332,D18-1,D18-1332,https://arxiv.org/abs/1808.08859,"('Nikolay Bogoychev', 'Kenneth Heafield', 'Alham Fikri Aji', 'Marcin Junczys-Dowmunt')",10,Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine Translation,EMNLP,2018
"Pronouns are frequently omitted in pro-drop languages, such as Chinese, generally leading to significant challenges with respect to the production of complete translations. Recently, Wang et al. (2018) proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models. In this work, we improve the original model from two perspectives. First, we employ a shared reconstructor to better exploit encoder and decoder representations. Second, we jointly learn to translate and predict DPs in an end-to-end manner, to avoid the errors propagated from an external DP prediction model. Experimental results show that our approach significantly improves both translation performance and DP prediction accuracy.",http://aclweb.org/anthology/D18-1333,D18-1,D18-1333,https://arxiv.org/abs/1810.06195,"('Longyue Wang', 'Zhaopeng Tu', 'Andy Way', 'Qun Liu')",10,Learning to Jointly Translate and Predict Dropped Pronouns with a Shared Reconstruction Mechanism,EMNLP,2018
"Spoken Language Translation (SLT) is becoming more widely used and becoming a communication tool that helps in crossing language barriers. One of the challenges of SLT is the translation from a language without gender agreement to a language with gender agreement such as English to Arabic. In this paper, we introduce an approach to tackle such limitation by enabling a Neural Machine Translation system to produce gender-aware translation. We show that NMT system can model the speaker/listener gender information to produce gender-aware translation. We propose a method to generate data used in adapting a NMT system to produce gender-aware. The proposed approach can achieve significant improvement of the translation quality by 2 BLEU points.",http://aclweb.org/anthology/D18-1334,D18-1,D18-1334,https://arxiv.org/abs/1802.09287,"('Eva Vanmassenhove', 'Christian Hardmeier', 'Andy Way')",10,Getting Gender Right in Neural Machine Translation,EMNLP,2018
"Draft of textbook chapter on neural machine translation. a comprehensive treatment of the topic, ranging from introduction to neural networks, computation graphs, description of the currently dominant attentional sequence-to-sequence model, recent refinements, alternative architectures and challenges. Written as chapter for the textbook Statistical Machine Translation. Used in the JHU Fall 2017 class on machine translation.",http://aclweb.org/anthology/D18-1335,D18-1,D18-1335,https://arxiv.org/abs/1709.07809,"('Parnia Bahar', 'Christopher Brix', 'Hermann Ney')",10,Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation,EMNLP,2018
"Autoregressive decoding is the only part of sequence-to-sequence models that prevents them from massive parallelization at inference time. Non-autoregressive models enable the decoder to generate all output symbols independently in parallel. We present a novel non-autoregressive architecture based on connectionist temporal classification and evaluate it on the task of neural machine translation. Unlike other non-autoregressive methods which operate in several steps, our model can be trained end-to-end. We conduct experiments on the WMT English-Romanian and English-German datasets. Our models achieve a significant speedup over the autoregressive models, keeping the translation quality comparable to other non-autoregressive models.",http://aclweb.org/anthology/D18-1336,D18-1,D18-1336,https://arxiv.org/abs/1811.04719,"('Jindřich Libovický', 'Jindřich Helcl')",10,End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification,EMNLP,2018
"We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.",http://aclweb.org/anthology/D18-1337,D18-1,D18-1337,https://arxiv.org/abs/1606.02012,"('Ashkan Alinejad', 'Maryam Siahbani', 'Anoop Sarkar')",10,Prediction Improves Simultaneous Neural Machine Translation,EMNLP,2018
"While current state-of-the-art NMT models, such as RNN seq2seq and Transformers, possess a large number of parameters, they are still shallow in comparison to convolutional models used for both text and vision applications. In this work we attempt to train significantly (2-3x) deeper Transformer and Bi-RNN encoders for machine translation. We propose a simple modification to the attention mechanism that eases the optimization of deeper models, and results in consistent gains of 0.7-1.1 BLEU on the benchmark WMT'14 English-German and WMT'15 Czech-English tasks for both architectures.",http://aclweb.org/anthology/D18-1338,D18-1,D18-1338,https://arxiv.org/abs/1808.07561,"('Ankur Bapna', 'Mia Chen', 'Orhan Firat', 'Yuan Cao', 'Yonghui Wu')",10,Training Deeper Neural Machine Translation Models with Transparent Attention,EMNLP,2018
We examine how various types of noise in the parallel training data impact the quality of neural machine translation systems. We create five types of artificial noise and analyze how they degrade performance in neural and statistical machine translation. We find that neural models are generally more harmed by noise than statistical models. For one especially egregious type of noise they learn to just copy the input sentence.,http://aclweb.org/anthology/D18-1339,D18-1,D18-1339,https://arxiv.org/abs/1805.12282,"('Rebecca Knowles', 'Philipp Koehn')",10,Context and Copying in Neural Machine Translation,EMNLP,2018
"Neural machine translation (NMT) systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring inter-sentence information. This may make the translation of a sentence ambiguous or even inconsistent with the translations of neighboring sentences. In order to handle this issue, we propose an inter-sentence gate model that uses the same encoder to encode two adjacent sentences and controls the amount of information flowing from the preceding sentence to the translation of the current sentence with an inter-sentence gate. In this way, our proposed model can capture the connection between sentences and fuse recency from neighboring sentences into neural machine translation. On several NIST Chinese-English translation tasks, our experiments demonstrate that the proposed inter-sentence gate model achieves substantial improvements over the baseline.",http://aclweb.org/anthology/D18-1340,D18-1,D18-1340,https://arxiv.org/abs/1806.04466,"('Qian Cao', 'Deyi Xiong')",10,Encoding Gated Translation Memory into Neural Machine Translation,EMNLP,2018
"Recurrent Neural Networks (RNNs) are extensively used for time-series modeling and prediction. We propose an approach for automatic construction of a binary classifier based on Long Short-Term Memory RNNs (LSTM-RNNs) for detection of a vehicle passage through a checkpoint. As an input to the classifier we use multidimensional signals of various sensors that are installed on the checkpoint. Obtained results demonstrate that the previous approach to handcrafting a classifier, consisting of a set of deterministic rules, can be successfully replaced by an automatic RNN training on an appropriately labelled data.",http://aclweb.org/anthology/D18-1341,D18-1,D18-1341,https://arxiv.org/abs/1609.08209,"('Thuy-Trang Vu', 'Gholamreza Haffari')",10,Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter Approach,EMNLP,2018
"Beam search is widely used in neural machine translation, and usually improves translation quality compared to greedy search. It has been widely observed that, however, beam sizes larger than 5 hurt translation quality. We explain why this happens, and propose several methods to address this problem. Furthermore, we discuss the optimal stopping criteria for these methods. Results show that our hyperparameter-free methods outperform the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation.",http://aclweb.org/anthology/D18-1342,D18-1,D18-1342,https://arxiv.org/abs/1808.09582,"('Yilin Yang', 'Liang Huang', 'Mingbo Ma')",10,Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation,EMNLP,2018
"Entity-Relationship (E-R) Search is a complex case of Entity Search where the goal is to search for multiple unknown entities and relationships connecting them. We assume that a E-R query can be decomposed as a sequence of sub-queries each containing keywords related to a specific entity or relationship. We adopt a probabilistic formulation of the E-R search problem. When creating specific representations for entities (e.g. context terms) and for pairs of entities (i.e. relationships) it is possible to create a graph of probabilistic dependencies between sub-queries and entity plus relationship representations. To the best of our knowledge this represents the first probabilistic model of E-R search. We propose and develop a novel supervised Early Fusion-based model for E-R search, the Entity-Relationship Dependence Model (ERDM). It uses Markov Random Field to model term dependencies of E-R sub-queries and entity/relationship documents. We performed experiments with more than 800M entities and relationships extractions from ClueWeb-09-B with FACC1 entity linking. We obtained promising results using 3 different query collections comprising 469 E-R queries, with results showing that it is possible to perform E-R search without using fix and pre-defined entity and relationship types, enabling a wide range of queries to be addressed.",http://aclweb.org/anthology/D18-1343,D18-1,D18-1343,https://arxiv.org/abs/1810.03235,"('Yadollah Yaghoobzadeh', 'Hinrich Schütze')",10,Multi-Multi-View Learning: Multilingual and Multi-Representation Entity Typing,EMNLP,2018
"Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop a neat, simple yet effective way to learn \emph{FRequency-AGnostic word Embedding} (FRAGE) using adversarial training. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.",http://aclweb.org/anthology/D18-1344,D18-1,D18-1344,https://arxiv.org/abs/1809.06858,"('Adithya Pratapa', 'Monojit Choudhury', 'Sunayana Sitaram')",10,Word Embeddings for Code-Mixed Language Processing,EMNLP,2018
"Character-level patterns have been widely used as features in English Named Entity Recognition (NER) systems. However, to date there has been no direct investigation of the inherent differences between name and non-name tokens in text, nor whether this property holds across multiple languages. This paper analyzes the capabilities of corpus-agnostic Character-level Language Models (CLMs) in the binary task of distinguishing name tokens from non-name tokens. We demonstrate that CLMs provide a simple and powerful model for capturing these differences, identifying named entity tokens in a diverse set of languages at close to the performance of full NER systems. Moreover, by adding very simple CLM-based features we can significantly improve the performance of an off-the-shelf NER system for multiple languages.",http://aclweb.org/anthology/D18-1345,D18-1,D18-1345,https://arxiv.org/abs/1809.05157,"('Xiaodong Yu', 'Stephen Mayhew', 'Mark Sammons', 'Dan Roth')",10,On the Strength of Character Language Models for Multilingual Named Entity Recognition,EMNLP,2018
This work focuses on building language models (LMs) for code-switched text. We propose two techniques that significantly improve these LMs: 1) A novel recurrent neural network unit with dual components that focus on each language in the code-switched text separately 2) Pretraining the LM using synthetic text from a generative model estimated using the training data. We demonstrate the effectiveness of our proposed techniques by reporting perplexities on a Mandarin-English task and derive significant reductions in perplexity.,http://aclweb.org/anthology/D18-1346,D18-1,D18-1346,https://arxiv.org/abs/1809.01962,"('Saurabh Garg', 'Tanmay Parekh', 'Preethi Jyothi')",10,Code-switched Language Models Using Dual RNNs and Same-Source Pretraining,EMNLP,2018
Machine Translation for Indian languages is an emerging research area. Transliteration is one such module that we design while designing a translation system. Transliteration means mapping of source language text into the target language. Simple mapping decreases the efficiency of overall translation system. We propose the use of stemming and part-of-speech tagging for transliteration. The effectiveness of translation can be improved if we use part-of-speech tagging and stemming assisted transliteration.We have shown that much of the content in Gujarati gets transliterated while being processed for translation to Hindi language.,http://aclweb.org/anthology/D18-1347,D18-1,D18-1347,https://arxiv.org/abs/1307.3310,"('Kelsey Ball', 'Dan Garrette')",10,"Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification",EMNLP,2018
"User intent detection plays a critical role in question-answering and dialog systems. Most previous works treat intent detection as a classification problem where utterances are labeled with predefined intents. However, it is labor-intensive and time-consuming to label users' utterances as intents are diversely expressed and novel intents will continually be involved. Instead, we study the zero-shot intent detection problem, which aims to detect emerging user intents where no labeled utterances are currently available. We propose two capsule-based architectures: INTENT-CAPSNET that extracts semantic features from utterances and aggregates them to discriminate existing intents, and INTENTCAPSNET-ZSL which gives INTENTCAPSNET the zero-shot learning ability to discriminate emerging intents via knowledge transfer from existing intents. Experiments on two real-world datasets show that our model not only can better discriminate diversely expressed existing intents, but is also able to discriminate emerging intents when no labeled utterances are available.",http://aclweb.org/anthology/D18-1348,D18-1,D18-1348,https://arxiv.org/abs/1809.00385,"('Congying Xia', 'Chenwei Zhang', 'Xiaohui Yan', 'Yi Chang', 'Philip Yu')",10,Zero-shot User Intent Detection via Capsule Neural Networks,EMNLP,2018
"Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where structured prediction is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.",http://aclweb.org/anthology/D18-1349,D18-1,D18-1349,https://arxiv.org/abs/1808.06161,"('Di Jin', 'Peter Szolovits')",10,Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts,EMNLP,2018
"In this study, we explore capsule networks with dynamic routing for text classification. We propose three strategies to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain ""background"" information or have not been successfully trained. A series of experiments are conducted with capsule networks on six text classification benchmarks. Capsule networks achieve state of the art on 4 out of 6 datasets, which shows the effectiveness of capsule networks for text classification. We additionally show that capsule networks exhibit significant improvement when transfer single-label to multi-label text classification over strong baseline methods. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for text modeling.",http://aclweb.org/anthology/D18-1350,D18-1,D18-1350,https://arxiv.org/abs/1804.00538,"('Min Yang', 'Wei Zhao', 'Jianbo Ye', 'Zeyang Lei', 'Zhou Zhao', 'Soufei Zhang')",10,Investigating Capsule Networks with Dynamic Routing for Text Classification,EMNLP,2018
"Many classification models work poorly on short texts due to data sparsity. To address this issue, we propose topic memory networks for short text classification with a novel topic memory mechanism to encode latent topic representations indicative of class labels. Different from most prior work that focuses on extending features with external knowledge or pre-trained topics, our model jointly explores topic inference and text classification with memory networks in an end-to-end manner. Experimental results on four benchmark datasets show that our model outperforms state-of-the-art models on short text classification, meanwhile generates coherent topics.",http://aclweb.org/anthology/D18-1351,D18-1,D18-1351,https://arxiv.org/abs/1809.03664,"('Jichuan Zeng', 'Jing Li', 'Yan Song', 'Cuiyun Gao', 'Michael R. Lyu', 'Irwin King')",10,Topic Memory Networks for Short Text Classification,EMNLP,2018
"Multi-label learning problems have manifested themselves in various machine learning applications. The key to successful multi-label learning algorithms lies in the exploration of inter-label correlations, which usually incur great computational cost. Another notable factor in multi-label learning is that the label vectors are usually extremely sparse, especially when the candidate label vocabulary is very large and only a few instances are assigned to each category. Recently, a label space transformation (LST) framework has been proposed targeting these challenges. However, current methods based on LST usually suffer from information loss in the label space dimension reduction process and fail to address the sparsity problem effectively. In this paper, we propose a distribution-based label space transformation (DLST) model. By defining the distribution based on the similarity of label vectors, a more comprehensive label structure can be captured. Then, by minimizing KL-divergence of two distributions, the information of the original label space can be approximately preserved in the latent space. Consequently, multi-label classifier trained using the dense latent codes yields better performance. The leverage of distribution enables DLST to fill out additional information about the label correlations. This endows DLST the capability to handle label set sparsity and training data sparsity in multi-label learning problems. With the optimal latent code, a kernel logistic regression function is learned for the mapping from feature space to the latent space. Then ML-KNN is employed to recover the original label vector from the transformed latent code. Extensive experiments on several benchmark datasets demonstrate that DLST not only achieves high classification performance but also is computationally more efficient.",http://aclweb.org/anthology/D18-1352,D18-1,D18-1352,https://arxiv.org/abs/1805.05687,"('Anthony Rios', 'Ramakanth Kavuluru')",10,Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces,EMNLP,2018
"With the recent advances of neural models and natural language processing, automatic generation of classical Chinese poetry has drawn significant attention due to its artistic and cultural value. Previous works mainly focus on generating poetry given keywords or other text information, while visual inspirations for poetry have been rarely explored. Generating poetry from images is much more challenging than generating poetry from text, since images contain very rich visual information which cannot be described completely using several keywords, and a good poem should convey the image accurately. In this paper, we propose a memory based neural model which exploits images to generate poems. Specifically, an Encoder-Decoder model with a topic memory network is proposed to generate classical Chinese poetry from images. To the best of our knowledge, this is the first work attempting to generate classical Chinese poetry from images with neural networks. A comprehensive experimental investigation with both human evaluation and quantitative analysis demonstrates that the proposed model can generate poems which convey images accurately.",http://aclweb.org/anthology/D18-1353,D18-1,D18-1353,https://arxiv.org/abs/1803.02994,"('Xiaoyuan Yi', 'Maosong Sun', 'Ruoyu Li', 'Wenhao Li')",10,Automatic Poetry Generation with Mutual Reinforcement Learning,EMNLP,2018
"Autoregressive decoding is the only part of sequence-to-sequence models that prevents them from massive parallelization at inference time. Non-autoregressive models enable the decoder to generate all output symbols independently in parallel. We present a novel non-autoregressive architecture based on connectionist temporal classification and evaluate it on the task of neural machine translation. Unlike other non-autoregressive methods which operate in several steps, our model can be trained end-to-end. We conduct experiments on the WMT English-Romanian and English-German datasets. Our models achieve a significant speedup over the autoregressive models, keeping the translation quality comparable to other non-autoregressive models.",http://aclweb.org/anthology/D18-1354,D18-1,D18-1354,https://arxiv.org/abs/1811.04719,"('Jiachen Du', 'Wenjie Li', 'Yulan He', 'Ruifeng Xu', 'Lidong Bing', 'Xuan Wang')",10,Variational Autoregressive Decoder for Neural Response Generation,EMNLP,2018
"Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from ma- chine translation studies and implicitly learned simplification mapping rules from normal- simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we pro- pose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state- of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at https://github.com/ Sanqiang/text_simplification.",http://aclweb.org/anthology/D18-1355,D18-1,D18-1355,https://arxiv.org/abs/1810.11193,"('Sanqiang Zhao', 'Rui Meng', 'Daqing He', 'Andi Saptono', 'Bambang Parmanto')",10,Integrating Transformer and Paraphrase Rules for Sentence Simplification,EMNLP,2018
"While neural, encoder-decoder models have had significant empirical success in text generation, there remain several unaddressed problems with this style of generation. Encoder-decoder models are largely (a) uninterpretable, and (b) difficult to control in terms of their phrasing or content. This work proposes a neural generation system using a hidden semi-markov model (HSMM) decoder, which learns latent, discrete templates jointly with learning to generate. We show that this model learns useful templates, and that these templates make generation both more interpretable and controllable. Furthermore, we show that this approach scales to real data sets and achieves strong performance nearing that of encoder-decoder text generation models.",http://aclweb.org/anthology/D18-1356,D18-1,D18-1356,https://arxiv.org/abs/1808.10122,"('Sam Wiseman', 'Stuart Shieber', 'Alexander Rush')",10,Learning Neural Templates for Text Generation,EMNLP,2018
"We propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. These models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. We also discover that attention-based neural machine translation is well suited for pronoun prediction and compares favorably with other approaches that were specifically designed for this task.",http://aclweb.org/anthology/D18-1357,D18-1,D18-1357,https://arxiv.org/abs/1704.05135,"('Renjie Zheng', 'Mingbo Ma', 'Liang Huang')",10,Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation,EMNLP,2018
"Populating ontology graphs represents a long-standing problem for the Semantic Web community. Recent advances in translation-based graph embedding methods for populating instance-level knowledge graphs lead to promising new approaching for the ontology population problem. However, unlike instance-level graphs, the majority of relation facts in ontology graphs come with comprehensive semantic relations, which often include the properties of transitivity and symmetry, as well as hierarchical relations. These comprehensive relations are often too complex for existing graph embedding methods, and direct application of such methods is not feasible. Hence, we propose On2Vec, a novel translation-based graph embedding method for ontology population. On2Vec integrates two model components that effectively characterize comprehensive relation facts in ontology graphs. The first is the Component-specific Model that encodes concepts and relations into low-dimensional embedding spaces without a loss of relational properties; the second is the Hierarchy Model that performs focused learning of hierarchical relation facts. Experiments on several well-known ontology graphs demonstrate the promising capabilities of On2Vec in predicting and verifying new relation facts. These promising results also make possible significant improvements in related methods.",http://aclweb.org/anthology/D18-1358,D18-1,D18-1358,https://arxiv.org/abs/1809.02382,"('Zhao Zhang', 'Fuzhen Zhuang', 'Meng Qu', 'Fen Lin', 'Qing He')",10,Knowledge Graph Embedding with Hierarchical Relation Structure,EMNLP,2018
"Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in knowledge bases, such as text, images, and numerical values. In this paper, we propose multimodal knowledge base embeddings (MKBE) that use different neural encoders for this variety of observed data, and combine them with existing relational models to learn embeddings of the entities and multimodal data. Further, using these learned embedings and different neural decoders, we introduce a novel multimodal imputation model to generate missing multimodal values, like text and images, from information in the knowledge base. We enrich existing relational datasets to create two novel benchmarks that contain additional information such as textual descriptions and images of the original entities. We demonstrate that our models utilize this additional information effectively to provide more accurate link prediction, achieving state-of-the-art results with a considerable gap of 5-7% over existing methods. Further, we evaluate the quality of our generated multimodal values via a user study. We have release the datasets and the open-source implementation of our models at https://github.com/pouyapez/mkbe",http://aclweb.org/anthology/D18-1359,D18-1,D18-1359,https://arxiv.org/abs/1809.01341,"('Pouya Pezeshkpour', 'Liyan Chen', 'Sameer Singh')",10,Embedding Multimodal Relational Data for Knowledge Base Completion,EMNLP,2018
"We introduce a multi-task setup of identifying and classifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called Scientific Information Extractor (SciIE) for with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.",http://aclweb.org/anthology/D18-1360,D18-1,D18-1360,https://arxiv.org/abs/1808.09602,"('Yi Luan', 'Luheng He', 'Mari Ostendorf', 'Hannaneh Hajishirzi')",10,"Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",EMNLP,2018
"The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects. Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisy-free simulation environment.",http://aclweb.org/anthology/D18-1361,D18-1,D18-1361,https://arxiv.org/abs/1808.07645,"('Huang Hu', 'Xianchao Wu', 'Bingfeng Luo', 'Chongyang Tao', 'Can Xu', 'wei wu', 'Zhan Chen')",10,Playing 20 Question Game with Policy-Based Reinforcement Learning,EMNLP,2018
"Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.",http://aclweb.org/anthology/D18-1362,D18-1,D18-1362,https://arxiv.org/abs/1808.10568,"('Xi Victoria Lin', 'Richard Socher', 'Caiming Xiong')",10,Multi-Hop Knowledge Graph Reasoning with Reward Shaping,EMNLP,2018
"Inductive learning is based on inferring a general rule from a finite data set and using it to label new data. In transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points, which are given to the learner prior to learning. Although transduction seems at the outset to be an easier task than induction, there have not been many provably useful algorithms for transduction. Moreover, the precise relation between induction and transduction has not yet been determined. The main theoretical developments related to transduction were presented by Vapnik more than twenty years ago. One of Vapnik's basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail. While tight, this bound is given implicitly via a computational routine. Our first contribution is a somewhat looser but explicit characterization of a slightly extended PAC-Bayesian version of Vapnik's transductive bound. This characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement. We then derive error bounds for compression schemes such as (transductive) support vector machines and for transduction algorithms based on clustering. The main observation used for deriving these new error bounds and algorithms is that the unlabeled test points, which in the transductive setting are known in advance, can be used in order to construct useful data dependent prior distributions over the hypothesis space.",http://aclweb.org/anthology/D18-1363,D18-1,D18-1363,https://arxiv.org/abs/1107.0046,"('Katharina Kann', 'Hinrich Schütze')",10,Neural Transductive Learning and Beyond: Morphological Generation in the Minimal-Resource Setting,EMNLP,2018
A new scheme to represent phonological changes during continuous speech recognition is suggested. A phonological tag coupled with its morphological tag is designed to represent the conditions of Korean phonological changes. A pairwise language model of these morphological and phonological tags is implemented in Korean speech recognition system. Performance of the model is verified through the TDNN-based speech recognition experiments.,http://aclweb.org/anthology/D18-1364,D18-1,D18-1364,https://arxiv.org/abs/cmp-lg/9607023,['Giorgio Magri'],10,Implicational Universals in Stochastic Constraint-Based Phonology,EMNLP,2018
"Character-level features are currently used in different neural network-based natural language processing algorithms. However, little is known about the character-level patterns those models learn. Moreover, models are often compared only quantitatively while a qualitative analysis is missing. In this paper, we investigate which character-level patterns neural networks learn and if those patterns coincide with manually-defined word segmentations and annotations. To that end, we extend the contextual decomposition technique (Murdoch et al. 2018) to convolutional neural networks which allows us to compare convolutional neural networks and bidirectional long short-term memory networks. We evaluate and compare these models for the task of morphological tagging on three morphologically different languages and show that these models implicitly discover understandable linguistic rules. Our implementation can be found at https://github.com/FredericGodin/ContextualDecomposition-NLP .",http://aclweb.org/anthology/D18-1365,D18-1,D18-1365,https://arxiv.org/abs/1808.09551,"('Fréderic Godin', 'Kris Demuynck', 'Joni Dambre', 'Wesley De Neve', 'Thomas Demeester')",10,Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?,EMNLP,2018
"Much work in Natural Language Processing (NLP) has been for resource-rich languages, making generalization to new, less-resourced languages challenging. We present two approaches for improving generalization to low-resourced languages by adapting continuous word representations using linguistically motivated subword units: phonemes, morphemes and graphemes. Our method requires neither parallel corpora nor bilingual dictionaries and provides a significant gain in performance over previous methods relying on these resources. We demonstrate the effectiveness of our approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU.",http://aclweb.org/anthology/D18-1366,D18-1,D18-1366,https://arxiv.org/abs/1808.09500,"('Aditi Chaudhary', 'Chunting Zhou', 'Lori Levin', 'Graham Neubig', 'David R. Mortensen', 'Jaime Carbonell')",10,Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations,EMNLP,2018
"Facial caricature is an art form of drawing faces in an exaggerated way to convey humor or sarcasm. In this paper, we propose the first Generative Adversarial Network (GAN) for unpaired photo-to-caricature translation, which we call ""CariGANs"". It explicitly models geometric exaggeration and appearance stylization using two components: CariGeoGAN, which only models the geometry-to-geometry transformation from face photos to caricatures, and CariStyGAN, which transfers the style appearance from caricatures to face photos without any geometry deformation. In this way, a difficult cross-domain translation problem is decoupled into two easier tasks. The perceptual study shows that caricatures generated by our CariGANs are closer to the hand-drawn ones, and at the same time better persevere the identity, compared to state-of-the-art methods. Moreover, our CariGANs allow users to control the shape exaggeration degree and change the color/texture style by tuning the parameters or giving an example caricature.",http://aclweb.org/anthology/D18-1367,D18-1,D18-1367,https://arxiv.org/abs/1811.00222,"('Enrica Troiano', 'Carlo Strapparava', 'Gözde Özbal', 'Serra Sinem Tekiroglu')",10,A Computational Exploration of Exaggeration,EMNLP,2018
"Capabilities to categorize a clause based on the type of situation entity (e.g., events, states and generic statements) the clause introduces to the discourse can benefit many NLP applications. Observing that the situation entity type of a clause depends on discourse functions the clause plays in a paragraph and the interpretation of discourse functions depends heavily on paragraph-wide contexts, we propose to build context-aware clause representations for predicting situation entity types of clauses. Specifically, we propose a hierarchical recurrent neural network model to read a whole paragraph at a time and jointly learn representations for all the clauses in the paragraph by extensively modeling context influences and inter-dependencies of clauses. Experimental results show that our model achieves the state-of-the-art performance for clause-level situation entity classification on the genre-rich MASC+Wiki corpus, which approaches human-level performance.",http://aclweb.org/anthology/D18-1368,D18-1,D18-1368,https://arxiv.org/abs/1809.07483,"('Zeyu Dai', 'Ruihong Huang')",10,Building Context-aware Clause Representations for Situation Entity Type Classification,EMNLP,2018
"People are increasingly relying on the Web and social media to find solutions to their problems in a wide range of domains. In this online setting, closely related problems often lead to the same characteristic learning pattern, in which people sharing these problems visit related pieces of information, perform almost identical queries or, more generally, take a series of similar actions. In this paper, we introduce a novel modeling framework for clustering continuous-time grouped streaming data, the hierarchical Dirichlet Hawkes process (HDHP), which allows us to automatically uncover a wide variety of learning patterns from detailed traces of learning activity. Our model allows for efficient inference, scaling to millions of actions taken by thousands of users. Experiments on real data gathered from Stack Overflow reveal that our framework can recover meaningful learning patterns in terms of both content and temporal dynamics, as well as accurately track users' interests and goals over time.",http://aclweb.org/anthology/D18-1369,D18-1,D18-1369,https://arxiv.org/abs/1610.05775,"('Yeon Seonwoo', 'Alice Oh', 'Sungjoon Park')",10,Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain,EMNLP,2018
"The goal of this research was to find a way to extend the capabilities of computers through the processing of language in a more human way, and present applications which demonstrate the power of this method. This research presents a novel approach, Rhetorical Analysis, to solving problems in Natural Language Processing (NLP). The main benefit of Rhetorical Analysis, as opposed to previous approaches, is that it does not require the accumulation of large sets of training data, but can be used to solve a multitude of problems within the field of NLP. The NLP problems investigated with Rhetorical Analysis were the Author Identification problem - predicting the author of a piece of text based on its rhetorical strategies, Election Prediction - predicting the winner of a presidential candidate's re-election campaign based on rhetorical strategies within that president's inaugural address, Natural Language Generation - having a computer produce text containing rhetorical strategies, and Document Summarization. The results of this research indicate that an Author Identification system based on Rhetorical Analysis could predict the correct author 100% of the time, that a re-election predictor based on Rhetorical Analysis could predict the correct winner of a re-election campaign 55% of the time, that a Natural Language Generation system based on Rhetorical Analysis could output text with up to 87.3% similarity to Shakespeare in style, and that a Document Summarization system based on Rhetorical Analysis could extract highly relevant sentences. Overall, this study demonstrated that Rhetorical Analysis could be a useful approach to solving problems in NLP.",http://aclweb.org/anthology/D18-1370,D18-1,D18-1370,https://arxiv.org/abs/1301.3547,"('Anne Lauscher', 'Goran Glavaš', 'Simone Paolo Ponzetto', 'Kai Eckert')",10,Investigating the Role of Argumentation in the Rhetorical Analysis of Scientific Publications with Neural Multi-Task Learning Models,EMNLP,2018
"We design and build the first neural temporal dependency parser. It utilizes a neural ranking model with minimal feature engineering, and parses time expressions and events in a text into a temporal dependency tree structure. We evaluate our parser on two domains: news reports and narrative stories. In a parsing-only evaluation setup where gold time expressions and events are provided, our parser reaches 0.81 and 0.70 f-score on unlabeled and labeled parsing respectively, a result that is very competitive against alternative approaches. In an end-to-end evaluation setup where time expressions and events are automatically recognized, our parser beats two strong baselines on both data domains. Our experimental results and discussions shed light on the nature of temporal dependency structures in different domains and provide insights that we believe will be valuable to future research in this area.",http://aclweb.org/anthology/D18-1371,D18-1,D18-1371,https://arxiv.org/abs/1809.00370,"('Yuchen Zhang', 'Nianwen Xue')",10,Neural Ranking Models for Temporal Dependency Structure Parsing,EMNLP,2018
"Understanding causal explanations - reasons given for happenings in one's life - has been found to be an important psychological factor linked to physical and mental health. Causal explanations are often studied through manual identification of phrases over limited samples of personal writing. Automatic identification of causal explanations in social media, while challenging in relying on contextual and sequential cues, offers a larger-scale alternative to expensive manual ratings and opens the door for new applications (e.g. studying prevailing beliefs about causes, such as climate change). Here, we explore automating causal explanation analysis, building on discourse parsing, and presenting two novel subtasks: causality detection (determining whether a causal explanation exists at all) and causal explanation identification (identifying the specific phrase that is the explanation). We achieve strong accuracies for both tasks but find different approaches best: an SVM for causality prediction (F1 = 0.791) and a hierarchy of Bidirectional LSTMs for causal explanation identification (F1 = 0.853). Finally, we explore applications of our complete pipeline (F1 = 0.868), showing demographic differences in mentions of causal explanation and that the association between a word and sentiment can change when it is used within a causal explanation.",http://aclweb.org/anthology/D18-1372,D18-1,D18-1372,https://arxiv.org/abs/1809.01202,"('Youngseo Son', 'Nipun Bayas', 'H. Andrew Schwartz')",10,Causal Explanation Analysis on Social Media,EMNLP,2018
"Multimodal learning has shown promising performance in content-based recommendation due to the auxiliary user and item information of multiple modalities such as text and images. However, the problem of incomplete and missing modality is rarely explored and most existing methods fail in learning a recommendation model with missing or corrupted modalities. In this paper, we propose LRMM, a novel framework that mitigates not only the problem of missing modalities but also more generally the cold-start problem of recommender systems. We propose modality dropout (m-drop) and a multimodal sequential autoencoder (m-auto) to learn multimodal representations for complementing and imputing missing modalities. Extensive experiments on real-world Amazon data show that LRMM achieves state-of-the-art performance on rating prediction tasks. More importantly, LRMM is more robust to previous methods in alleviating data-sparsity and the cold-start problem.",http://aclweb.org/anthology/D18-1373,D18-1,D18-1373,https://arxiv.org/abs/1808.06791,"('Cheng Wang', 'Mathias Niepert', 'Hui Li')",10,LRMM: Learning to Recommend with Missing Modalities,EMNLP,2018
"Writer identification from musical score documents is a challenging task due to its inherent problem of overlapping of musical symbols with staff lines. Most of the existing works in the literature of writer identification in musical score documents were performed after a preprocessing stage of staff lines removal. In this paper we propose a novel writer identification framework in musical documents without removing staff lines from documents. In our approach, Hidden Markov Model has been used to model the writing style of the writers without removing staff lines. The sliding window features are extracted from musical score lines and they are used to build writer specific HMM models. Given a query musical sheet, writer specific confidence for each musical line is returned by each writer specific model using a loglikelihood score. Next, a loglikelihood score in page level is computed by weighted combination of these scores from the corresponding line images of the page. A novel Factor Analysis based feature selection technique is applied in sliding window features to reduce the noise appearing from staff lines which proves efficiency in writer identification performance.In our framework we have also proposed a novel score line detection approach in musical sheet using HMM. The experiment has been performed in CVC-MUSCIMA dataset and the results obtained that the proposed approach is efficient for score line detection and writer identification without removing staff lines. To get the idea of computation time of our method, detail analysis of execution time is also provided.",http://aclweb.org/anthology/D18-1374,D18-1,D18-1374,https://arxiv.org/abs/1707.06828,"('Michal Lukasik', 'Richard Zens')",10,Content Explorer: Recommending Novel Entities for a Document Writer,EMNLP,2018
"The existing literature provides evidence that limit order book data can be used to predict short-term price movements in stock markets. This paper proposes a new neural network architecture for predicting return jump arrivals in equity markets with high-frequency limit order book data. This new architecture, based on Convolutional Long Short-Term Memory with Attention, is introduced to apply time series representation learning with memory and to focus the prediction attention on the most important features to improve performance. The data set consists of order book data on five liquid U.S. stocks. The use of the attention mechanism makes it possible to analyze the importance of the inclusion limit order book data and other input variables. By using this mechanism, we provide evidence that the use of limit order book data was found to improve the performance of the proposed model in jump prediction, either clearly or marginally, depending on the underlying stock. This suggests that path-dependence in limit order book markets is a stock specific feature. Moreover, we find that the proposed approach with an attention mechanism outperforms the multi-layer perceptron network as well as the convolutional neural network and Long Short-Term memory model.",http://aclweb.org/anthology/D18-1375,D18-1,D18-1375,https://arxiv.org/abs/1810.10845,"('Suraj Maharjan', 'Manuel Montes', 'Fabio A. González', 'Thamar Solorio')",10,A Genre-Aware Attention Model to Improve the Likability Prediction of Books,EMNLP,2018
"We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests.",http://aclweb.org/anthology/D18-1376,D18-1,D18-1376,https://arxiv.org/abs/1606.03667,"('Hou Pong Chan', 'Irwin King')",10,Thread Popularity Prediction and Tracking with a Permutation-invariant Model,EMNLP,2018
"Machine learning approaches in sentiment analysis principally rely on the abundance of resources. To limit this dependence, we propose a novel method called Siamese Network Architecture for Sentiment Analysis (SNASA) to learn representations of resource-poor languages by jointly training them with resource-rich languages using a siamese network.   SNASA model consists of twin Bi-directional Long Short-Term Memory Recurrent Neural Networks (Bi-LSTM RNN) with shared parameters joined by a contrastive loss function, based on a similarity metric. The model learns the sentence representations of resource-poor and resource-rich language in a common sentiment space by using a similarity metric based on their individual sentiments. The model, hence, projects sentences with similar sentiment closer to each other and the sentences with different sentiment farther from each other. Experiments on large-scale datasets of resource-rich languages - English and Spanish and resource-poor languages - Hindi and Telugu reveal that SNASA outperforms the state-of-the-art sentiment analysis approaches based on distributional semantics, semantic rules, lexicon lists and deep neural network representations without sh",http://aclweb.org/anthology/D18-1377,D18-1,D18-1377,https://arxiv.org/abs/1804.00805,"('Navonil Majumder', 'Soujanya Poria', 'Alexander Gelbukh', 'Md Shad Akhtar', 'Erik Cambria', 'Asif Ekbal')",10,IARM: Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based Sentiment Analysis,EMNLP,2018
"Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words. Given reviews from different domains, some existing methods for word embeddings exploit sentiment information, but they cannot produce domain-sensitive embeddings. On the other hand, some other existing methods can generate domain-sensitive word embeddings, but they cannot distinguish words with similar contexts but opposite sentiment polarity. We propose a new method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words. Our method can automatically determine and produce domain-common embeddings and domain-specific embeddings. The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time. Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level.",http://aclweb.org/anthology/D18-1378,D18-1,D18-1378,https://arxiv.org/abs/1805.03801,"('Zhe Zhang', 'Munindar Singh')",10,Limbic: Author-Based Sentiment Aspect Modeling Regularized with Word Embeddings and Discourse Relations,EMNLP,2018
"An image is a very effective tool for conveying emotions. Many researchers have investigated in computing the image emotions by using various features extracted from images. In this paper, we focus on two high level features, the object and the background, and assume that the semantic information of images is a good cue for predicting emotion. An object is one of the most important elements that define an image, and we find out through experiments that there is a high correlation between the object and the emotion in images. Even with the same object, there may be slight difference in emotion due to different backgrounds, and we use the semantic information of the background to improve the prediction performance. By combining the different levels of features, we build an emotion based feed forward deep neural network which produces the emotion values of a given image. The output emotion values in our framework are continuous values in the 2-dimensional space (Valence and Arousal), which are more effective than using a few number of emotion categories in describing emotions. Experiments confirm the effectiveness of our network in predicting the emotion of images.",http://aclweb.org/anthology/D18-1379,D18-1,D18-1379,https://arxiv.org/abs/1705.07543,"('Yang Yang', 'Deyu ZHOU', 'Yulan He')",10,An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking,EMNLP,2018
"Deep learning approaches for sentiment classification do not fully exploit sentiment linguistic knowledge. In this paper, we propose a Multi-sentiment-resource Enhanced Attention Network (MEAN) to alleviate the problem by integrating three kinds of sentiment linguistic knowledge (e.g., sentiment lexicon, negation words, intensity words) into the deep neural network via attention mechanisms. By using various types of sentiment resources, MEAN utilizes sentiment-relevant information from different representation subspaces, which makes it more effective to capture the overall semantics of the sentiment, negation and intensity words for sentiment prediction. The experimental results demonstrate that MEAN has robust superiority over strong competitors.",http://aclweb.org/anthology/D18-1380,D18-1,D18-1380,https://arxiv.org/abs/1807.04990,"('Feifan Fan', 'Yansong Feng', 'Dongyan Zhao')",10,Multi-grained Attention Network for Aspect-Level Sentiment Classification,EMNLP,2018
"The rise of social media such as blogs and social networks has fueled interest in sentiment analysis. With the proliferation of reviews, ratings, recommendations and other forms of online expression, online opinion has turned into a kind of virtual currency for businesses looking to market their products, identify new opportunities and manage their reputations, therefore many are now looking to the field of sentiment analysis. In this paper, we present a feature-based sentence level approach for Arabic sentiment analysis. Our approach is using Arabic idioms/saying phrases lexicon as a key importance for improving the detection of the sentiment polarity in Arabic sentences as well as a number of novels and rich set of linguistically motivated features contextual Intensifiers, contextual Shifter and negation handling), syntactic features for conflicting phrases which enhance the sentiment classification accuracy. Furthermore, we introduce an automatic expandable wide coverage polarity lexicon of Arabic sentiment words. The lexicon is built with gold-standard sentiment words as a seed which is manually collected and annotated and it expands and detects the sentiment orientation automatically of new sentiment words using synset aggregation technique and free online Arabic lexicons and thesauruses. Our data focus on modern standard Arabic (MSA) and Egyptian dialectal Arabic tweets and microblogs (hotel reservation, product reviews, etc.). The experimental results using our resources and techniques with SVM classifier indicate high performance levels, with accuracies of over 95%.",http://aclweb.org/anthology/D18-1381,D18-1,D18-1381,https://arxiv.org/abs/1505.03105,"('Yi Tay', 'Anh Tuan Luu', 'Siu Cheung Hui', 'Jian Su')",10,Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for Sentiment Classification,EMNLP,2018
"The use of user/product information in sentiment analysis is important, especially for cold-start users/products, whose number of reviews are very limited. However, current models do not deal with the cold-start problem which is typical in review websites. In this paper, we present Hybrid Contextualized Sentiment Classifier (HCSC), which contains two modules: (1) a fast word encoder that returns word vectors embedded with short and long range dependency features; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism that considers the existence of cold-start problem when attentively pooling the encoded word vectors. HCSC introduces shared vectors that are constructed from similar users/products, and are used when the original distinct vectors do not have sufficient information (i.e. cold-start). This is decided by a frequency-guided selective gate vector. Our experiments show that in terms of RMSE, HCSC performs significantly better when compared with on famous datasets, despite having less complexity, and thus can be trained much faster. More importantly, our model performs significantly better than previous models when the training data is sparse and has cold-start problems.",http://aclweb.org/anthology/D18-1382,D18-1,D18-1382,https://arxiv.org/abs/1806.05507,"('Deepanway Ghosal', 'Md Shad Akhtar', 'Dushyant Chauhan', 'Soujanya Poria', 'Asif Ekbal', 'Pushpak Bhattacharyya')",10,Contextual Inter-modal Attention for Multi-modal Sentiment Analysis,EMNLP,2018
"Most of existing work learn sentiment-specific word representation for improving Twitter sentiment classification, which encoded both n-gram and distant supervised tweet sentiment information in learning process. They assume all words within a tweet have the same sentiment polarity as the whole tweet, which ignores the word its own sentiment polarity. To address this problem, we propose to learn sentiment-specific word embedding by exploiting both lexicon resource and distant supervised information. We develop a multi-level sentiment-enriched word embedding learning method, which uses parallel asymmetric neural network to model n-gram, word level sentiment and tweet level sentiment in learning process. Experiments on standard benchmarks show our approach outperforms state-of-the-art methods.",http://aclweb.org/anthology/D18-1383,D18-1,D18-1383,https://arxiv.org/abs/1611.00126,"('Ruidan He', 'Wee Sun Lee', 'Hwee Tou Ng', 'Daniel Dahlmeier')",10,Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification,EMNLP,2018
"Text is the main method of communicating information in the digital age. Messages, blogs, news articles, reviews, and opinionated information abound on the Internet. People commonly purchase products online and post their opinions about purchased items. This feedback is displayed publicly to assist others with their purchasing decisions, creating the need for a mechanism with which to extract and summarize useful information for enhancing the decision-making process. Our contribution is to improve the accuracy of extraction by combining different techniques from three major areas, named Data Mining, Natural Language Processing techniques and Ontologies. The proposed framework sequentially mines products aspects and users opinions, groups representative aspects by similarity, and generates an output summary. This paper focuses on the task of extracting product aspects and users opinions by extracting all possible aspects and opinions from reviews using natural language, ontology, and frequent (tag) sets. The proposed framework, when compared with an existing baseline model, yielded promising results.",http://aclweb.org/anthology/D18-1384,D18-1,D18-1384,https://arxiv.org/abs/1404.1982,"('Zhiyi Luo', 'Shanshan Huang', 'Frank F. Xu', 'Bill Yuchen Lin', 'Hanyuan Shi', 'Kenny Zhu')",10,ExtRA: Extracting Prominent Review Aspects from Customer Feedback,EMNLP,2018
"With the increasing popularity of smart devices, rumors with multimedia content become more and more common on social networks. The multimedia information usually makes rumors look more convincing. Therefore, finding an automatic approach to verify rumors with multimedia content is a pressing task. Previous rumor verification research only utilizes multimedia as input features. We propose not to use the multimedia content but to find external information in other news platforms pivoting on it. We introduce a new features set, cross-lingual cross-platform features that leverage the semantic similarity between the rumors and the external information. When implemented, machine learning methods utilizing such features achieved the state-of-the-art rumor verification results.",http://aclweb.org/anthology/D18-1385,D18-1,D18-1385,https://arxiv.org/abs/1808.04911,"('Weiming Wen', 'Songwen Su', 'Zhou Yu')",10,Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content,EMNLP,2018
"We introduce an adversarial method for producing high-recall explanations of neural text classifier decisions. Building on an existing architecture for extractive explanations via hard attention, we add an adversarial layer which scans the residual of the attention for remaining predictive signal. Motivated by the important domain of detecting personal attacks in social media comments, we additionally demonstrate the importance of manually setting a semantically appropriate `default' behavior for the model by explicitly manipulating its bias term. We develop a validation set of human-annotated personal attacks to evaluate the impact of these changes.",http://aclweb.org/anthology/D18-1386,D18-1,D18-1386,https://arxiv.org/abs/1809.01499,"('Samuel Carton', 'Qiaozhu Mei', 'Paul Resnick')",10,Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts,EMNLP,2018
"Website privacy policies represent the single most important source of information for users to gauge how their personal data are collected, used and shared by companies. However, privacy policies are often vague and people struggle to understand the content. Their opaqueness poses a significant challenge to both users and policy regulators. In this paper, we seek to identify vague content in privacy policies. We construct the first corpus of human-annotated vague words and sentences and present empirical studies on automatic vagueness detection. In particular, we investigate context-aware and context-agnostic models for predicting vague words, and explore auxiliary-classifier generative adversarial networks for characterizing sentence vagueness. Our experimental results demonstrate the effectiveness of proposed approaches. Finally, we provide suggestions for resolving vagueness and improving the usability of privacy policies.",http://aclweb.org/anthology/D18-1387,D18-1,D18-1387,https://arxiv.org/abs/1808.06219,"('Logan Lebanoff', 'Fei Liu')",10,Automatic Detection of Vague Words and Sentences in Privacy Policies,EMNLP,2018
"A news article's title, content and link structure often reveal its political ideology. However, most existing works on automatic political ideology detection only leverage textual cues. Drawing inspiration from recent advances in neural inference, we propose a novel attention based multi-view model to leverage cues from all of the above views to identify the ideology evinced by a news article. Our model draws on advances in representation learning in natural language processing and network science to capture cues from both textual content and the network structure of news articles. We empirically evaluate our model against a battery of baselines and show that our model outperforms state of the art by 10 percentage points F1 score.",http://aclweb.org/anthology/D18-1388,D18-1,D18-1388,https://arxiv.org/abs/1809.03485,"('Vivek Kulkarni', 'Junting Ye', 'Steve Skiena', 'William Yang Wang')",10,Multi-view Models for Political Ideology Detection of News Articles,EMNLP,2018
"We present a study on predicting the factuality of reporting and bias of news media. While previous work has focused on studying the veracity of claims or documents, here we are interested in characterizing entire news media. These are under-studied but arguably important research problems, both in their own right and as a prior for fact-checking systems. We experiment with a large list of news websites and with a rich set of features derived from (i) a sample of articles from the target news medium, (ii) its Wikipedia page, (iii) its Twitter account, (iv) the structure of its URL, and (v) information about the Web traffic it attracts. The experimental results show sizable performance gains over the baselines, and confirm the importance of each feature type.",http://aclweb.org/anthology/D18-1389,D18-1,D18-1389,https://arxiv.org/abs/1810.01765,"('Ramy Baly', 'Georgi Karadzhov', 'Dimitar Alexandrov', 'James Glass', 'Preslav Nakov')",10,Predicting Factuality of Reporting and Bias of News Media Sources,EMNLP,2018
"Automatic judgment prediction aims to predict the judicial results based on case materials. It has been studied for several decades mainly by lawyers and judges, considered as a novel and prospective application of artificial intelligence techniques in the legal field. Most existing methods follow the text classification framework, which fails to model the complex interactions among complementary case materials. To address this issue, we formalize the task as Legal Reading Comprehension according to the legal scenario. Following the working protocol of human judges, LRC predicts the final judgment results based on three types of information, including fact description, plaintiffs' pleas, and law articles. Moreover, we propose a novel LRC model, AutoJudge, which captures the complex semantic interactions among facts, pleas, and laws. In experiments, we construct a real-world civil case dataset for LRC. Experimental results on this dataset demonstrate that our model achieves significant improvement over state-of-the-art models. We will publish all source codes and datasets of this work on \urlgithub.com for further research.",http://aclweb.org/anthology/D18-1390,D18-1,D18-1390,https://arxiv.org/abs/1809.06537,"('Haoxi Zhong', 'Guo Zhipeng', 'Cunchao Tu', 'Chaojun Xiao', 'Zhiyuan Liu', 'Maosong Sun')",10,Legal Judgment Prediction via Topological Learning,EMNLP,2018
"Existing work on automated hate speech detection typically focuses on binary classification or on differentiating among a small set of categories. In this paper, we propose a novel method on a fine-grained hate speech classification task, which focuses on differentiating among 40 hate groups of 13 different hate group categories. We first explore the Conditional Variational Autoencoder (CVAE) as a discriminative model and then extend it to a hierarchical architecture to utilize the additional hate category information for more accurate prediction. Experimentally, we show that incorporating the hate category information for training can significantly improve the classification performance and our proposed model outperforms commonly-used discriminative models.",http://aclweb.org/anthology/D18-1391,D18-1,D18-1391,https://arxiv.org/abs/1809.00088,"('Jing Qian', 'Mai ElSherief', 'Elizabeth Belding', 'William Yang Wang')",10,Hierarchical CVAE for Fine-Grained Hate Speech Classification,EMNLP,2018
"Predictive models over social media language have shown promise in capturing community outcomes, but approaches thus far largely neglect the socio-demographic context (e.g. age, education rates, race) of the community from which the language originates. For example, it may be inaccurate to assume people in Mobile, Alabama, where the population is relatively older, will use words the same way as those from San Francisco, where the median age is younger with a higher rate of college education. In this paper, we present residualized factor adaptation, a novel approach to community prediction tasks which both (a) effectively integrates community attributes, as well as (b) adapts linguistic features to community attributes (factors). We use eleven demographic and socioeconomic attributes, and evaluate our approach over five different community-level predictive tasks, spanning health (heart disease mortality, percent fair/poor health), psychology (life satisfaction), and economics (percent housing price increase, foreclosure rate). Our evaluation shows that residualized factor adaptation significantly improves 4 out of 5 community-level outcome predictions over prior state-of-the-art for incorporating socio-demographic contexts.",http://aclweb.org/anthology/D18-1392,D18-1,D18-1392,https://arxiv.org/abs/1808.09479,"('Mohammadzaman Zamani', 'H. Andrew Schwartz', 'Veronica Lynn', 'Salvatore Giorgi', 'Niranjan Balasubramanian')",10,Residualized Factor Adaptation for Community Social Media Prediction Tasks,EMNLP,2018
"Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and ""fake news'"". Here, we draw on two concepts from the political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.",http://aclweb.org/anthology/D18-1393,D18-1,D18-1393,https://arxiv.org/abs/1808.09386,"('Anjalie Field', 'Doron Kliger', 'Shuly Wintner', 'Jennifer Pan', 'Dan Jurafsky', 'Yulia Tsvetkov')",10,Framing and Agenda-setting in Russian News: a Computational Analysis of Intricate Political Strategies,EMNLP,2018
"Vlogs provide a rich public source of data in a novel setting. This paper examined the continuous sentiment styles employed in 27,333 vlogs using a dynamic intra-textual approach to sentiment analysis. Using unsupervised clustering, we identified seven distinct continuous sentiment trajectories characterized by fluctuations of sentiment throughout a vlog's narrative time. We provide a taxonomy of these seven continuous sentiment styles and found that vlogs whose sentiment builds up towards a positive ending are the most prevalent in our sample. Gender was associated with preferences for different continuous sentiment trajectories. This paper discusses the findings with respect to previous work and concludes with an outlook towards possible uses of the corpus, method and findings of this paper for related areas of research.",http://aclweb.org/anthology/D18-1394,D18-1,D18-1394,https://arxiv.org/abs/1808.09722,"('Bennett Kleinberg', 'Maximilian Mozes', 'Isabelle van der Vegt')",10,Identifying the sentiment styles of YouTube’s vloggers,EMNLP,2018
"In this era of digitization, knowing the user's sociolect aspects have become essential features to build the user specific recommendation systems. These sociolect aspects could be found by mining the user's language sharing in the form of text in social media and reviews. This paper describes about the experiment that was performed in PAN Author Profiling 2017 shared task. The objective of the task is to find the sociolect aspects of the users from their tweets. The sociolect aspects considered in this experiment are user's gender and native language information. Here user's tweets written in a different language from their native language are represented as Document - Term Matrix with document frequency as the constraint. Further classification is done using the Support Vector Machine by taking gender and native language as target classes. This experiment attains the average accuracy of 73.42% in gender prediction and 76.26% in the native language identification task.",http://aclweb.org/anthology/D18-1395,D18-1,D18-1395,https://arxiv.org/abs/1708.06068,"('Gili Goldin', 'Ella Rabinovich', 'Shuly Wintner')",10,Native Language Identification with User Generated Content,EMNLP,2018
"Neural machine translation usually adopts autoregressive models and suffers from exposure bias as well as the consequent error propagation problem. Many previous works have discussed the relationship between error propagation and the \emph{accuracy drop} (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem. In this paper, we conduct a series of analyses to deeply understand this problem and get several interesting findings. (1) The role of error propagation on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem. (2) Characteristics of a language play a more important role in causing the accuracy drop: the left part of the translation result in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese). Our discoveries are confirmed on different model structures including Transformer and RNN, and in other sequence generation tasks such as text summarization.",http://aclweb.org/anthology/D18-1396,D18-1,D18-1396,https://arxiv.org/abs/1809.00120,"('Lijun Wu', 'Xu Tan', 'Di He', 'Fei Tian', 'Tao Qin', 'Jianhuang Lai', 'Tie-Yan Liu')",10,Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter,EMNLP,2018
"Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system. However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged. In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning. We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training. Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data. By integrating all our findings, we obtain competitive results on WMT14 English- German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.",http://aclweb.org/anthology/D18-1397,D18-1,D18-1397,https://arxiv.org/abs/1808.08866,"('Lijun Wu', 'Fei Tian', 'Tao Qin', 'Jianhuang Lai', 'Tie-Yan Liu')",10,A Study of Reinforcement Learning for Neural Machine Translation,EMNLP,2018
"There is no free lunch, no single learning algorithm that will outperform other algorithms on all data. In practice different approaches are tried and the best algorithm selected. An alternative solution is to build new algorithms on demand by creating a framework that accommodates many algorithms. The best combination of parameters and procedures is searched here in the space of all possible models belonging to the framework of Similarity-Based Methods (SBMs). Such meta-learning approach gives a chance to find the best method in all cases. Issues related to the meta-learning and first tests of this approach are presented.",http://aclweb.org/anthology/D18-1398,D18-1,D18-1398,https://arxiv.org/abs/1806.06207,"('Jiatao Gu', 'Yong Wang', 'Yun Chen', 'Victor O. K. Li', 'Kyunghyun Cho')",10,Meta-Learning for Low-Resource Neural Machine Translation,EMNLP,2018
"Recent work achieved remarkable results in training neural machine translation (NMT) systems in a fully unsupervised way, with new and dedicated architectures that rely on monolingual corpora only. In this work, we propose to define unsupervised NMT (UNMT) as NMT trained with the supervision of synthetic bilingual data. Our approach straightforwardly enables the use of state-of-the-art architectures proposed for supervised NMT by replacing human-made bilingual data with synthetic bilingual data for training. We propose to initialize the training of UNMT with synthetic bilingual data generated by unsupervised statistical machine translation (USMT). The UNMT system is then incrementally improved using back-translation. Our preliminary experiments show that our approach achieves a new state-of-the-art for unsupervised machine translation on the WMT16 German--English news translation task, for both translation directions.",http://aclweb.org/anthology/D18-1399,D18-1,D18-1399,https://arxiv.org/abs/1810.12703,"('Mikel Artetxe', 'Gorka Labaka', 'Eneko Agirre')",10,Unsupervised Statistical Machine Translation,EMNLP,2018
"We introduce a novel multimodal machine translation model that utilizes parallel visual and textual information. Our model jointly optimizes the learning of a shared visual-language embedding and a translator. The model leverages a visual attention grounding mechanism that links the visual semantics with the corresponding textual semantics. Our approach achieves competitive state-of-the-art results on the Multi30K and the Ambiguous COCO datasets. We also collected a new multilingual multimodal product description dataset to simulate a real-world international online shopping scenario. On this dataset, our visual attention grounding model outperforms other methods by a large margin.",http://aclweb.org/anthology/D18-1400,D18-1,D18-1400,https://arxiv.org/abs/1808.08266,"('Mingyang Zhou', 'Runxiang Cheng', 'Yong Jae Lee', 'Zhou Yu')",10,A Visual Attention Grounding Neural Model for Multimodal Machine Translation,EMNLP,2018
"Sentiment analysis is the Natural Language Processing (NLP) task dealing with the detection and classification of sentiments in texts. While some tasks deal with identifying the presence of sentiment in the text (Subjectivity analysis), other tasks aim at determining the polarity of the text categorizing them as positive, negative and neutral. Whenever there is a presence of sentiment in the text, it has a source (people, group of people or any entity) and the sentiment is directed towards some entity, object, event or person. Sentiment analysis tasks aim to determine the subject, the target and the polarity or valence of the sentiment. In our work, we try to automatically extract sentiment (positive or negative) from Facebook posts using a machine learning approach.While some works have been done in code-mixed social media data and in sentiment analysis separately, our work is the first attempt (as of now) which aims at performing sentiment analysis of code-mixed social media text. We have used extensive pre-processing to remove noise from raw text. Multilayer Perceptron model has been used to determine the polarity of the sentiment. We have also developed the corpus for this task by manually labeling Facebook posts with their associated sentiments.",http://aclweb.org/anthology/D18-1401,D18-1,D18-1401,https://arxiv.org/abs/1707.01184,"('Chenlin Shen', 'Changlong Sun', 'Jingjing Wang', 'Yangyang Kang', 'Shoushan Li', 'Xiaozhong Liu', 'Luo Si', 'Min Zhang', 'Guodong Zhou')",10,Sentiment Classification towards Question-Answering with Hierarchical Matching Network,EMNLP,2018
"Argument mining is a core technology for automating argument search in large document collections. Despite its usefulness for this task, most current approaches to argument mining are designed for use only with specific text types and fall short when applied to heterogeneous texts. In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts. We source annotations for over 25,000 instances covering eight controversial topics. The results of cross-topic experiments show that our attention-based neural network generalizes best to unseen topics and outperforms vanilla BiLSTM models by 6% in accuracy and 11% in F-score.",http://aclweb.org/anthology/D18-1402,D18-1,D18-1402,https://arxiv.org/abs/1802.05758,"('Christian Stab', 'Tristan Miller', 'Benjamin Schiller', 'Pranav Rai', 'Iryna Gurevych')",10,Cross-topic Argument Mining from Heterogeneous Sources,EMNLP,2018
"We present a neural framework for opinion summarization from online product reviews which is knowledge-lean and only requires light supervision (e.g., in the form of product domain labels and user-provided ratings). Our method combines two weakly supervised components to identify salient opinions and form extractive summaries from multiple reviews: an aspect extractor trained under a multi-task objective, and a sentiment predictor based on multiple instance learning. We introduce an opinion summarization dataset that includes a training set of product reviews from six diverse domains and human-annotated development and test sets with gold standard aspect annotations, salience labels, and opinion summaries. Automatic evaluation shows significant improvements over baselines, and a large-scale study indicates that our opinion summaries are preferred by human judges according to multiple criteria.",http://aclweb.org/anthology/D18-1403,D18-1,D18-1403,https://arxiv.org/abs/1808.08858,"('Stefanos Angelidis', 'Mirella Lapata')",10,Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised,EMNLP,2018
"Automatic emotion recognition has become a trending research topic in the past decade. While works based on facial expressions or speech abound, recognizing affect from body gestures remains a less explored topic. We present a new comprehensive survey hoping to boost research in the field. We first introduce emotional body gestures as a component of what is commonly known as ""body language"" and comment general aspects as gender differences and culture dependence. We then define a complete framework for automatic emotional body gesture recognition. We introduce person detection and comment static and dynamic body pose estimation methods both in RGB and 3D. We then comment the recent literature related to representation learning and emotion recognition from images of emotionally expressive gestures. We also discuss multi-modal approaches that combine speech or face with body gestures for improved emotion recognition. While pre-processing methodologies (e.g. human detection and pose estimation) are nowadays mature technologies fully developed for robust large scale analysis, we show that for emotion recognition the quantity of labelled data is scarce, there is no agreement on clearly defined output spaces and the representations are shallow and largely based on naive geometrical representations.",http://aclweb.org/anthology/D18-1404,D18-1,D18-1404,https://arxiv.org/abs/1801.07481,"('Elvis Saravia', 'Hsien-Chi Toby Liu', 'Yen-Hao Huang', 'Junlin Wu', 'Yi-Shin Chen')",10,CARER: Contextualized Affect Representations for Emotion Recognition,EMNLP,2018
"Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for log-linear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a ranking objective. We show that the ranking-based variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and trade-offs of both methods.",http://aclweb.org/anthology/D18-1405,D18-1,D18-1405,https://arxiv.org/abs/1809.01812,"('Zhuang Ma', 'Michael Collins')",10,Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency,EMNLP,2018
"Capturing the interdependencies between real valued time series can be achieved by finding common similar patterns. The abstraction of time series makes the process of finding similarities closer to the way as humans do. Therefore, the abstraction by means of a symbolic levels and finding the common patterns attracts researchers. One particular algorithm, Longest Common Subsequence, has been used successfully as a similarity measure between two sequences including real valued time series. In this paper, we propose Fuzzy Longest Common Subsequence matching for time series.",http://aclweb.org/anthology/D18-1406,D18-1,D18-1406,https://arxiv.org/abs/1508.03671,"('Semih Yavuz', 'Chung-Cheng Chiu', 'Patrick Nguyen', 'Yonghui Wu')",10,CaLcs: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization,EMNLP,2018
"One way to interpret neural model predictions is to highlight the most important input features---for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word's importance is determined by either input perturbation---measuring the decrease in model confidence when that word is removed---or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.",http://aclweb.org/anthology/D18-1407,D18-1,D18-1407,https://arxiv.org/abs/1804.07781,"('Shi Feng', 'Eric Wallace', 'Alvin Grissom II', 'Mohit Iyyer', 'Pedro Rodriguez', 'Jordan Boyd-Graber')",10,Pathologies of Neural Models Make Interpretations Difficult,EMNLP,2018
"We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.",http://aclweb.org/anthology/D18-1408,D18-1,D18-1408,https://arxiv.org/abs/1704.07073,"('Wei Wu', 'Houfeng Wang', 'Tianyu Liu', 'Shuming Ma')",10,Phrase-level Self-Attention Networks for Universal Sentence Encoding,EMNLP,2018
"In this work, we propose a novel method for training neural networks to perform single-document extractive summarization without heuristically-generated extractive labels. We call our approach BanditSum as it treats extractive summarization as a contextual bandit (CB) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action). A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences that maximize ROUGE score. We perform a series of experiments demonstrating that BanditSum is able to achieve ROUGE scores that are better than or comparable to the state-of-the-art for extractive summarization, and converges using significantly fewer update steps than competing approaches. In addition, we show empirically that BanditSum performs significantly better than competing approaches when good summary sentences appear late in the source document.",http://aclweb.org/anthology/D18-1409,D18-1,D18-1409,https://arxiv.org/abs/1809.09672,"('Yue Dong', 'Yikang Shen', 'Eric Crawford', 'Herke van Hoof', 'Jackie Chi Kit Cheung')",10,BanditSum: Extractive Summarization as a Contextual Bandit,EMNLP,2018
"Current lexical simplification approaches rely heavily on heuristics and corpus level features that do not always align with human judgment. We create a human-rated word-complexity lexicon of 15,000 English words and propose a novel neural readability ranking model with a Gaussian-based feature vectorization layer that utilizes these human ratings to measure the complexity of any given word or phrase. Our model performs better than the state-of-the-art systems for different lexical simplification tasks and evaluation datasets. Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by applying our model to the Paraphrase Database (PPDB).",http://aclweb.org/anthology/D18-1410,D18-1,D18-1410,https://arxiv.org/abs/1810.05754,"('Mounica Maddela', 'Wei Xu')",10,A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification,EMNLP,2018
"We present a new approach to learning semantic parsers from multiple datasets, even when the target semantic formalisms are drastically different, and the underlying corpora do not overlap. We handle such ""disjoint"" data by treating annotations for unobserved formalisms as latent structured variables. Building on state-of-the-art baselines, we show improvements both in frame-semantic parsing and semantic dependency parsing by modeling them jointly.",http://aclweb.org/anthology/D18-1411,D18-1,D18-1411,https://arxiv.org/abs/1804.05990,"('Guanghui Qin', 'Jin-Ge Yao', 'Xuening Wang', 'Jinpeng Wang', 'Chin-Yew Lin')",10,Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data,EMNLP,2018
"We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.",http://aclweb.org/anthology/D18-1412,D18-1,D18-1412,https://arxiv.org/abs/1808.10485,"('Swabha Swayamdipta', 'Sam Thomson', 'Kenton Lee', 'Luke Zettlemoyer', 'Chris Dyer', 'Noah A. Smith')",10,Syntactic Scaffolds for Semantic Structures,EMNLP,2018
"Scripts define knowledge about how everyday scenarios (such as going to a restaurant) are expected to unfold. One of the challenges to learning scripts is the hierarchical nature of the knowledge. For example, a suspect arrested might plead innocent or guilty, and a very different track of events is then expected to happen. To capture this type of information, we propose an autoencoder model with a latent space defined by a hierarchy of categorical variables. We utilize a recently proposed vector quantization based approach, which allows continuous embeddings to be associated with each latent variable value. This permits the decoder to softly decide what portions of the latent hierarchy to condition on by attending over the value embeddings for a given setting. Our model effectively encodes and generates scripts, outperforming a recent language modeling-based method on several standard tasks, and allowing the autoencoder model to achieve substantially lower perplexity scores compared to the previous language modeling-based method.",http://aclweb.org/anthology/D18-1413,D18-1,D18-1413,https://arxiv.org/abs/1808.09542,"('Noah Weber', 'Leena Shekhar', 'Niranjan Balasubramanian', 'Nate Chambers')",10,Hierarchical Quantized Representations for Script Generation,EMNLP,2018
"This paper studies semantic parsing for interlanguage (L2), taking semantic role labeling (SRL) as a case task and learner Chinese as a case language. We first manually annotate the semantic roles for a set of learner texts to derive a gold standard for automatic SRL. Based on the new data, we then evaluate three off-the-shelf SRL systems, i.e., the PCFGLA-parser-based, neural-parser-based and neural-syntax-agnostic systems, to gauge how successful SRL for learner Chinese can be. We find two non-obvious facts: 1) the L1-sentence-trained systems performs rather badly on the L2 data; 2) the performance drop from the L1 data to the L2 data of the two parser-based systems is much smaller, indicating the importance of syntactic parsing in SRL for interlanguages. Finally, the paper introduces a new agreement-based model to explore the semantic coherency information in the large-scale L2-L1 parallel data. We then show such information is very effective to enhance SRL for learner texts. Our model achieves an F-score of 72.06, which is a 2.02 point improvement over the best baseline.",http://aclweb.org/anthology/D18-1414,D18-1,D18-1414,https://arxiv.org/abs/1808.09409,"('Zi Lin', 'Yuguang Duan', 'Yuanyuan Zhao', 'Weiwei Sun', 'Xiaojun Wan')",10,Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data,EMNLP,2018
"In this paper, we present a probabilistic framework for goal-driven spoken dialog systems. A new dynamic stochastic state (DS-state) is then defined to characterize the goal set of a dialog state at different stages of the dialog process. Furthermore, an entropy minimization dialog management(EMDM) strategy is also proposed to combine with the DS-states to facilitate a robust and efficient solution in reaching a user's goals. A Song-On-Demand task, with a total of 38117 songs and 12 attributes corresponding to each song, is used to test the performance of the proposed approach. In an ideal simulation, assuming no errors, the EMDM strategy is the most efficient goal-seeking method among all tested approaches, returning the correct song within 3.3 dialog turns on average. Furthermore, in a practical scenario, with top five candidates to handle the unavoidable automatic speech recognition (ASR) and natural language understanding (NLU) errors, the results show that only 61.7\% of the dialog goals can be successfully obtained in 6.23 dialog turns on average when random questions are asked by the system, whereas if the proposed DS-states are updated with the top 5 candidates from the SLU output using the proposed EMDM strategy executed at every DS-state, then a 86.7\% dialog success rate can be accomplished effectively within 5.17 dialog turns on average. We also demonstrate that entropy-based DM strategies are more efficient than non-entropy based DM. Moreover, using the goal set distributions in EMDM, the results are better than those without them, such as in sate-of-the-art database summary DM.",http://aclweb.org/anthology/D18-1415,D18-1,D18-1415,https://arxiv.org/abs/1504.07182,"('Weikang Wang', 'Jiajun Zhang', 'Han Zhang', 'Mei-Yuh Hwang', 'Chengqing Zong', 'Zhifei Li')",10,A Teacher-Student Framework for Maintainable Dialog Manager,EMNLP,2018
"This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving the effectiveness and robustness of Deep Dyna-Q (DDQ), a recently proposed framework that extends the Dyna-Q algorithm to integrate planning for task-completion dialogue policy learning. To obviate DDQ's high dependency on the quality of simulated experiences, we incorporate an RNN-based discriminator in D3Q to differentiate simulated experience from real user experience in order to control the quality of training data. Experiments show that D3Q significantly outperforms DDQ by controlling the quality of simulated experience used for planning. The effectiveness and robustness of D3Q is further demonstrated in a domain extension setting, where the agent's capability of adapting to a changing environment is tested.",http://aclweb.org/anthology/D18-1416,D18-1,D18-1416,https://arxiv.org/abs/1808.09442,"('Shang-Yu Su', 'Xiujun Li', 'Jianfeng Gao', 'Jingjing Liu', 'Yun-Nung Chen')",10,Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning,EMNLP,2018
"Multimedia or spoken content presents more attractive information than plain text content, but the former is more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's therefore highly attractive to develop machines which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, a new task of machine comprehension of spoken content was proposed recently. The initial goal was defined as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native languages are not English. An Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture was also proposed for this task, which considered only the sequential relationship within the speech utterances. In this paper, we propose a new Hierarchical Attention Model (HAM), which constructs multi-hopped attention mechanism over tree-structured rather than sequential representations for the utterances. Improved comprehension performance robust with respect to ASR errors were obtained.",http://aclweb.org/anthology/D18-1417,D18-1,D18-1417,https://arxiv.org/abs/1608.07775,"('Changliang Li', 'Liang Li', 'Ji Qi')",10,A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding,EMNLP,2018
"In a dialog, there can be multiple valid next utterances at any point. The present end-to-end neural methods for dialog do not take this into account. They learn with the assumption that at any time there is only one correct next utterance. In this work, we focus on this problem in the goal-oriented dialog setting where there are different paths to reach a goal. We propose a new method, that uses a combination of supervised learning and reinforcement learning approaches to address this issue. We also propose a new and more effective testbed, permuted-bAbI dialog tasks, by introducing multiple valid next utterances to the original-bAbI dialog tasks, which allows evaluation of goal-oriented dialog systems in a more realistic setting. We show that there is a significant drop in performance of existing end-to-end neural methods from 81.5% per-dialog accuracy on original-bAbI dialog tasks to 30.3% on permuted-bAbI dialog tasks. We also show that our proposed method improves the performance and achieves 47.3% per-dialog accuracy on permuted-bAbI dialog tasks.",http://aclweb.org/anthology/D18-1418,D18-1,D18-1418,https://arxiv.org/abs/1808.09996,"('Janarthanan Rajendran', 'Jatin Ganhotra', 'Satinder Singh', 'Lazaros Polymenakos')",10,Learning End-to-End Goal-Oriented Dialog with Multiple Answers,EMNLP,2018
"Despite widespread interests in reinforcement-learning for task-oriented dialogue systems, several obstacles can frustrate research and development progress. First, reinforcement learners typically require interaction with the environment, so conventional dialogue corpora cannot be used directly. Second, each task presents specific challenges, requiring separate corpus of task-specific annotated data. Third, collecting and annotating human-machine or human-human conversations for task-oriented dialogues requires extensive domain knowledge. Because building an appropriate dataset can be both financially costly and time-consuming, one popular approach is to build a user simulator based upon a corpus of example dialogues. Then, one can train reinforcement learning agents in an online fashion as they interact with the simulator. Dialogue agents trained on these simulators can serve as an effective starting point. Once agents master the simulator, they may be deployed in a real environment to interact with humans, and continue to be trained online. To ease empirical algorithmic comparisons in dialogues, this paper introduces a new, publicly available simulation framework, where our simulator, designed for the movie-booking domain, leverages both rules and collected data. The simulator supports two tasks: movie ticket booking and movie seeking. Finally, we demonstrate several agents and detail the procedure to add and test your own agent in the proposed framework.",http://aclweb.org/anthology/D18-1419,D18-1,D18-1419,https://arxiv.org/abs/1612.05688,"('Wei Wei', 'Quoc Le', 'Andrew Dai', 'Jia Li')",10,AirDialogue: An Environment for Goal-Oriented Dialogue Research,EMNLP,2018
"We propose the task of Quantifiable Sequence Editing (QuaSE): editing an input sequence to generate an output sequence that satisfies a given numerical outcome value measuring a certain property of the sequence, with the requirement of keeping the main content of the input sequence. For example, an input sequence could be a word sequence, such as review sentence and advertisement text. For a review sentence, the outcome could be the review rating; for an advertisement, the outcome could be the click-through rate. The major challenge in performing QuaSE is how to perceive the outcome-related wordings, and only edit them to change the outcome. In this paper, the proposed framework contains two latent factors, namely, outcome factor and content factor, disentangled from the input sentence to allow convenient editing to change the outcome and keep the content. Our framework explores the pseudo-parallel sentences by modeling their content similarity and outcome differences to enable a better disentanglement of the latent factors, which allows generating an output to better satisfy the desired outcome and keep the content. The dual reconstruction structure further enhances the capability of generating expected output by exploiting the couplings of latent factors of pseudo-parallel sentences. For evaluation, we prepared a dataset of Yelp review sentences with the ratings as outcome. Extensive experimental results are reported and discussed to elaborate the peculiarities of our framework.",http://aclweb.org/anthology/D18-1420,D18-1,D18-1420,https://arxiv.org/abs/1804.07007,"('Yi Liao', 'Lidong Bing', 'Piji Li', 'Shuming Shi', 'Wai Lam', 'Tong Zhang')",10,QuaSE: Sequence Editing under Quantifiable Guidance,EMNLP,2018
"Automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing (NLP), and plays a key role in a number of applications such as question answering, search, and dialogue. In this paper, we present a deep reinforcement learning approach to paraphrase generation. Specifically, we propose a new framework for the task, which consists of a \textit{generator} and an \textit{evaluator}, both of which are learned from data. The generator, built as a sequence-to-sequence learning model, can produce paraphrases given a sentence. The evaluator, constructed as a deep matching model, can judge whether two sentences are paraphrases of each other. The generator is first trained by deep learning and then further fine-tuned by reinforcement learning in which the reward is given by the evaluator. For the learning of the evaluator, we propose two methods based on supervised learning and inverse reinforcement learning respectively, depending on the type of available training data. Empirical study shows that the learned evaluator can guide the generator to produce more accurate paraphrases. Experimental results demonstrate the proposed models (the generators) outperform the state-of-the-art methods in paraphrase generation in both automatic evaluation and human evaluation.",http://aclweb.org/anthology/D18-1421,D18-1,D18-1421,https://arxiv.org/abs/1711.00279,"('Zichao Li', 'Xin Jiang', 'Lifeng Shang', 'Hang Li')",10,Paraphrase Generation with Deep Reinforcement Learning,EMNLP,2018
"Recent neural models for data-to-text generation are mostly based on data-driven end-to-end training over encoder-decoder networks. Even though the generated texts are mostly fluent and informative, they often generate descriptions that are not consistent with the input structured data. This is a critical issue especially in domains that require inference or calculations over raw data. In this paper, we attempt to improve the fidelity of neural data-to-text generation by utilizing pre-executed symbolic operations. We propose a framework called Operation-guided Attention-based sequence-to-sequence network (OpAtt), with a specifically designed gating mechanism as well as a quantization module for operation results to utilize information from pre-executed operations. Experiments on two sports datasets show our proposed method clearly improves the fidelity of the generated texts to the input structured data.",http://aclweb.org/anthology/D18-1422,D18-1,D18-1422,https://arxiv.org/abs/1809.02735,"('Feng Nie', 'Jinpeng Wang', 'Jin-Ge Yao', 'Rong Pan', 'Chin-Yew Lin')",10,Operation-guided Neural Networks for High Fidelity Data-To-Text Generation,EMNLP,2018
"Computer poetry generation is our first step towards computer writing. Writing must have a theme. The current approaches of using sequence-to-sequence models with attention often produce non-thematic poems. We present a novel conditional variational autoencoder with a hybrid decoder adding the deconvolutional neural networks to the general recurrent neural networks to fully learn topic information via latent variables. This approach significantly improves the relevance of the generated poems by representing each line of the poem not only in a context-sensitive manner but also in a holistic way that is highly related to the given keyword and the learned topic. A proposed augmented word2vec model further improves the rhythm and symmetry. Tests show that the generated poems by our approach are mostly satisfying with regulated rules and consistent themes, and 73.42% of them receive an Overall score no less than 3 (the highest score is 5).",http://aclweb.org/anthology/D18-1423,D18-1,D18-1423,https://arxiv.org/abs/1711.07632,"('Juntao Li', 'Yan Song', 'Haisong Zhang', 'Dongmin Chen', 'Shuming Shi', 'Dongyan Zhao', 'Rui Yan')",10,Generating Classical Chinese Poems via Conditional Variational Autoencoder and Adversarial Training,EMNLP,2018
"We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).",http://aclweb.org/anthology/D18-1424,D18-1,D18-1424,https://arxiv.org/abs/1312.6116,"('Yao Zhao', 'Xiaochuan Ni', 'Yuanyuan Ding', 'Qifa Ke')",10,Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks,EMNLP,2018
"Task oriented dialog systems typically first parse user utterances to semantic frames comprised of intents and slots. Previous work on task oriented intent and slot-filling work has been restricted to one intent per query and one slot label per token, and thus cannot model complex compositional requests. Alternative semantic parsing systems have represented queries as logical forms, but these are challenging to annotate and parse. We propose a hierarchical annotation scheme for semantic parsing that allows the representation of compositional queries, and can be efficiently and accurately parsed by standard constituency parsing models. We release a dataset of 44k annotated queries (fb.me/semanticparsingdialog), and show that parsing models outperform sequence-to-sequence approaches on this dataset.",http://aclweb.org/anthology/D18-1425,D18-1,D18-1425,https://arxiv.org/abs/1810.07942,"('Tao Yu', 'Rui Zhang', 'Kai Yang', 'Michihiro Yasunaga', 'Dongxu Wang', 'Zifan Li', 'James Ma', 'Irene Li', 'Qingning Yao', 'Shanelle Roman', 'Zilin Zhang', 'Dragomir Radev')",10,Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task,EMNLP,2018
"Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.",http://aclweb.org/anthology/D18-1426,D18-1,D18-1426,https://arxiv.org/abs/1804.07899,"('Markus Freitag', 'Scott Roy')",10,Unsupervised Natural Language Generation with Denoising Autoencoders,EMNLP,2018
"Neural question generation (NQG) is the task of generating a question from a given passage with deep neural networks. Previous NQG models suffer from a problem that a significant proportion of the generated questions include words in the question target, resulting in the generation of unintended questions. In this paper, we propose answer-separated seq2seq, which better utilizes the information from both the passage and the target answer. By replacing the target answer in the original passage with a special token, our model learns to identify which interrogative word should be used. We also propose a new module termed keyword-net, which helps the model better capture the key information in the target answer and generate an appropriate question. Experimental results demonstrate that our answer separation method significantly reduces the number of improper questions which include answers. Consequently, our model significantly outperforms previous state-of-the-art NQG models.",http://aclweb.org/anthology/D18-1427,D18-1,D18-1427,https://arxiv.org/abs/1809.02393,"('Xingwu Sun', 'Jing Liu', 'Yajuan Lyu', 'Wei He', 'Yanjun Ma', 'Shi Wang')",10,Answer-focused and Position-aware Neural Question Generation,EMNLP,2018
"Existing text generation methods tend to produce repeated and ""boring"" expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The proposed model assigns low reward for repeatedly generated text and high reward for ""novel"" and fluent text, encouraging the generator to produce diverse and informative text. Moreover, we propose a novel language-model based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines. The code is available at https://github.com/lancopku/DPGAN",http://aclweb.org/anthology/D18-1428,D18-1,D18-1428,https://arxiv.org/abs/1802.01345,"('Jingjing Xu', 'Xuancheng Ren', 'Junyang Lin', 'Xu Sun')",10,Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation,EMNLP,2018
"There has always been criticism for using $n$-gram based similarity metrics, such as BLEU, NIST, etc, for evaluating the performance of NLG systems. However, these metrics continue to remain popular and are recently being used for evaluating the performance of systems which automatically generate questions from documents, knowledge graphs, images, etc. Given the rising interest in such automatic question generation (AQG) systems, it is important to objectively examine whether these metrics are suitable for this task. In particular, it is important to verify whether such metrics used for evaluating AQG systems focus on answerability of the generated question by preferring questions which contain all relevant information such as question type (Wh-types), entities, relations, etc. In this work, we show that current automatic evaluation metrics based on $n$-gram similarity do not always correlate well with human judgments about answerability of a question. To alleviate this problem and as a first step towards better evaluation metrics for AQG, we introduce a scoring function to capture answerability and show that when this scoring function is integrated with existing metrics, they correlate significantly better with human judgments. The scripts and data developed as a part of this work are made publicly available at https://github.com/PrekshaNema25/Answerability-Metric",http://aclweb.org/anthology/D18-1429,D18-1,D18-1429,https://arxiv.org/abs/1808.10192,"('Preksha Nema', 'Mitesh M. Khapra')",10,Towards a Better Metric for Evaluating Question Generation Systems,EMNLP,2018
"Research has shown that sequence-to-sequence neural models, particularly those with the attention mechanism, can successfully generate classical Chinese poems. However, neural models are not capable of generating poems that match specific styles, such as the impulsive style of Li Bai, a famous poet in the Tang Dynasty. This work proposes a memory-augmented neural model to enable the generation of style-specific poetry. The key idea is a memory structure that stores how poems with a desired style were generated by humans, and uses similar fragments to adjust the generation. We demonstrate that the proposed algorithm generates poems with flexible styles, including styles of a particular era and an individual poet.",http://aclweb.org/anthology/D18-1430,D18-1,D18-1430,https://arxiv.org/abs/1807.06500,"('Cheng Yang', 'Maosong Sun', 'Xiaoyuan Yi', 'Wenhao Li')",10,Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement,EMNLP,2018
"Neural conversation models tend to generate safe, generic responses for most inputs. This is due to the limitations of likelihood-based decoding objectives in generation tasks with diverse outputs, such as conversation. To address this challenge, we propose a simple yet effective approach for incorporating side information in the form of distributional constraints over the generated responses. We propose two constraints that help generate more content rich responses that are based on a model of syntax and topics (Griffiths et al., 2005) and semantic similarity (Arora et al., 2016). We evaluate our approach against a variety of competitive baselines, using both automatic metrics and human judgments, showing that our proposed approach generates responses that are much less generic without sacrificing plausibility. A working demo of our code can be found at https://github.com/abaheti95/DC-NeuralConversation.",http://aclweb.org/anthology/D18-1431,D18-1,D18-1431,https://arxiv.org/abs/1809.01215,"('Ashutosh Baheti', 'Alan Ritter', 'Jiwei Li', 'Bill Dolan')",10,Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints,EMNLP,2018
"We present three enhancements to existing encoder-decoder models for open-domain conversational agents, aimed at effectively modeling coherence and promoting output diversity: (1) We introduce a measure of coherence as the GloVe embedding similarity between the dialogue context and the generated response, (2) we filter our training corpora based on the measure of coherence to obtain topically coherent and lexically diverse context-response pairs, (3) we then train a response generator using a conditional variational autoencoder model that incorporates the measure of coherence as a latent variable and uses a context gate to guarantee topical consistency with the context and promote lexical diversity. Experiments on the OpenSubtitles corpus show a substantial improvement over competitive neural models in terms of BLEU score as well as metrics of coherence and diversity.",http://aclweb.org/anthology/D18-1432,D18-1,D18-1432,https://arxiv.org/abs/1809.06873,"('Xinnuo Xu', 'Ondřej Dušek', 'Ioannis Konstas', 'Verena Rieser')",10,"Better Conversations by Modeling, Filtering, and Optimizing for Coherence and Diversity",EMNLP,2018
"This paper presents a video summarization technique for an Internet video to provide a quick way to overview its content. This is a challenging problem because finding important or informative parts of the original video requires to understand its content. Furthermore the content of Internet videos is very diverse, ranging from home videos to documentaries, which makes video summarization much more tough as prior knowledge is almost not available. To tackle this problem, we propose to use deep video features that can encode various levels of content semantics, including objects, actions, and scenes, improving the efficiency of standard video summarization techniques. For this, we design a deep neural network that maps videos as well as descriptions to a common semantic space and jointly trained it with associated pairs of videos and descriptions. To generate a video summary, we extract the deep features from each segment of the original video and apply a clustering-based summarization technique to them. We evaluate our video summaries using the SumMe dataset as well as baseline approaches. The results demonstrated the advantages of incorporating our deep semantic features in a video summarization technique.",http://aclweb.org/anthology/D18-1433,D18-1,D18-1433,https://arxiv.org/abs/1609.08758,"('Spencer Whitehead', 'Heng Ji', 'Mohit Bansal', 'Shih-Fu Chang', 'Clare Voss')",10,Incorporating Background Knowledge into Video Description Generation,EMNLP,2018
"Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).",http://aclweb.org/anthology/D18-1434,D18-1,D18-1434,https://arxiv.org/abs/1808.03986,"('Badri Narayana Patro', 'Sandeep Kumar', 'Vinod Kumar Kurmi', 'Vinay Namboodiri')",10,Multimodal Differential Network for Visual Question Generation,EMNLP,2018
"We study how to generate captions that are not only accurate in describing an image but also discriminative across different images. The problem is both fundamental and interesting, as most machine-generated captions, despite phenomenal research progresses in the past several years, are expressed in a very monotonic and featureless format. While such captions are normally accurate, they often lack important characteristics in human languages - distinctiveness for each caption and diversity for different images. To address this problem, we propose a novel conditional generative adversarial network for generating diverse captions across images. Instead of estimating the quality of a caption solely on one image, the proposed comparative adversarial learning framework better assesses the quality of captions by comparing a set of captions within the image-caption joint space. By contrasting with human-written captions and image-mismatched captions, the caption generator effectively exploits the inherent characteristics of human languages, and generates more discriminative captions. We show that our proposed network is capable of producing accurate and diverse captions across images.",http://aclweb.org/anthology/D18-1435,D18-1,D18-1435,https://arxiv.org/abs/1804.00861,"('Di Lu', 'Spencer Whitehead', 'Lifu Huang', 'Heng Ji', 'Shih-Fu Chang')",10,Entity-aware Image Caption Generation,EMNLP,2018
"We propose a novel non-rigid image registration algorithm that is built upon fully convolutional networks (FCNs) to optimize and learn spatial transformations between pairs of images to be registered. Different from most existing deep learning based image registration methods that learn spatial transformations from training data with known corresponding spatial transformations, our method directly estimates spatial transformations between pairs of images by maximizing an image-wise similarity metric between fixed and deformed moving images, similar to conventional image registration algorithms. At the same time, our method also learns FCNs for encoding the spatial transformations at the same spatial resolution of images to be registered, rather than learning coarse-grained spatial transformation information. The image registration is implemented in a multi-resolution image registration framework to jointly optimize and learn spatial transformations and FCNs at different resolutions with deep self-supervision through typical feedforward and backpropagation computation. Since our method simultaneously optimizes and learns spatial transformations for the image registration, our method can be directly used to register a pair of images, and the registration of a set of images is also a training procedure for FCNs so that the trained FCNs can be directly adopted to register new images by feedforward computation of the learned FCNs without any optimization. The proposed method has been evaluated for registering 3D structural brain magnetic resonance (MR) images and obtained better performance than state-of-the-art image registration algorithms.",http://aclweb.org/anthology/D18-1436,D18-1,D18-1436,https://arxiv.org/abs/1709.00799,"('Harsh Jhamtani', 'Taylor Berg-Kirkpatrick')",10,Learning to Describe Differences Between Pairs of Similar Images,EMNLP,2018
"Despite continuously improving performance, contemporary image captioning models are prone to ""hallucinating"" objects that are not actually in a scene. One problem is that standard metrics only measure similarity to ground truth captions and may not fully capture image relevance. In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination. We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination. We investigate these questions on the standard image caption- ing benchmark, MSCOCO, using a diverse set of models. Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors.",http://aclweb.org/anthology/D18-1437,D18-1,D18-1437,https://arxiv.org/abs/1809.02156,"('Anna Rohrbach', 'Lisa Anne Hendricks', 'Kaylee Burns', 'Trevor Darrell', 'Kate Saenko')",10,Object Hallucination in Image Captioning,EMNLP,2018
"We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.",http://aclweb.org/anthology/D18-1438,D18-1,D18-1438,https://arxiv.org/abs/1708.02977,"('Jingqiang Chen', 'Hai Zhuge')",10,Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN,EMNLP,2018
"In this paper, we study automatic keyphrase generation. Although conventional approaches to this task show promising results, they neglect correlation among keyphrases, resulting in duplication and coverage issues. To solve these problems, we propose a new sequence-to-sequence architecture for keyphrase generation named CorrRNN, which captures correlation among multiple keyphrases in two ways. First, we employ a coverage vector to indicate whether the word in the source document has been summarized by previous phrases to improve the coverage for keyphrases. Second, preceding phrases are taken into account to eliminate duplicate phrases and improve result coherence. Experiment results show that our model significantly outperforms the state-of-the-art method on benchmark datasets in terms of both accuracy and diversity.",http://aclweb.org/anthology/D18-1439,D18-1,D18-1439,https://arxiv.org/abs/1808.07185,"('Jun Chen', 'Xiaoming Zhang', 'Yu Wu', 'Zhao Yan', 'Zhoujun Li')",10,Keyphrase Generation with Correlation Constraints,EMNLP,2018
"A good neural sequence-to-sequence summarization model should have a strong encoder that can distill and memorize the important information from long input texts so that the decoder can generate salient summaries based on the encoder's memory. In this paper, we aim to improve the memorization capabilities of the encoder of a pointer-generator model by adding an additional 'closed-book' decoder without attention and pointer mechanisms. Such a decoder forces the encoder to be more selective in the information encoded in its memory state because the decoder can't rely on the extra information provided by the attention and possibly copy modules, and hence improves the entire model. On the CNN/Daily Mail dataset, our 2-decoder model outperforms the baseline significantly in terms of ROUGE and METEOR metrics, for both cross-entropy and reinforced setups (and on human evaluation). Moreover, our model also achieves higher scores in a test-only DUC-2002 generalizability setup. We further present a memory ability test, two saliency metrics, as well as several sanity-check ablations (based on fixed-encoder, gradient-flow cut, and model capacity) to prove that the encoder of our 2-decoder model does in fact learn stronger memory representations than the baseline encoder.",http://aclweb.org/anthology/D18-1440,D18-1,D18-1440,https://arxiv.org/abs/1809.04585,"('Yichen Jiang', 'Mohit Bansal')",10,Closed-Book Training to Improve Summarization Encoder Memory,EMNLP,2018
"Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance using the latter is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset. Code is available at \url{https://github.com/sheffieldnlp/AMR2Text-summ}",http://aclweb.org/anthology/D18-1441,D18-1,D18-1441,https://arxiv.org/abs/1808.09160,"('Wei Li', 'Xinyan Xiao', 'Yajuan Lyu', 'Yuanzhuo Wang')",10,Improving Neural Abstractive Document Summarization with Structural Regularization,EMNLP,2018
"In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, result- ing in a sub-optimal representation. To ad- dress this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.",http://aclweb.org/anthology/D18-1442,D18-1,D18-1442,https://arxiv.org/abs/1809.10324,"('Xiuying Chen', 'Shen Gao', 'Chongyang Tao', 'Yan Song', 'Dongyan Zhao', 'Rui Yan')",10,Iterative Document Representation Learning Towards Summarization with Polishing,EMNLP,2018
"Till now, neural abstractive summarization methods have achieved great success for single document summarization (SDS). However, due to the lack of large scale multi-document summaries, such methods can be hardly applied to multi-document summarization (MDS). In this paper, we investigate neural abstractive methods for MDS by adapting a state-of-the-art neural abstractive summarization model for SDS. We propose an approach to extend the neural abstractive model trained on large scale SDS data to the MDS task. Our approach only makes use of a small number of multi-document summaries for fine tuning. Experimental results on two benchmark DUC datasets demonstrate that our approach can outperform a variety of baseline neural models.",http://aclweb.org/anthology/D18-1443,D18-1,D18-1443,https://arxiv.org/abs/1804.09010,"('Sebastian Gehrmann', 'Yuntian Deng', 'Alexander Rush')",10,Bottom-Up Abstractive Summarization,EMNLP,2018
"Neural network-based approaches have become widespread for abstractive text summarization. Though previously proposed models for abstractive text summarization addressed the problem of repetition of the same contents in the summary, they did not explicitly consider its information structure. One of the reasons these previous models failed to account for information structure in the generated summary is that standard datasets include summaries of variable lengths, resulting in problems in analyzing information flow, specifically, the manner in which the first sentence is related to the following sentences. Therefore, we use a dataset containing summaries with only three bullet points, and propose a neural network-based abstractive summarization model that considers the information structures of the generated summaries. Our experimental results show that the information structure of a summary can be controlled, thus improving the performance of the overall summarization.",http://aclweb.org/anthology/D18-1444,D18-1,D18-1444,https://arxiv.org/abs/1809.10867,"('Yizhu Liu', 'Zhiyi Luo', 'Kenny Zhu')",10,Controlling Length in Abstractive Summarization Using a Convolutional Neural Network,EMNLP,2018
"We propose a method to perform automatic document summarisation without using reference summaries. Instead, our method interactively learns from users' preferences. The merit of preference-based interactive summarisation is that preferences are easier for users to provide than reference summaries. Existing preference-based interactive learning methods suffer from high sample complexity, i.e. they need to interact with the oracle for many rounds in order to converge. In this work, we propose a new objective function, which enables us to leverage active learning, preference learning and reinforcement learning techniques in order to reduce the sample complexity. Both simulation and real-user experiments suggest that our method significantly advances the state of the art. Our source code is freely available at https://github.com/UKPLab/emnlp2018-april.",http://aclweb.org/anthology/D18-1445,D18-1,D18-1445,https://arxiv.org/abs/1808.09658,"('Yang Gao', 'Christian M. Meyer', 'Iryna Gurevych')",10,APRIL: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning,EMNLP,2018
"The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.",http://aclweb.org/anthology/D18-1446,D18-1,D18-1446,https://arxiv.org/abs/1507.02907,"('Logan Lebanoff', 'Kaiqiang Song', 'Fei Liu')",10,Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization,EMNLP,2018
"We study the problem of generating keyphrases that summarize the key points for a given document. While sequence-to-sequence (seq2seq) models have achieved remarkable performance on this task (Meng et al., 2017), model training often relies on large amounts of labeled data, which is only applicable to resource-rich domains. In this paper, we propose semi-supervised keyphrase generation methods by leveraging both labeled data and large-scale unlabeled samples for learning. Two strategies are proposed. First, unlabeled documents are first tagged with synthetic keyphrases obtained from unsupervised keyphrase extraction methods or a selflearning algorithm, and then combined with labeled samples for training. Furthermore, we investigate a multi-task learning framework to jointly learn to generate keyphrases as well as the titles of the articles. Experimental results show that our semi-supervised learning-based methods outperform a state-of-the-art model trained with labeled data only.",http://aclweb.org/anthology/D18-1447,D18-1,D18-1447,https://arxiv.org/abs/1808.06773,"('Hai Ye', 'Lu Wang')",10,Semi-Supervised Learning for Neural Keyphrase Generation,EMNLP,2018
"In this paper, we introduce How2, a multimodal collection of instructional videos with English subtitles and crowdsourced Portuguese translations. We also present integrated sequence-to-sequence baselines for machine translation, automatic speech recognition, spoken language translation, and multimodal summarization. By making available data and code for several multimodal natural language tasks, we hope to stimulate more research on these and similar challenges, to obtain a deeper understanding of multimodality in language processing.",http://aclweb.org/anthology/D18-1448,D18-1,D18-1448,https://arxiv.org/abs/1811.00347,"('Junnan Zhu', 'Haoran Li', 'Tianshang Liu', 'Yu Zhou', 'Jiajun Zhang', 'Chengqing Zong')",10,MSMO: Multimodal Summarization with Multimodal Output,EMNLP,2018
"Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being ""frustratingly easy"" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.",http://aclweb.org/anthology/D18-1449,D18-1,D18-1449,https://arxiv.org/abs/1511.05547,['Hayato Kobayashi'],10,Frustratingly Easy Model Ensemble for Abstractive Summarization,EMNLP,2018
"To analyze the limitations and the future directions of the extractive summarization paradigm, this paper proposes an Integer Linear Programming (ILP) formulation to obtain extractive oracle summaries in terms of ROUGE-N. We also propose an algorithm that enumerates all of the oracle summaries for a set of reference summaries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries.",http://aclweb.org/anthology/D18-1450,D18-1,D18-1450,https://arxiv.org/abs/1701.01614,"('Tsutomu Hirao', 'Hidetaka Kamigaito', 'Masaaki Nagata')",10,Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries,EMNLP,2018
"Auto-encoders compress input data into a latent-space representation and reconstruct the original data from the representation. This latent representation is not easily interpreted by humans. In this paper, we propose training an auto-encoder that encodes input text into human-readable sentences, and unpaired abstractive summarization is thereby achieved. The auto-encoder is composed of a generator and a reconstructor. The generator encodes the input text into a shorter word sequence, and the reconstructor recovers the generator input from the generator output. To make the generator output human-readable, a discriminator restricts the output of the generator to resemble human-written sentences. By taking the generator output as the summary of the input text, abstractive summarization is achieved without document-summary pairs as training data. Promising results are shown on both English and Chinese corpora.",http://aclweb.org/anthology/D18-1451,D18-1,D18-1451,https://arxiv.org/abs/1810.02851,"('Yaushian Wang', 'Hung-yi Lee')",10,Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks,EMNLP,2018
"We address jointly two important tasks for Question Answering in community forums: given a new question, (i) find related existing questions, and (ii) find relevant answers to this new question. We further use an auxiliary task to complement the previous two, i.e., (iii) find good answers with respect to the thread question in a question-comment thread. We use deep neural networks (DNNs) to learn meaningful task-specific embeddings, which we then incorporate into a conditional random field (CRF) model for the multitask setting, performing joint learning over a complex graph structure. While DNNs alone achieve competitive results when trained to produce the embeddings, the CRF, which makes use of the embeddings and the dependencies between the tasks, improves the results significantly and consistently across a variety of evaluation metrics, thus showing the complementarity of DNNs and structured learning.",http://aclweb.org/anthology/D18-1452,D18-1,D18-1452,https://arxiv.org/abs/1809.08928,"('Shafiq Joty', 'Lluís Màrquez', 'Preslav Nakov')",10,Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings,EMNLP,2018
"A challenge in creating a dataset for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues. In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice). We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. We then manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiple-choice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in MRC.",http://aclweb.org/anthology/D18-1453,D18-1,D18-1453,https://arxiv.org/abs/1808.09384,"('Saku Sugawara', 'Kentaro Inui', 'Satoshi Sekine', 'Akiko Aizawa')",10,What Makes Reading Comprehension Questions Easier?,EMNLP,2018
"The Winograd Schema Challenge (WSC) is a test of machine intelligence, designed to be an improvement on the Turing test. A Winograd Schema consists of a sentence and a corresponding question. To successfully answer these questions, one requires the use of commonsense knowledge and reasoning. This work focuses on extracting common sense knowledge which can be used to generate answers for the Winograd schema challenge. Common sense knowledge is extracted based on events (or actions) and their participants; called Event-Based Conditional Commonsense (ECC). I propose an approach using Narrative Event Chains [Chambers et al., 2008] to extract ECC knowledge. These are stored in templates, to be later used for answering the WSC questions. This approach works well with respect to a subset of WSC tasks.",http://aclweb.org/anthology/D18-1454,D18-1,D18-1454,https://arxiv.org/abs/1801.02281,"('Lisa Bauer', 'Yicheng Wang', 'Mohit Bansal')",10,Commonsense for Generative Multi-Hop Question Answering Tasks,EMNLP,2018
"Open Domain Question Answering (QA) is evolving from complex pipelined systems to end-to-end deep neural networks. Specialized neural models have been developed for extracting answers from either text alone or Knowledge Bases (KBs) alone. In this paper we look at a more practical setting, namely QA over the combination of a KB and entity-linked text, which is appropriate when an incomplete KB is available with a large text corpus. Building on recent advances in graph representation learning we propose a novel model, GRAFT-Net, for extracting answers from a question-specific subgraph containing text and KB entities and relations. We construct a suite of benchmark tasks for this problem, varying the difficulty of questions, the amount of training data, and KB completeness. We show that GRAFT-Net is competitive with the state-of-the-art when tested using either KBs or text alone, and vastly outperforms existing methods in the combined setting. Source code is available at https://github.com/OceanskySun/GraftNet .",http://aclweb.org/anthology/D18-1455,D18-1,D18-1455,https://arxiv.org/abs/1809.00782,"('Haitian Sun', 'Bhuwan Dhingra', 'Manzil Zaheer', 'Kathryn Mazaitis', 'Ruslan Salakhutdinov', 'William Cohen')",10,Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text,EMNLP,2018
"In this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans in a passage, the MS-MARCO dataset defines the task as answering a question from multiple passages and the words in the answer are not necessary in the passages. We therefore develop an extraction-then-synthesis framework to synthesize answers from extraction results. Specifically, the answer extraction model is first employed to predict the most important sub-spans from the passage as evidence, and the answer synthesis model takes the evidence as additional features along with the question and passage to further elaborate the final answers. We build the answer extraction model with state-of-the-art neural networks for single passage reading comprehension, and propose an additional task of passage ranking to help answer extraction in multiple passages. The answer synthesis model is based on the sequence-to-sequence neural networks with extracted evidences as features. Experiments show that our extraction-then-synthesis method outperforms state-of-the-art methods.",http://aclweb.org/anthology/D18-1456,D18-1,D18-1456,https://arxiv.org/abs/1706.04815,"('Souvik Kundu', 'Hwee Tou Ng')",10,A Nil-Aware Answer Extraction Framework for Question Answering,EMNLP,2018
"Advanced neural machine translation (NMT) models generally implement encoder and decoder as multiple layers, which allows systems to model complex functions and capture complicated linguistic structures. However, only the top layers of encoder and decoder are leveraged in the subsequent process, which misses the opportunity to exploit the useful information embedded in other layers. In this work, we propose to simultaneously expose all of these signals with layer aggregation and multi-layer attention mechanisms. In addition, we introduce an auxiliary regularization term to encourage different layers to capture diverse information. Experimental results on widely-used WMT14 English-German and WMT17 Chinese-English translation data demonstrate the effectiveness and universality of the proposed approach.",http://aclweb.org/anthology/D18-1457,D18-1,D18-1457,https://arxiv.org/abs/1810.10181,"('Zi-Yi Dou', 'Zhaopeng Tu', 'Xing Wang', 'Shuming Shi', 'Tong Zhang')",10,Exploiting Deep Representations for Neural Machine Translation,EMNLP,2018
"Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.",http://aclweb.org/anthology/D18-1458,D18-1,D18-1458,https://arxiv.org/abs/1808.08946,"('Gongbo Tang', 'Mathias Müller', 'Annette Rios', 'Rico Sennrich')",10,Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures,EMNLP,2018
"In this paper, we propose an additionsubtraction twin-gated recurrent network (ATR) to simplify neural machine translation. The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs. With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated. Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved. Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification. Forward self-attention can be easily established in ATR, which makes the proposed network interpretable. Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English- German and English-French language pairs in terms of both translation quality and speed. Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.",http://aclweb.org/anthology/D18-1459,D18-1,D18-1459,https://arxiv.org/abs/1810.12546,"('Biao Zhang', 'Deyi Xiong', 'jinsong su', 'Qian Lin', 'Huiji Zhang')",10,Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks,EMNLP,2018
"Although neural machine translation has achieved promising results, it suffers from slow translation speed. The direct consequence is that a trade-off has to be made between translation quality and speed, thus its performance can not come into full play. We apply cube pruning, a popular technique to speed up dynamic programming, into neural machine translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less \$\mathrm{softmax}\$ operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by \$3.3\times\$ on GPUs and \$3.5\times\$ on CPUs.",http://aclweb.org/anthology/D18-1460,D18-1,D18-1460,https://arxiv.org/abs/1809.02992,"('Wen Zhang', 'Liang Huang', 'Yang Feng', 'Lei Shen', 'Qun Liu')",10,Speeding Up Neural Machine Translation Decoding by Cube Pruning,EMNLP,2018
"Translating characters instead of words or word-fragments has the potential to simplify the processing pipeline for neural machine translation (NMT), and improve results by eliminating hyper-parameters and manual feature engineering. However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges. In this paper, we show that the modeling problem can be solved by standard sequence-to-sequence architectures of sufficient depth, and that deep models operating at the character level outperform identical models operating over word fragments. This result implies that alternative architectures for handling character input are better viewed as methods for reducing computation time than as improved ways of modeling longer sequences. From this perspective, we evaluate several techniques for character-level NMT, verify that they do not match the performance of our deep character baseline model, and evaluate the performance versus computation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins.",http://aclweb.org/anthology/D18-1461,D18-1,D18-1461,https://arxiv.org/abs/1808.09943,"('Colin Cherry', 'George Foster', 'Ankur Bapna', 'Orhan Firat', 'Wolfgang Macherey')",10,Revisiting Character-Based Neural Machine Translation with Capacity and Compression,EMNLP,2018
"Narrative story generation is a challenging problem because it demands the generated sentences with tight semantic connections, which has not been well studied by most existing generative models. To address this problem, we propose a skeleton-based model to promote the coherence of generated stories. Different from traditional models that generate a complete sentence at a stroke, the proposed model first generates the most critical phrases, called skeleton, and then expands the skeleton to a complete and fluent sentence. The skeleton is not manually defined, but learned by a reinforcement learning method. Compared to the state-of-the-art models, our skeleton-based model can generate significantly more coherent text according to human evaluation and automatic evaluation. The G-score is improved by 20.1% in the human evaluation. The code is available at https://github.com/lancopku/Skeleton-Based-Generation-Model",http://aclweb.org/anthology/D18-1462,D18-1,D18-1462,https://arxiv.org/abs/1808.06945,"('Jingjing Xu', 'Xuancheng Ren', 'Yi Zhang', 'Qi Zeng', 'Xiaoyan Cai', 'Xu Sun')",10,A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation,EMNLP,2018
"Sequence-to-Sequence (seq2seq) models have become overwhelmingly popular in building end-to-end trainable dialogue systems. Though highly efficient in learning the backbone of human-computer communications, they suffer from the problem of strongly favoring short generic responses. In this paper, we argue that a good response should smoothly connect both the preceding dialogue history and the following conversations. We strengthen this connection through mutual information maximization. To sidestep the non-differentiability of discrete natural language tokens, we introduce an auxiliary continuous code space and map such code space to a learnable prior distribution for generation purpose. Experiments on two dialogue datasets validate the effectiveness of our model, where the generated responses are closely related to the dialogue context and lead to more interactive conversations.",http://aclweb.org/anthology/D18-1463,D18-1,D18-1463,https://arxiv.org/abs/1810.00671,"('Xiaoyu Shen', 'Hui Su', 'Wenjie Li', 'Dietrich Klakow')",10,NEXUS Network: Connecting the Preceding and the Following in Dialogue Generation,EMNLP,2018
"Recent applications of neural language models have led to an increased interest in the automatic generation of natural language. However impressive, the evaluation of neurally generated text has so far remained rather informal and anecdotal. Here, we present an attempt at the systematic assessment of one aspect of the quality of neurally generated text. We focus on a specific aspect of neural language generation: its ability to reproduce authorial writing styles. Using established models for authorship attribution, we empirically assess the stylistic qualities of neurally generated text. In comparison to conventional language models, neural models generate fuzzier text that is relatively harder to attribute correctly. Nevertheless, our results also suggest that neurally generated text offers more valuable perspectives for the augmentation of training data.",http://aclweb.org/anthology/D18-1464,D18-1,D18-1464,https://arxiv.org/abs/1708.05536,"('Mohsen Mesgar', 'Michael Strube')",10,A Neural Local Coherence Model for Text Quality Assessment,EMNLP,2018
"In this thesis, we explore the use of deep neural networks for generation of natural language. Specifically, we implement two sequence-to-sequence neural variational models - variational autoencoders (VAE) and variational encoder-decoders (VED). VAEs for text generation are difficult to train due to issues associated with the Kullback-Leibler (KL) divergence term of the loss function vanishing to zero. We successfully train VAEs by implementing optimization heuristics such as KL weight annealing and word dropout. We also demonstrate the effectiveness of this continuous latent space through experiments such as random sampling, linear interpolation and sampling from the neighborhood of the input. We argue that if VAEs are not designed appropriately, it may lead to bypassing connections which results in the latent space being ignored during training. We show experimentally with the example of decoder hidden state initialization that such bypassing connections degrade the VAE into a deterministic model, thereby reducing the diversity of generated sentences. We discover that the traditional attention mechanism used in sequence-to-sequence VED models serves as a bypassing connection, thereby deteriorating the model's latent space. In order to circumvent this issue, we propose the variational attention mechanism where the attention context vector is modeled as a random variable that can be sampled from a distribution. We show empirically using automatic evaluation metrics, namely entropy and distinct measures, that our variational attention model generates more diverse output sentences than the deterministic attention model. A qualitative analysis with human evaluation study proves that our model simultaneously produces sentences that are of high quality and equally fluent as the ones generated by the deterministic attention counterpart.",http://aclweb.org/anthology/D18-1465,D18-1,D18-1465,https://arxiv.org/abs/1808.09012,"('Baiyun Cui', 'Yingming Li', 'Ming Chen', 'Zhongfei Zhang')",10,Deep Attentive Sentence Ordering Network,EMNLP,2018
,http://aclweb.org/anthology/D18-1466,D18-1,D18-1466,,"('Ieva Staliūnaitė', 'Hannah Rohde', 'Bonnie Webber', 'Annie Louis')",10,"Getting to ""Hearer-old"": Charting Referring Expressions Across Time",EMNLP,2018
"In an online community, new words come and go: today's ""haha"" may be replaced by tomorrow's ""lol."" Changes in online writing are usually studied as a social process, with innovations diffusing through a network of individuals in a speech community. But unlike other types of innovation, language change is shaped and constrained by the system in which it takes part. To investigate the links between social and structural factors in language change, we undertake a large-scale analysis of nonstandard word growth in the online community Reddit. We find that dissemination across many linguistic contexts is a sign of growth: words that appear in more linguistic contexts grow faster and survive longer. We also find that social dissemination likely plays a less important role in explaining word growth and decline than previously hypothesized.",http://aclweb.org/anthology/D18-1467,D18-1,D18-1467,https://arxiv.org/abs/1709.00345,"('Ian Stewart', 'Jacob Eisenstein')",10,"Making ""fetch"" happen: The influence of social and linguistic context on nonstandard word growth and decline",EMNLP,2018
"Latent representation learned from multi-layered neural networks via hierarchical feature abstraction enables recent success of deep learning. Under the deep learning framework, generalization performance highly depends on the learned latent representation which is obtained from an appropriate training scenario with a task-specific objective on a designed network model. In this work, we propose a novel latent space modeling method to learn better latent representation. We designed a neural network model based on the assumption that good base representation can be attained by maximizing the total correlation between the input, latent, and output variables. From the base model, we introduce a semantic noise modeling method which enables class-conditional perturbation on latent space to enhance the representational power of learned latent feature. During training, latent vector representation can be stochastically perturbed by a modeled class-conditional additive noise while maintaining its original semantic feature. It implicitly brings the effect of semantic augmentation on the latent space. The proposed model can be easily learned by back-propagation with common gradient-based optimization algorithms. Experimental results show that the proposed method helps to achieve performance benefits against various previous approaches. We also provide the empirical analyses for the proposed class-conditional perturbation process including t-SNE visualization.",http://aclweb.org/anthology/D18-1468,D18-1,D18-1468,https://arxiv.org/abs/1611.01268,['Yugo Murawaki'],10,Analyzing Correlated Evolution of Multiple Features Using Latent Representations,EMNLP,2018
"The retrofitting techniques, which inject external resources into word representations, have compensated the weakness of distributed representations in semantic and relational knowledge between words. Implicitly retrofitting word vectors by expansional technique showed that the method outperforms retrofitting in word similarity task with generalization. In this paper, we propose deep extrofitting: in-depth stacking of extrofitting. We first stack extrofitting for word vector generalization. Next, we combine extrofitting with retrofitting, finding new vector space on specialization that prevents retrofitting from converging in a few iterations. When experimenting with GloVe, we show that our methods outperform the previous methods on most of word similarity task while requiring only synonyms as external resources. We also report further analysis on the effect of word vector specialization and word vector generalization in text classification task.",http://aclweb.org/anthology/D18-1469,D18-1,D18-1469,https://arxiv.org/abs/1808.07337,"('Dirk Hovy', 'Christoph Purschke')",10,Capturing Regional Variation with Distributed Place Representations and Geographic Retrofitting,EMNLP,2018
"Human social interaction is often intermittent. Two acquainted persons can have extended periods without social interaction punctuated by periods of repeated interaction. In this case, the repeated interaction can be characterized by a seed initiative by either of the persons and a number of follow-up interactions. The tendency to initiate social interaction plays an important role in the formation of social networks and is in general not symmetric between persons. In this paper, we study the dynamics of initiative by analysing and modeling a detailed call and text message network sampled from a group of 700 individuals. We show that in an average relationship between two individuals, one part is almost twice as likely to initiate communication compared to the other part. The asymmetry has social consequences and ultimately might lead to the discontinuation of a relationship. We explain the observed asymmetry by a positive feedback mechanism where individuals already taking initiative are more likely to take initiative in the future. In general, people with many initiatives receive attention from a broader spectrum of friends than people with few initiatives. Lastly, we compare the likelihood of taking initiative with the basic personality traits of the five factor model.",http://aclweb.org/anthology/D18-1470,D18-1,D18-1470,https://arxiv.org/abs/1604.06260,"('Farzana Rashid', 'Eduardo Blanco')",10,Characterizing Interactions and Relationships between People,EMNLP,2018
"The recent surge in women reporting sexual assault and harassment (e.g., #metoo campaign) has highlighted a longstanding societal crisis. This injustice is partly due to a culture of discrediting women who report such crimes and also, rape myths (e.g., 'women lie about rape'). Social web can facilitate the further proliferation of deceptive beliefs and culture of rape myths through intentional messaging by malicious actors. This multidisciplinary study investigates Twitter posts related to sexual assaults and rape myths for characterizing the types of malicious intent, which leads to the beliefs on discrediting women and rape myths. Specifically, we first propose a novel malicious intent typology for social media using the guidance of social construction theory from policy literature that includes Accusational, Validational, or Sensational intent categories. We then present and evaluate a malicious intent classification model for a Twitter post using semantic features of the intent senses learned with the help of convolutional neural networks. Lastly, we analyze a Twitter dataset of four months using the intent classification model to study narrative contexts in which malicious intents are expressed and discuss their implications for gender violence policy design.",http://aclweb.org/anthology/D18-1471,D18-1,D18-1471,https://arxiv.org/abs/1810.01012,"('Eric Holgate', 'Isabel Cachola', 'Daniel Preoţiuc-Pietro', 'Junyi Jessy Li')",10,Why Swear? Analyzing and Inferring the Intentions of Vulgar Expressions,EMNLP,2018
"Nonlinearity is crucial to the performance of a deep (neural) network (DN). To date there has been little progress understanding the menagerie of available nonlinearities, but recently progress has been made on understanding the r\^ole played by piecewise affine and convex nonlinearities like the ReLU and absolute value activation functions and max-pooling. In particular, DN layers constructed from these operations can be interpreted as {\em max-affine spline operators} (MASOs) that have an elegant link to vector quantization (VQ) and $K$-means. While this is good theoretical progress, the entire MASO approach is predicated on the requirement that the nonlinearities be piecewise affine and convex, which precludes important activation functions like the sigmoid, hyperbolic tangent, and softmax. {\em This paper extends the MASO framework to these and an infinitely large class of new nonlinearities by linking deterministic MASOs with probabilistic Gaussian Mixture Models (GMMs).} We show that, under a GMM, piecewise affine, convex nonlinearities like ReLU, absolute value, and max-pooling can be interpreted as solutions to certain natural ""hard"" VQ inference problems, while sigmoid, hyperbolic tangent, and softmax can be interpreted as solutions to corresponding ""soft"" VQ inference problems. We further extend the framework by hybridizing the hard and soft VQ optimizations to create a $\beta$-VQ inference that interpolates between hard, soft, and linear VQ inference. A prime example of a $\beta$-VQ DN nonlinearity is the {\em swish} nonlinearity, which offers state-of-the-art performance in a range of computer vision tasks but was developed ad hoc by experimentation. Finally, we validate with experiments an important assertion of our theory, namely that DN performance can be significantly improved by enforcing orthogonality in its linear filters.",http://aclweb.org/anthology/D18-1472,D18-1,D18-1472,https://arxiv.org/abs/1810.09274,"('Steffen Eger', 'Paul Youssef', 'Iryna Gurevych')",10,Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks,EMNLP,2018
"Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks such as image captioning and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact algorithm significantly improves performance over the stochastic approximation and outperforms soft attention.",http://aclweb.org/anthology/D18-1473,D18-1,D18-1473,https://arxiv.org/abs/1808.10024,"('Shijie Wu', 'Pamela Shapiro', 'Ryan Cotterell')",10,Hard Non-Monotonic Attention for Character-Level Transduction,EMNLP,2018
We investigate theoretically the dynamics of a spatially symmetric shuttle-system subjected to an ac gate voltage. We demonstrate that in such a system parametric excitation gives rise to mechanical vibrations when the frequency of the ac signal is close to the eigenfrequency of the mechanical subsystem. These mechanical oscillations result in a dc shuttle current in a certain direction due to spontaneous symmetry breaking. The direction of the current is defined by the phase shift between the ac gate voltage and the parametrically excited mechanical oscillations. Dependance of the shuttle current on the dc gate voltage is also analyzed.,http://aclweb.org/anthology/D18-1474,D18-1,D18-1474,https://arxiv.org/abs/1212.1035,"('Tsu-Jui Fu', 'Wei-Yun Ma')",10,Speed Reading: Learning to Read ForBackward via Shuttle,EMNLP,2018
"Graph theory provides a primary tool for analyzing and designing computer communication networks. In the past few decades, Graph theory has been used to study various types of networks, including the Internet, wide Area Networks, Local Area Networks, and networking protocols such as border Gateway Protocol, Open shortest Path Protocol, and Networking Networks. In this paper, we present some key graph theory concepts used to represent different types of networks. Then we describe how networks are modeled to investigate problems related to network protocols. Finally, we present some of the tools used to generate graph for representing practical networks.",http://aclweb.org/anthology/D18-1475,D18-1,D18-1475,https://arxiv.org/abs/0907.3099,"('Baosong Yang', 'Zhaopeng Tu', 'Derek F. Wong', 'Fandong Meng', 'Lidia S. Chao', 'Tong Zhang')",10,Modeling Localness for Self-Attention Networks,EMNLP,2018
"We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images.",http://aclweb.org/anthology/D18-1476,D18-1,D18-1476,https://arxiv.org/abs/1809.08799,"('Anoop R Katti', 'Christian Reisswig', 'Cordula Guder', 'Sebastian Brarda', 'Steffen Bickel', 'Johannes Höhne', 'Jean Baptiste Faddoul')",10,Chargrid: Towards Understanding 2D Documents,EMNLP,2018
"Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model on translation by incorporating SRU into the architecture.",http://aclweb.org/anthology/D18-1477,D18-1,D18-1477,https://arxiv.org/abs/1709.02755,"('Tao Lei', 'Yu Zhang', 'Sida I. Wang', 'Hui Dai', 'Yoav Artzi')",10,Simple Recurrent Units for Highly Parallelizable Recurrence,EMNLP,2018
"Pseudo-relevance feedback (PRF) is commonly used to boost the performance of traditional information retrieval (IR) models by using top-ranked documents to identify and weight new query terms, thereby reducing the effect of query-document vocabulary mismatches. While neural retrieval models have recently demonstrated strong results for ad-hoc retrieval, combining them with PRF is not straightforward due to incompatibilities between existing PRF approaches and neural architectures. To bridge this gap, we propose an end-to-end neural PRF framework that can be used with existing neural IR models by embedding different neural models as building blocks. Extensive experiments on two standard test collections confirm the effectiveness of the proposed NPRF framework in improving the performance of two state-of-the-art neural IR models.",http://aclweb.org/anthology/D18-1478,D18-1,D18-1478,https://arxiv.org/abs/1810.12936,"('Canjia Li', 'Yingfei Sun', 'Ben He', 'Le Wang', 'Kai Hui', 'Andrew Yates', 'Le Sun', 'Jungang Xu')",10,NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval,EMNLP,2018
"Learning a matching function between two text sequences is a long standing problem in NLP research. This task enables many potential applications such as question answering and paraphrase identification. This paper proposes Co-Stack Residual Affinity Networks (CSRAN), a new and universal neural architecture for this problem. CSRAN is a deep architecture, involving stacked (multi-layered) recurrent encoders. Stacked/Deep architectures are traditionally difficult to train, due to the inherent weaknesses such as difficulty with feature propagation and vanishing gradients. CSRAN incorporates two novel components to take advantage of the stacked architecture. Firstly, it introduces a new bidirectional alignment mechanism that learns affinity weights by fusing sequence pairs across stacked hierarchies. Secondly, it leverages a multi-level attention refinement component between stacked recurrent layers. The key intuition is that, by leveraging information across all network hierarchies, we can not only improve gradient flow but also improve overall performance. We conduct extensive experiments on six well-studied text sequence matching datasets, achieving state-of-the-art performance on all.",http://aclweb.org/anthology/D18-1479,D18-1,D18-1479,https://arxiv.org/abs/1810.02938,"('Yi Tay', 'Anh Tuan Luu', 'Siu Cheung Hui')",10,Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences,EMNLP,2018
"A hallmark of variational autoencoders (VAEs) for text processing is their combination of powerful encoder-decoder models, such as LSTMs, with simple latent distributions, typically multivariate Gaussians. These models pose a difficult optimization problem: there is an especially bad local optimum where the variational posterior always equals the prior and the model does not use the latent variable at all, a kind of ""collapse"" which is encouraged by the KL divergence term of the objective. In this work, we experiment with another choice of latent distribution, namely the von Mises-Fisher (vMF) distribution, which places mass on the surface of the unit hypersphere. With this choice of prior and posterior, the KL divergence term now only depends on the variance of the vMF distribution, giving us the ability to treat it as a fixed hyperparameter. We show that doing so not only averts the KL collapse, but consistently gives better likelihoods than Gaussians across a range of modeling conditions, including recurrent language modeling and bag-of-words document modeling. An analysis of the properties of our vMF representations shows that they learn richer and more nuanced structures in their latent representations than their Gaussian counterparts.",http://aclweb.org/anthology/D18-1480,D18-1,D18-1480,https://arxiv.org/abs/1808.10805,"('Jiacheng Xu', 'Greg Durrett')",10,Spherical Latent Spaces for Stable Variational Autoencoders,EMNLP,2018
"In order to learn universal sentence representations, previous methods focus on complex recurrent neural networks or supervised learning. In this paper, we propose a mean-max attention autoencoder (mean-max AAE) within the encoder-decoder framework. Our autoencoder rely entirely on the MultiHead self-attention mechanism to reconstruct the input sequence. In the encoding we propose a mean-max strategy that applies both mean and max pooling operations over the hidden vectors to capture diverse information of the input. To enable the information to steer the reconstruction process dynamically, the decoder performs attention over the mean-max representation. By training our model on a large collection of unlabelled data, we obtain high-quality representations of sentences. Experimental results on a broad range of 10 transfer tasks demonstrate that our model outperforms the state-of-the-art unsupervised single methods, including the classical skip-thoughts and the advanced skip-thoughts+LN model. Furthermore, compared with the traditional recurrent neural network, our mean-max AAE greatly reduce the training time.",http://aclweb.org/anthology/D18-1481,D18-1,D18-1481,https://arxiv.org/abs/1809.06590,"('Minghua Zhang', 'Yunfang Wu', 'Weikang Li', 'Wei Li')",10,Learning Universal Sentence Representations with Mean-Max Attention Autoencoder,EMNLP,2018
"While the celebrated Word2Vec technique yields semantically rich representations for individual words, there has been relatively less success in extending to generate unsupervised sentences or documents embeddings. Recent work has demonstrated that a distance measure between documents called \emph{Word Mover's Distance} (WMD) that aligns semantically similar words, yields unprecedented KNN classification accuracy. However, WMD is expensive to compute, and it is hard to extend its use beyond a KNN classifier. In this paper, we propose the \emph{Word Mover's Embedding } (WME), a novel approach to building an unsupervised document (sentence) embedding from pre-trained word embeddings. In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks, the proposed technique consistently matches or outperforms state-of-the-art techniques, with significantly higher accuracy on problems of short length.",http://aclweb.org/anthology/D18-1482,D18-1,D18-1482,https://arxiv.org/abs/1811.01713,"('Lingfei Wu', 'Ian En-Hsu Yen', 'Kun Xu', 'Fangli Xu', 'Avinash Balakrishnan', 'Pin-Yu Chen', 'Pradeep Ravikumar', 'Michael J. Witbrock')",10,Word Mover's Embedding: From Word2Vec to Document Embedding,EMNLP,2018
"Clustering news across languages enables efficient media monitoring by aggregating articles from multilingual sources into coherent stories. Doing so in an online setting allows scalable processing of massive news streams. To this end, we describe a novel method for clustering an incoming stream of multilingual documents into monolingual and crosslingual story clusters. Unlike typical clustering approaches that consider a small and known number of labels, we tackle the problem of discovering an ever growing number of cluster labels in an online fashion, using real news datasets in multiple languages. Our method is simple to implement, computationally efficient and produces state-of-the-art results on datasets in German, English and Spanish.",http://aclweb.org/anthology/D18-1483,D18-1,D18-1483,https://arxiv.org/abs/1809.00540,"('Sebastião Miranda', 'Arturs Znotins', 'Shay B. Cohen', 'Guntis Barzdins')",10,Multilingual Clustering of Streaming News,EMNLP,2018
"Neural text classification methods typically treat output classes as categorical labels which lack description and semantics. This leads to an inability to train them well on large label sets or to generalize to unseen labels and makes speed and parameterization dependent on the size of the label set. Joint input-label space methods ameliorate the above issues by exploiting label texts or descriptions, but often at the expense of weak performance on the labels seen frequently during training. In this paper, we propose a label-aware text classification model which addresses these issues without compromising performance on the seen labels. The model consists of a joint input-label multiplicative space and a label-set-size independent classification unit and is trained with cross-entropy loss to optimize accuracy. We evaluate our model on text classification for multilingual news and for biomedical text with a large label set. The label-aware model consistently outperforms both monolingual and multilingual classification models which do not leverage label semantics and previous joint input-label space models.",http://aclweb.org/anthology/D18-1484,D18-1,D18-1484,https://arxiv.org/abs/1806.06219,"('Honglun Zhang', 'Liqiang Xiao', 'Wenqing Chen', 'Yongkun Wang', 'Yaohui Jin')",10,Multi-Task Label Embedding for Text Classification,EMNLP,2018
"We propose a novel model for multi-label text classification, which is based on sequence-to-sequence learning. The model generates higher-level semantic unit representations with multi-level dilated convolution as well as a corresponding hybrid attention mechanism that extracts both the information at the word-level and the level of the semantic unit. Our designed dilated convolution effectively reduces dimension and supports an exponential expansion of receptive fields without loss of local information, and the attention-over-attention mechanism is able to capture more summary relevant information from the source context. Results of our experiments show that the proposed model has significant advantages over the baseline models on the dataset RCV1-V2 and Ren-CECps, and our analysis demonstrates that our model is competitive to the deterministic hierarchical models and it is more robust to classifying low-frequency labels.",http://aclweb.org/anthology/D18-1485,D18-1,D18-1485,https://arxiv.org/abs/1808.08561,"('Junyang Lin', 'Qi Su', 'Pengcheng Yang', 'Shuming Ma', 'Xu Sun')",10,Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification,EMNLP,2018
"Most natural language processing systems based on machine learning are not robust to domain shift. For example, a state-of-the-art syntactic dependency parser trained on Wall Street Journal sentences has an absolute drop in performance of more than ten points when tested on textual data from the Web. An efficient solution to make these methods more robust to domain shift is to first learn a word representation using large amounts of unlabeled data from both domains, and then use this representation as features in a supervised learning algorithm. In this paper, we propose to use hidden Markov models to learn word representations for part-of-speech tagging. In particular, we study the influence of using data from the source, the target or both domains to learn the representation and the different ways to represent words using an HMM.",http://aclweb.org/anthology/D18-1486,D18-1,D18-1486,https://arxiv.org/abs/1312.4092,"('Liqiang Xiao', 'Honglun Zhang', 'Wenqing Chen', 'Yongkun Wang', 'Yaohui Jin')",10,MCapsNet: Capsule Network for Text with Multi-Task Learning,EMNLP,2018
"Importance of document clustering is now widely acknowledged by researchers for better management, smart navigation, efficient filtering, and concise summarization of large collection of documents like World Wide Web (WWW). The next challenge lies in semantically performing clustering based on the semantic contents of the document. The problem of document clustering has two main components: (1) to represent the document in such a form that inherently captures semantics of the text. This may also help to reduce dimensionality of the document, and (2) to define a similarity measure based on the semantic representation such that it assigns higher numerical values to document pairs which have higher semantic relationship. Feature space of the documents can be very challenging for document clustering. A document may contain multiple topics, it may contain a large set of class-independent general-words, and a handful class-specific core-words. With these features in mind, traditional agglomerative clustering algorithms, which are based on either Document Vector model (DVM) or Suffix Tree model (STC), are less efficient in producing results with high cluster quality. This paper introduces a new approach for document clustering based on the Topic Map representation of the documents. The document is being transformed into a compact form. A similarity measure is proposed based upon the inferred information through topic maps data and structures. The suggested method is implemented using agglomerative hierarchal clustering and tested on standard Information retrieval (IR) datasets. The comparative experiment reveals that the proposed approach is effective in improving the cluster quality.",http://aclweb.org/anthology/D18-1487,D18-1,D18-1487,https://arxiv.org/abs/1112.6219,"('Katherine Keith', ""Brendan O'Connor"")",10,Uncertainty-aware generative models for inferring document class prevalence,EMNLP,2018
"Causal understanding is essential for many kinds of decision-making, but causal inference from observational data has typically only been applied to structured, low-dimensional datasets. While text classifiers produce low-dimensional outputs, their use in causal inference has not previously been studied. To facilitate causal analyses based on language data, we consider the role that text classifiers can play in causal inference through established modeling mechanisms from the causality literature on missing data and measurement error. We demonstrate how to conduct causal analyses using text classifiers on simulated and Yelp data, and discuss the opportunities and challenges of future work that uses text data in causal inference.",http://aclweb.org/anthology/D18-1488,D18-1,D18-1488,https://arxiv.org/abs/1810.00956,"('Zach Wood-Doughty', 'Ilya Shpitser', 'Mark Dredze')",10,Challenges of Using Text Classifiers for Causal Inference,EMNLP,2018
"This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: https://github.com/nttcslab-nlp/doc_lm.",http://aclweb.org/anthology/D18-1489,D18-1,D18-1489,https://arxiv.org/abs/1808.10143,"('Sho Takase', 'Jun Suzuki', 'Masaaki Nagata')",10,Direct Output Connection for a High-Rank Language Model,EMNLP,2018
"In this paper we introduce a novel pattern match neural network architecture that uses neighbor similarity scores as features, eliminating the need for feature engineering in a disfluency detection task. We evaluate the approach in disfluency detection for four different speech genres, showing that the approach is as effective as hand-engineered pattern match features when used on in-domain data and achieves superior performance in cross-domain scenarios.",http://aclweb.org/anthology/D18-1490,D18-1,D18-1490,https://arxiv.org/abs/1811.07236,"('Paria Jamshid Lou', 'Peter Anderson', 'Mark Johnson')",10,Disfluency Detection using Auto-Correlational Neural Networks,EMNLP,2018
"LSTMs are powerful tools for modeling contextual information, as evidenced by their success at the task of language modeling. However, modeling contexts in very high dimensional space can lead to poor generalizability. We introduce the Pyramidal Recurrent Unit (PRU), which enables learning representations in high dimensional space with more generalization power and fewer parameters. PRUs replace the linear transformation in LSTMs with more sophisticated interactions including pyramidal and grouped linear transformations. This architecture gives strong results on word-level language modeling while reducing the number of parameters significantly. In particular, PRU improves the perplexity of a recent state-of-the-art language model Merity et al. (2018) by up to 1.3 points while learning 15-20% fewer parameters. For similar number of model parameters, PRU outperforms all previous RNN models that exploit different gating mechanisms and transformations. We provide a detailed examination of the PRU and its behavior on the language modeling tasks. Our code is open-source and available at https://sacmehta.github.io/PRU/",http://aclweb.org/anthology/D18-1491,D18-1,D18-1491,https://arxiv.org/abs/1808.09029,"('Sachin Mehta', 'Rik Koncel-Kedziorski', 'Mohammad Rastegari', 'Hannaneh Hajishirzi')",10,Pyramidal Recurrent Unit for Language Modeling,EMNLP,2018
"Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where structured prediction is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.",http://aclweb.org/anthology/D18-1492,D18-1,D18-1492,https://arxiv.org/abs/1808.06161,"('Haoyue Shi', 'Hao Zhou', 'Jiaze Chen', 'Lei Li')",10,On Tree-Based Neural Sentence Modeling,EMNLP,2018
"Most language modeling methods rely on large-scale data to statistically learn the sequential patterns of words. In this paper, we argue that words are atomic language units but not necessarily atomic semantic units. Inspired by HowNet, we use sememes, the minimum semantic units in human languages, to represent the implicit semantics behind words for language modeling, named Sememe-Driven Language Model (SDLM). More specifically, to predict the next word, SDLM first estimates the sememe distribution gave textual context. Afterward, it regards each sememe as a distinct semantic expert, and these experts jointly identify the most probable senses and the corresponding word. In this way, SDLM enables language models to work beyond word-level manipulation to fine-grained sememe-level semantics and offers us more powerful tools to fine-tune language models and improve the interpretability as well as the robustness of language models. Experiments on language modeling and the downstream application of headline gener- ation demonstrate the significant effect of SDLM. Source code and data used in the experiments can be accessed at https:// github.com/thunlp/SDLM-pytorch.",http://aclweb.org/anthology/D18-1493,D18-1,D18-1493,https://arxiv.org/abs/1810.12387,"('Yihong Gu', 'Jun Yan', 'Hao Zhu', 'Zhiyuan Liu', 'Ruobing Xie', 'Maosong Sun', 'Fen Lin', 'Leyu Lin')",10,Language Modeling with Sparse Product of Sememe Experts,EMNLP,2018
"The goal of our industrial ticketing system is to retrieve a relevant solution for an input query, by matching with historical tickets stored in knowledge base. A query is comprised of subject and description, while a historical ticket consists of subject, description and solution. To retrieve a relevant solution, we use textual similarity paradigm to learn similarity in the query and historical tickets. The task is challenging due to significant term mismatch in the query and ticket pairs of asymmetric lengths, where subject is a short text but description and solution are multi-sentence texts. We present a novel Replicated Siamese LSTM model to learn similarity in asymmetric text pairs, that gives 22% and 7% gain (Accuracy@10) for retrieval task, respectively over unsupervised and supervised baselines. We also show that the topic and distributed semantic features for short and long texts improved both similarity learning and retrieval.",http://aclweb.org/anthology/D18-1494,D18-1,D18-1494,https://arxiv.org/abs/1807.02854,"('Minghui Huang', 'Yanghui Rao', 'Yuwei Liu', 'Haoran Xie', 'Fu Lee Wang')",10,Siamese Network-Based Supervised Topic Modeling,EMNLP,2018
"As the emergence and the thriving development of social networks, a huge number of short texts are accumulated and need to be processed. Inferring latent topics of collected short texts is useful for understanding its hidden structure and predicting new contents. Unlike conventional topic models such as latent Dirichlet allocation (LDA), a biterm topic model (BTM) was recently proposed for short texts to overcome the sparseness of document-level word co-occurrences by directly modeling the generation process of word pairs. Stochastic inference algorithms based on collapsed Gibbs sampling (CGS) and collapsed variational inference have been proposed for BTM. However, they either require large computational complexity, or rely on very crude estimation. In this work, we develop a stochastic divergence minimization inference algorithm for BTM to estimate latent topics more accurately in a scalable way. Experiments demonstrate the superiority of our proposed algorithm compared with existing inference algorithms.",http://aclweb.org/anthology/D18-1495,D18-1,D18-1495,https://arxiv.org/abs/1705.00394,"('Qile Zhu', 'Zheng Feng', 'Xiaolin Li')",10,GraphBTM: Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model,EMNLP,2018
"Conventional topic models are ineffective for topic extraction from microblog messages, because the data sparseness exhibited in short messages lacking structure and contexts results in poor message-level word co-occurrence patterns. To address this issue, we organize microblog messages as conversation trees based on their reposting and replying relations, and propose an unsupervised model that jointly learns word distributions to represent: 1) different roles of conversational discourse, 2) various latent topics in reflecting content information. By explicitly distinguishing the probabilities of messages with varying discourse roles in containing topical words, our model is able to discover clusters of discourse words that are indicative of topical content. In an automatic evaluation on large-scale microblog corpora, our joint model yields topics with better coherence scores than competitive topic models from previous studies. Qualitative analysis on model outputs indicates that our model induces meaningful representations for both discourse and topics. We further present an empirical study on microblog summarization based on the outputs of our joint model. The results show that the jointly modeled discourse and topic representations can effectively indicate summary-worthy content in microblog conversations.",http://aclweb.org/anthology/D18-1496,D18-1,D18-1496,https://arxiv.org/abs/1809.03690,"('Akshay Srivatsan', 'Zachary Wojtowicz', 'Taylor Berg-Kirkpatrick')",10,Modeling Online Discourse with Coupled Distributed Topics,EMNLP,2018
"We propose a method for learning disentangled representations of texts that code for distinct and complementary aspects, with the aim of affording efficient model transfer and interpretability. To induce disentangled embeddings, we propose an adversarial objective based on the (dis)similarity between triplets of documents with respect to specific aspects. Our motivating application is embedding biomedical abstracts describing clinical trials in a manner that disentangles the populations, interventions, and outcomes in a given trial. We show that our method learns representations that encode these clinically salient aspects, and that these can be effectively used to perform aspect-specific retrieval. We demonstrate that the approach generalizes beyond our motivating application in experiments on two multi-aspect review corpora.",http://aclweb.org/anthology/D18-1497,D18-1,D18-1497,https://arxiv.org/abs/1804.07212,"('Sarthak Jain', 'Edward Banner', 'Jan-Willem van de Meent', 'Iain J Marshall', 'Byron C. Wallace')",10,Learning Disentangled Representations of Texts with Application to Biomedical Abstracts,EMNLP,2018
"We propose a mixture-of-experts approach for unsupervised domain adaptation from multiple sources. The key idea is to explicitly capture the relationship between a target example and different source domains. This relationship, expressed by a point-to-set metric, determines how to combine predictors trained on various domains. The metric is learned in an unsupervised fashion using meta-training. Experimental results on sentiment analysis and part-of-speech tagging demonstrate that our approach consistently outperforms multiple baselines and can robustly handle negative transfer.",http://aclweb.org/anthology/D18-1498,D18-1,D18-1498,https://arxiv.org/abs/1809.02256,"('Jiang Guo', 'Darsh Shah', 'Regina Barzilay')",10,Multi-Source Domain Adaptation with Mixture of Experts,EMNLP,2018
It has been argued that humans rapidly adapt their lexical and syntactic expectations to match the statistics of the current linguistic context. We provide further support to this claim by showing that the addition of a simple adaptation mechanism to a neural language model improves our predictions of human reading times compared to a non-adaptive model. We analyze the performance of the model on controlled materials from psycholinguistic experiments and show that it adapts not only to lexical items but also to abstract syntactic structures.,http://aclweb.org/anthology/D18-1499,D18-1,D18-1499,https://arxiv.org/abs/1808.09930,"('Marten van Schijndel', 'Tal Linzen')",10,A Neural Model of Adaptation in Reading,EMNLP,2018
"Interpreting the performance of deep learning models beyond test set accuracy is challenging. Characteristics of individual data points are often not considered during evaluation, and each data point is treated equally. We examine the impact of a test set question's difficulty to determine if there is a relationship between difficulty and performance. We model difficulty using well-studied psychometric methods on human response patterns. Experiments on Natural Language Inference (NLI) and Sentiment Analysis (SA) show that the likelihood of answering a question correctly is impacted by the question's difficulty. As DNNs are trained with more data, easy examples are learned more quickly than hard examples.",http://aclweb.org/anthology/D18-1500,D18-1,D18-1500,https://arxiv.org/abs/1702.04811,"('John Lalor', 'Hao Wu', 'Tsendsuren Munkhdalai', 'Hong Yu')",10,Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study,EMNLP,2018
"We investigate neural models' ability to capture lexicosyntactic inferences: inferences triggered by the interaction of lexical and syntactic information. We take the task of event factuality prediction as a case study and build a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts. We use this dataset, which we make publicly available, to probe the behavior of current state-of-the-art neural systems, showing that these systems make certain systematic errors that are clearly visible through the lens of factuality prediction.",http://aclweb.org/anthology/D18-1501,D18-1,D18-1501,https://arxiv.org/abs/1808.06232,"('Aaron Steven White', 'Rachel Rudinger', 'Kyle Rawlins', 'Benjamin Van Durme')",10,Lexicosyntactic Inference in Neural Models,EMNLP,2018
"In this paper, we propose the new fixed-size ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular RNN-LMs.",http://aclweb.org/anthology/D18-1502,D18-1,D18-1502,https://arxiv.org/abs/1505.01504,"('Sedtawut Watcharawittayakul', 'Mingbin Xu', 'Hui Jiang')",10,Dual Fixed-Size Ordinally Forgetting Encoding (FOFE) for Competitive Neural Language Models,EMNLP,2018
"Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks such as language modeling (Linzen et al., 2016) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures---recurrent versus non-recurrent---with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose.",http://aclweb.org/anthology/D18-1503,D18-1,D18-1503,https://arxiv.org/abs/1803.03585,"('Ke Tran', 'Arianna Bisazza', 'Christof Monz')",10,The Importance of Being Recurrent for Modeling Hierarchical Structure,EMNLP,2018
"Sentiment analysis is the Natural Language Processing (NLP) task dealing with the detection and classification of sentiments in texts. While some tasks deal with identifying the presence of sentiment in the text (Subjectivity analysis), other tasks aim at determining the polarity of the text categorizing them as positive, negative and neutral. Whenever there is a presence of sentiment in the text, it has a source (people, group of people or any entity) and the sentiment is directed towards some entity, object, event or person. Sentiment analysis tasks aim to determine the subject, the target and the polarity or valence of the sentiment. In our work, we try to automatically extract sentiment (positive or negative) from Facebook posts using a machine learning approach.While some works have been done in code-mixed social media data and in sentiment analysis separately, our work is the first attempt (as of now) which aims at performing sentiment analysis of code-mixed social media text. We have used extensive pre-processing to remove noise from raw text. Multilayer Perceptron model has been used to determine the polarity of the sentiment. We have also developed the corpus for this task by manually labeling Facebook posts with their associated sentiments.",http://aclweb.org/anthology/D18-1504,D18-1,D18-1504,https://arxiv.org/abs/1707.01184,"('Dehong Ma', 'Sujian Li', 'Houfeng Wang')",10,Joint Learning for Targeted Sentiment Analysis,EMNLP,2018
"We analyze the performance of different sentiment classification models on syntactically complex inputs like A-but-B sentences. The first contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in arXiv:1603.06318v4 [cs.LG], which incorporates explicit logic rules for sentiment classification, is ineffective. In contrast, using contextualized ELMo embeddings (arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields significantly better performance. Additionally, we provide analysis and visualizations that demonstrate ELMo's ability to implicitly learn logic rules. Finally, a crowdsourced analysis reveals how ELMo outperforms baseline models even on sentences with ambiguous sentiment labels.",http://aclweb.org/anthology/D18-1505,D18-1,D18-1505,https://arxiv.org/abs/1808.07733,"('Kalpesh Krishna', 'Preethi Jyothi', 'Mohit Iyyer')",10,Revisiting the Importance of Encoding Logic Rules in Sentiment Classification,EMNLP,2018
"In this dissertation the practical speech emotion recognition technology is studied, including several cognitive related emotion types, namely fidgetiness, confidence and tiredness. The high quality of naturalistic emotional speech data is the basis of this research. The following techniques are used for inducing practical emotional speech: cognitive task, computer game, noise stimulation, sleep deprivation and movie clips.   A practical speech emotion recognition system is studied based on Gaussian mixture model. A two-class classifier set is adopted for performance improvement under the small sample case. Considering the context information in continuous emotional speech, a Gaussian mixture model embedded with Markov networks is proposed.   A further study is carried out for system robustness analysis. First, noise reduction algorithm based on auditory masking properties is fist introduced to the practical speech emotion recognition. Second, to deal with the complicated unknown emotion types under real situation, an emotion recognition method with rejection ability is proposed, which enhanced the system compatibility against unknown emotion samples. Third, coping with the difficulties brought by a large number of unknown speakers, an emotional feature normalization method based on speaker-sensitive feature clustering is proposed. Fourth, by adding the electrocardiogram channel, a bi-modal emotion recognition system based on speech signals and electrocardiogram signals is first introduced.   The speech emotion recognition methods studied in this dissertation may be extended into the cross-language speech emotion recognition and the whispered speech emotion recognition.",http://aclweb.org/anthology/D18-1506,D18-1,D18-1506,https://arxiv.org/abs/1709.09364,"('Xiangju Li', 'Kaisong Song', 'Shi Feng', 'Daling Wang', 'Yifei Zhang')",10,A Co-Attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness,EMNLP,2018
"Computational detection and understanding of empathy is an important factor in advancing human-computer interaction. Yet to date, text-based empathy prediction has the following major limitations: It underestimates the psychological complexity of the phenomenon, adheres to a weak notion of ground truth where empathic states are ascribed by third parties, and lacks a shared corpus. In contrast, this contribution presents the first publicly available gold standard for empathy prediction. It is constructed using a novel annotation methodology which reliably captures empathy assessments by the writer of a statement using multi-item scales. This is also the first computational work distinguishing between multiple forms of empathy, empathic concern, and personal distress, as recognized throughout psychology. Finally, we present experimental results for three different predictive models, of which a CNN performs the best.",http://aclweb.org/anthology/D18-1507,D18-1,D18-1507,https://arxiv.org/abs/1808.10399,"('Sven Buechel', 'Anneke Buffone', 'Barry Slaff', 'Lyle Ungar', 'Joao Sedoc')",10,Modeling Empathy and Distress in Reaction to News Stories,EMNLP,2018
"Sentiment analysis on large-scale social media data is important to bridge the gaps between social media contents and real world activities including political election prediction, individual and public emotional status monitoring and analysis, and so on. Although textual sentiment analysis has been well studied based on platforms such as Twitter and Instagram, analysis of the role of extensive emoji uses in sentiment analysis remains light. In this paper, we propose a novel scheme for Twitter sentiment analysis with extra attention on emojis. We first learn bi-sense emoji embeddings under positive and negative sentimental tweets individually, and then train a sentiment classifier by attending on these bi-sense emoji embeddings with an attention-based long short-term memory network (LSTM). Our experiments show that the bi-sense embedding is effective for extracting sentiment-aware embeddings of emojis and outperforms the state-of-the-art models. We also visualize the attentions to show that the bi-sense emoji embedding provides better guidance on the attention mechanism to obtain a more robust understanding of the semantics and sentiments.",http://aclweb.org/anthology/D18-1508,D18-1,D18-1508,https://arxiv.org/abs/1807.07961,"('Francesco Barbieri', 'Luis Espinosa Anke', 'Jose Camacho-Collados', 'Steven Schockaert', 'Horacio Saggion')",10,Interpretable Emoji Prediction via Label-Wise Attention LSTMs,EMNLP,2018
"Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.",http://aclweb.org/anthology/D18-1509,D18-1,D18-1509,https://arxiv.org/abs/1702.02429,"('Xinyi Wang', 'Hieu Pham', 'Pengcheng Yin', 'Graham Neubig')",10,A Tree-based Decoder for Neural Machine Translation,EMNLP,2018
"Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.",http://aclweb.org/anthology/D18-1510,D18-1,D18-1510,https://arxiv.org/abs/1809.03132,"('Chenze Shao', 'Xilin Chen', 'Yang Feng')",10,Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation,EMNLP,2018
"In Neural Machine Translation (NMT), the decoder can capture the features of the entire prediction history with neural connections and representations. This means that partial hypotheses with different prefixes will be regarded differently no matter how similar they are. However, this might be inefficient since some partial hypotheses can contain only local differences that will not influence future predictions. In this work, we introduce recombination in NMT decoding based on the concept of the ""equivalence"" of partial hypotheses. Heuristically, we use a simple $n$-gram suffix based equivalence function and adapt it into beam search decoding. Through experiments on large-scale Chinese-to-English and English-to-Germen translation tasks, we show that the proposed method can obtain similar translation quality with a smaller beam size, making NMT decoding more efficient.",http://aclweb.org/anthology/D18-1511,D18-1,D18-1511,https://arxiv.org/abs/1808.08482,"('Zhisong Zhang', 'Rui Wang', 'Masao Utiyama', 'Eiichiro Sumita', 'Hai Zhao')",10,Exploring Recombination for Efficient Decoding of Neural Machine Translation,EMNLP,2018
"Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.",http://aclweb.org/anthology/D18-1512,D18-1,D18-1512,https://arxiv.org/abs/1808.07048,"('Samuel Läubli', 'Rico Sennrich', 'Martin Volk')",10,Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation,EMNLP,2018
"We compare the performance of the APT and AutoPRF metrics for pronoun translation against a manually annotated dataset comprising human judgements as to the correctness of translations of the PROTEST test suite. Although there is some correlation with the human judgements, a range of issues limit the performance of the automated metrics. Instead, we recommend the use of semi-automatic metrics and test suites in place of fully automatic metrics.",http://aclweb.org/anthology/D18-1513,D18-1,D18-1513,https://arxiv.org/abs/1808.04164,"('Liane Guillou', 'Christian Hardmeier')",10,Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point,EMNLP,2018
"Deep neural networks (DNNs) have achieved exceptional performances in many tasks, particularly, in supervised classification tasks. However, achievements with supervised classification tasks are based on large datasets with well-separated classes. Typically, real-world applications involve wild datasets that include similar classes; thus, evaluating similarities between classes and understanding relations among classes are important. To address this issue, a similarity metric, ClassSim, based on the misclassification ratios of trained DNNs is proposed herein. We conducted image recognition experiments to demonstrate that the proposed method provides better similarities compared with existing methods and is useful for classification problems. Source code including all experimental results is available at https://github.com/karino2/ClassSim/.",http://aclweb.org/anthology/D18-1514,D18-1,D18-1514,https://arxiv.org/abs/1802.01267,"('Xu Han', 'Hao Zhu', 'Pengfei Yu', 'Ziyun Wang', 'Yuan Yao', 'Zhiyuan Liu', 'Maosong Sun')",10,FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation,EMNLP,2018
"The best systems at the SemEval-16 and SemEval-17 community question answering shared tasks -- a task that amounts to question relevancy ranking -- involve complex pipelines and manual feature engineering. Despite this, many of these still fail at beating the IR baseline, i.e., the rankings provided by Google's search engine. We present a strong baseline for question relevancy ranking by training a simple multi-task feed forward network on a bag of 14 distance measures for the input question pair. This baseline model, which is fast to train and uses only language-independent features, outperforms the best shared task systems on the task of retrieving relevant previously asked questions.",http://aclweb.org/anthology/D18-1515,D18-1,D18-1515,https://arxiv.org/abs/1808.08836,"('Ana Gonzalez', 'Isabelle Augenstein', 'Anders Søgaard')",10,A strong baseline for question relevancy ranking,EMNLP,2018
"Research on link prediction in knowledge graphs has mainly focused on static multi-relational data. In this work we consider temporal knowledge graphs where relations between entities may only hold for a time interval or a specific point in time. In line with previous work on static knowledge graphs, we propose to address this problem by learning latent entity and relation type representations. To incorporate temporal information, we utilize recurrent neural networks to learn time-aware representations of relation types which can be used in conjunction with existing latent factorization methods. The proposed approach is shown to be robust to common challenges in real-world KGs: the sparsity and heterogeneity of temporal expressions. Experiments show the benefits of our approach on four temporal KGs. The data sets are available under a permissive BSD-3 license 1.",http://aclweb.org/anthology/D18-1516,D18-1,D18-1516,https://arxiv.org/abs/1809.03202,"('Alberto Garcia-Duran', 'Sebastijan Dumančić', 'Mathias Niepert')",10,Learning Sequence Encoders for Temporal Knowledge Graph Completion,EMNLP,2018
"There have been some works that learn a lexicon together with the corpus to improve the word embeddings. However, they either model the lexicon separately but update the neural networks for both the corpus and the lexicon by the same likelihood, or minimize the distance between all of the synonym pairs in the lexicon. Such methods do not consider the relatedness and difference of the corpus and the lexicon, and may not be the best optimized. In this paper, we propose a novel method that considers the relatedness and difference of the corpus and the lexicon. It trains word embeddings by learning the corpus to predicate a word and its corresponding synonym under the context at the same time. For polysemous words, we use a word sense disambiguation filter to eliminate the synonyms that have different meanings for the context. To evaluate the proposed method, we compare the performance of the word embeddings trained by our proposed model, the control groups without the filter or the lexicon, and the prior works in the word similarity tasks and text classification task. The experimental results show that the proposed model provides better embeddings for polysemous words and improves the performance for text classification.",http://aclweb.org/anthology/D18-1517,D18-1,D18-1517,https://arxiv.org/abs/1707.07628,"('Weiyi Lu', 'Thien Huu Nguyen')",10,Similar but not the Same: Word Sense Disambiguation Improves Event Detection via Neural Representation Matching,EMNLP,2018
"Recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words. However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only single representation per word. Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches. In this paper we propose the Adaptive Skip-gram model which is a nonparametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution. We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on word-sense induction task.",http://aclweb.org/anthology/D18-1518,D18-1,D18-1518,https://arxiv.org/abs/1502.07257,"('Hongyin Luo', 'Jim Glass')",10,Learning Word Representations with Cross-Sentence Dependency for End-to-End Co-reference Resolution,EMNLP,2018
"Hypernym Discovery is the task of identifying potential hypernyms for a given term. A hypernym is a more generalized word that is super-ordinate to more specific words. This paper explores several approaches that rely on co-occurrence frequencies of word pairs, Hearst Patterns based on regular expressions, and word embeddings created from the UMBC corpus. Our system Babbage participated in Subtask 1A for English and placed 6th of 19 systems when identifying concept hypernyms, and 12th of 18 systems for entity hypernyms.",http://aclweb.org/anthology/D18-1519,D18-1,D18-1519,https://arxiv.org/abs/1805.10271,"('Hong-You Chen', 'Cheng-Syuan Lee', 'Keng-Te Liao', 'Shou-de Lin')",10,Word Relation Autoencoder for Unseen Hypernym Extraction Using Word Embeddings,EMNLP,2018
"Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.",http://aclweb.org/anthology/D18-1520,D18-1,D18-1520,https://arxiv.org/abs/1704.08960,['Akira Utsumi'],10,Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation,EMNLP,2018
"Word embeddings play a significant role in many modern NLP systems. Since learning one representation per word is problematic for polysemous words and homonymous words, researchers propose to use one embedding per word sense. Their approaches mainly train word sense embeddings on a corpus. In this paper, we propose to use word sense definitions to learn one embedding per word sense. Experimental results on word similarity tasks and a word sense disambiguation task show that word sense embeddings produced by our approach are of high quality.",http://aclweb.org/anthology/D18-1521,D18-1,D18-1521,https://arxiv.org/abs/1606.04835,"('Jieyu Zhao', 'Yichao Zhou', 'Zeyu Li', 'Wei Wang', 'Kai-Wei Chang')",10,Learning Gender-Neutral Word Embeddings,EMNLP,2018
"We introduce a weakly supervised approach for inferring the property of abstractness of words and expressions in the complete absence of labeled data. Exploiting only minimal linguistic clues and the contextual usage of a concept as manifested in textual data, we train sufficiently powerful classifiers, obtaining high correlation with human labels. The results imply the applicability of this approach to additional properties of concepts, additional languages, and resource-scarce scenarios.",http://aclweb.org/anthology/D18-1522,D18-1,D18-1522,https://arxiv.org/abs/1809.01285,"('Ella Rabinovich', 'Benjamin Sznajder', 'Artem Spector', 'Ilya Shnayderman', 'Ranit Aharonov', 'David Konopnicki', 'Noam Slonim')",10,Learning Concept Abstractness Using Weak Supervision,EMNLP,2018
"An established method for Word Sense Induction (WSI) uses a language model to predict probable substitutes for target words, and induces senses by clustering these resulting substitute vectors.   We replace the ngram-based language model (LM) with a recurrent one. Beyond being more accurate, the use of the recurrent LM allows us to effectively query it in a creative way, using what we call dynamic symmetric patterns.   The combination of the RNN-LM and the dynamic symmetric patterns results in strong substitute vectors for WSI, allowing to surpass the current state-of-the-art on the SemEval 2013 WSI shared task by a large margin.",http://aclweb.org/anthology/D18-1523,D18-1,D18-1523,https://arxiv.org/abs/1808.08518,"('Asaf Amrami', 'Yoav Goldberg')",10,Word Sense Induction with Neural biLM and Symmetric Patterns,EMNLP,2018
"Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",http://aclweb.org/anthology/D18-1524,D18-1,D18-1524,https://arxiv.org/abs/1705.02364,"('Jamie Kiros', 'William Chan')",10,InferLite: Simple Universal Sentence Representations from Natural Language Inference Data,EMNLP,2018
"We consider visual domains in which a class label specifies the content of an image, and class-irrelevant properties that differentiate instances constitute the style. We present a domain-independent method that permits the open-ended recombination of style of one image with the content of another. Open ended simply means that the method generalizes to style and content not present in the training data. The method starts by constructing a content embedding using an existing deep metric-learning technique. This trained content encoder is incorporated into a variational autoencoder (VAE), paired with a to-be-trained style encoder. The VAE reconstruction loss alone is inadequate to ensure a decomposition of the latent representation into style and content. Our method thus includes an auxiliary loss, leakage filtering, which ensures that no style information remaining in the content representation is used for reconstruction and vice versa. We synthesize novel images by decoding the style representation obtained from one image with the content representation from another. Using this method for data-set augmentation, we obtain state-of-the-art performance on few-shot learning tasks.",http://aclweb.org/anthology/D18-1525,D18-1,D18-1525,https://arxiv.org/abs/1810.00110,"('Olga Kovaleva', 'Anna Rumshisky', 'Alexey Romanov')",10,Similarity-Based Reconstruction Loss for Meaning Representation,EMNLP,2018
"We investigate the effects of multi-task learning using the recently introduced task of semantic tagging. We employ semantic tagging as an auxiliary task for three different NLP tasks: part-of-speech tagging, Universal Dependency parsing, and Natural Language Inference. We compare full neural network sharing, partial neural network sharing, and what we term the learning what to share setting where negative transfer between tasks is less likely. Our findings show considerable improvements for all tasks, particularly in the learning what to share setting, which shows consistent gains across all tasks.",http://aclweb.org/anthology/D18-1526,D18-1,D18-1526,https://arxiv.org/abs/1808.09716,"('Mostafa Abdou', 'Artur Kulmizev', 'Vinit Ravishankar', 'Lasha Abzianidze', 'Johan Bos')",10,What can we learn from Semantic Tagging?,EMNLP,2018
"We introduce the cross-match test - an exact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric for word embeddings. We show that cross-match is an effective means of measuring distributional similarity between different vector representations and of evaluating the statistical significance of different vector embedding models. Additionally, we find that cross-match can be used to provide a quantitative measure of linguistic similarity for selecting bridge languages for machine translation. We demonstrate that the results of the hypothesis test align with our expectations and note that the framework of two sample hypothesis testing is not limited to word embeddings and can be extended to all vector representations.",http://aclweb.org/anthology/D18-1527,D18-1,D18-1527,https://arxiv.org/abs/1709.00831,"('Rujun Han', 'Michael Gill', 'Arthur Spirling', 'Kyunghyun Cho')",10,Conditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop,EMNLP,2018
"The study and modeling of driver's gaze dynamics is important because, if and how the driver is monitoring the driving environment is vital for driver assistance in manual mode, for take-over requests in highly automated mode and for semantic perception of the surround in fully autonomous mode. We developed a machine vision based framework to classify driver's gaze into context rich zones of interest and model driver's gaze behavior by representing gaze dynamics over a time period using gaze accumulation, glance duration and glance frequencies. As a use case, we explore the driver's gaze dynamic patterns during maneuvers executed in freeway driving, namely, left lane change maneuver, right lane change maneuver and lane keeping. It is shown that condensing gaze dynamics into durations and frequencies leads to recurring patterns based on driver activities. Furthermore, modeling these patterns show predictive powers in maneuver detection up to a few hundred milliseconds a priori.",http://aclweb.org/anthology/D18-1528,D18-1,D18-1528,https://arxiv.org/abs/1802.00066,"('Victoria Yaneva', 'Le An Ha', 'Richard Evans', 'Ruslan Mitkov')",10,Classifying Referential and Non-referential It Using Gaze,EMNLP,2018
"We present an iterative procedure to build a Chinese language model (LM). We segment Chinese text into words based on a word-based Chinese language model. However, the construction of a Chinese LM itself requires word boundaries. To get out of the chicken-and-egg problem, we propose an iterative procedure that alternates two operations: segmenting text into words and building an LM. Starting with an initial segmented corpus and an LM based upon it, we use a Viterbi-liek algorithm to segment another set of data. Then, we build an LM based on the second set and use the resulting LM to segment again the first corpus. The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors. Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation, but discovers unseen words surprisingly well. The resulting word-based LM has a perplexity of 188 for a general Chinese corpus.",http://aclweb.org/anthology/D18-1529,D18-1,D18-1529,https://arxiv.org/abs/cmp-lg/9606021,"('Ji Ma', 'Kuzman Ganchev', 'David Weiss')",10,State-of-the-art Chinese Word Segmentation with Bi-LSTMs,EMNLP,2018
"In Sanskrit, small words (morphemes) are combined to form compound words through a process known as Sandhi. Sandhi splitting is the process of splitting a given compound word into its constituent morphemes. Although rules governing word splitting exists in the language, it is highly challenging to identify the location of the splits in a compound word. Though existing Sandhi splitting systems incorporate these pre-defined splitting rules, they have a low accuracy as the same compound word might be broken down in multiple ways to provide syntactically correct splits.   In this research, we propose a novel deep learning architecture called Double Decoder RNN (DD-RNN), which (i) predicts the location of the split(s) with 95% accuracy, and (ii) predicts the constituent words (learning the Sandhi splitting rules) with 79.5% accuracy, outperforming the state-of-art by 20%. Additionally, we show the generalization capability of our deep learning model, by showing competitive results in the problem of Chinese word segmentation, as well.",http://aclweb.org/anthology/D18-1530,D18-1,D18-1530,https://arxiv.org/abs/1801.00428,"('Rahul Aralikatte', 'Neelamadhav Gantayat', 'Naveen Panwar', 'Anush Sankaran', 'Senthil Mani')",10,Sanskrit Sandhi Splitting using seq2(seq)2,EMNLP,2018
"Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.",http://aclweb.org/anthology/D18-1531,D18-1,D18-1531,https://arxiv.org/abs/1810.03167,"('Zhiqing Sun', 'Zhi-Hong Deng')",10,Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling,EMNLP,2018
"We present LemmaTag, a featureless neural network architecture that jointly generates part-of-speech tags and lemmas for sentences by using bidirectional RNNs with character-level and word-level embeddings. We demonstrate that both tasks benefit from sharing the encoding part of the network, predicting tag subcategories, and using the tagger output as an input to the lemmatizer. We evaluate our model across several languages with complex morphology, which surpasses state-of-the-art accuracy in both part-of-speech tagging and lemmatization in Czech, German, and Arabic.",http://aclweb.org/anthology/D18-1532,D18-1,D18-1532,https://arxiv.org/abs/1808.03703,"('Daniel Kondratyuk', 'Tomáš Gavenčiak', 'Milan Straka', 'Jan Hajič')",10,LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with BRNNs,EMNLP,2018
"The paper is devoted to study the structure of Hawaiian groups of some topological spaces. We present some behaviors of Hawaiian groups with respect to product spaces, weak join spaces, cone spaces, covering spaces and locally trivial bundles. In particular, we determine the structure of the $n$-dimensional Hawaiian group of the $m$-dimensional Hawaiian earring space, for all $1\leq m\leq n$.",http://aclweb.org/anthology/D18-1533,D18-1,D18-1533,https://arxiv.org/abs/1111.0731,"('Brendan Shillingford', 'Oiwi Parker Jones')",10,Recovering Missing Characters in Old Hawaiian Writing,EMNLP,2018
"Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result---it further improves the performance even when added to the already very strong model.",http://aclweb.org/anthology/D18-1534,D18-1,D18-1534,https://arxiv.org/abs/1609.06038,"('Natalie Schluter', 'Daniel Varab')",10,When data permutations are pathological: the case of neural natural language inference,EMNLP,2018
"Most textual entailment models focus on lexical gaps between the premise text and the hypothesis, but rarely on knowledge gaps. We focus on filling these knowledge gaps in the Science Entailment task, by leveraging an external structured knowledge base (KB) of science facts. Our new architecture combines standard neural entailment models with a knowledge lookup module. To facilitate this lookup, we propose a fact-level decomposition of the hypothesis, and verifying the resulting sub-facts against both the textual premise and the structured KB. Our model, NSnet, learns to aggregate predictions from these heterogeneous data formats. On the SciTail dataset, NSnet outperforms a simpler combination of the two predictions by 3% and the base entailment model by 5%.",http://aclweb.org/anthology/D18-1535,D18-1,D18-1535,https://arxiv.org/abs/1808.09333,"('Dongyeop Kang', 'Tushar Khot', 'Ashish Sabharwal', 'Peter Clark')",10,Bridging Knowledge Gaps in Neural Entailment via Symbolic Models,EMNLP,2018
"Semantic role labelling (SRL) is a task in natural language processing which detects and classifies the semantic arguments associated with the predicates of a sentence. It is an important step towards understanding the meaning of a natural language. There exists SRL systems for well-studied languages like English, Chinese or Japanese but there is not any such system for the Vietnamese language. In this paper, we present the first SRL system for Vietnamese with encouraging accuracy. We first demonstrate that a simple application of SRL techniques developed for English could not give a good accuracy for Vietnamese. We then introduce a new algorithm for extracting candidate syntactic constituents, which is much more accurate than the common node-mapping algorithm usually used in the identification step. Finally, in the classification step, in addition to the common linguistic features, we propose novel and useful features for use in SRL. Our SRL system achieves an $F_1$ score of 73.53\% on the Vietnamese PropBank corpus. This system, including software and corpus, is available as an open source project and we believe that it is a good baseline for the development of future Vietnamese SRL systems.",http://aclweb.org/anthology/D18-1536,D18-1,D18-1536,https://arxiv.org/abs/1705.04038,"('Jing Chen', 'Qingcai Chen', 'Xin Liu', 'Haijun Yang', 'Daohe Lu', 'Buzhou Tang')",10,The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification,EMNLP,2018
"Deep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these models are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals. We present several examples for which our methods are able to reveal interesting insights and identify the critical information contributing to the model decisions.",http://aclweb.org/anthology/D18-1537,D18-1,D18-1537,https://arxiv.org/abs/1808.03894,"('Reza Ghaeini', 'Xiaoli Fern', 'Prasad Tadepalli')",10,Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference,EMNLP,2018
"Neural models have shown several state-of-the-art performances on Semantic Role Labeling (SRL). However, the neural models require an immense amount of semantic-role corpora and are thus not well suited for low-resource languages or domains. The paper proposes a semi-supervised semantic role labeling method that outperforms the state-of-the-art in limited SRL training corpora. The method is based on explicitly enforcing syntactic constraints by augmenting the training objective with a syntactic-inconsistency loss component and uses SRL-unlabeled instances to train a joint-objective LSTM. On CoNLL-2012 English section, the proposed semi-supervised training with 1%, 10% SRL-labeled data and varying amounts of SRL-unlabeled data achieves +1.58, +0.78 F1, respectively, over the pre-trained models that were trained on SOTA architecture with ELMo on the same SRL-labeled data. Additionally, by using the syntactic-inconsistency loss on inference time, the proposed model achieves +3.67, +2.1 F1 over pre-trained model on 1%, 10% SRL-labeled data, respectively.",http://aclweb.org/anthology/D18-1538,D18-1,D18-1538,https://arxiv.org/abs/1808.09543,"('Sanket Vaibhav Mehta', 'Jay Yoon Lee', 'Jaime Carbonell')",10,Towards Semi-Supervised Learning for Deep Semantic Role Labeling,EMNLP,2018
"When the semantics of a sentence are not representable in a semantic parser's output schema, parsing will inevitably fail. Detection of these instances is commonly treated as an out-of-domain classification problem. However, there is also a more subtle scenario in which the test data is drawn from the same domain. In addition to formalizing this problem of domain-adjacency, we present a comparison of various baselines that could be used to solve it. We also propose a new simple sentence representation that emphasizes words which are unexpected. This approach improves the performance of a downstream semantic parser run on in-domain and domain-adjacent instances.",http://aclweb.org/anthology/D18-1539,D18-1,D18-1539,https://arxiv.org/abs/1808.08626,"('James Ferguson', 'Janara Christensen', 'Edward Li', 'Edgar Gonzàlez')",10,Identifying Domain Adjacent Instances for Semantic Parsers,EMNLP,2018
"The web provides a rich, open-domain environment with textual, structural, and spatial properties. We propose a new task for grounding language in this environment: given a natural language command (e.g., ""click on the second article""), choose the correct element on the web page (e.g., a hyperlink or text box). We collected a dataset of over 50,000 commands that capture various phenomena such as functional references (e.g. ""find who made this site""), relational reasoning (e.g. ""article by john""), and visual reasoning (e.g. ""top-most article""). We also implemented and analyzed three baseline models that capture different phenomena present in the dataset.",http://aclweb.org/anthology/D18-1540,D18-1,D18-1540,https://arxiv.org/abs/1808.09132,"('Panupong Pasupat', 'Tian-Shun Jiang', 'Evan Liu', 'Kelvin Guu', 'Percy Liang')",10,Mapping natural language commands to web elements,EMNLP,2018
"Grammatical error correction, like other machine learning tasks, greatly benefits from large quantities of high quality training data, which is typically expensive to produce. While writing a program to automatically generate realistic grammatical errors would be difficult, one could learn the distribution of naturallyoccurring errors and attempt to introduce them into other datasets. Initial work on inducing errors in this way using statistical machine translation has shown promise; we investigate cheaply constructing synthetic samples, given a small corpus of human-annotated data, using an off-the-rack attentive sequence-to-sequence model and a straight-forward post-processing procedure. Our approach yields error-filled artificial data that helps a vanilla bi-directional LSTM to outperform the previous state of the art at grammatical error detection, and a previously introduced model to gain further improvements of over 5% $F_{0.5}$ score. When attempting to determine if a given sentence is synthetic, a human annotator at best achieves 39.39 $F_1$ score, indicating that our model generates mostly human-like instances.",http://aclweb.org/anthology/D18-1541,D18-1,D18-1541,https://arxiv.org/abs/1810.00668,"('Sudhanshu Kasewa', 'Pontus Stenetorp', 'Sebastian Riedel')",10,Wronging a Right: Generating Better Errors to Improve Grammatical Error Detection,EMNLP,2018
"Models based on Convolutional Neural Networks (CNNs) have been proven very successful for semantic segmentation and object parsing that yield hierarchies of features. Our key insight is to build convolutional networks that take input of arbitrary size and produce object parsing output with efficient inference and learning. In this work, we focus on the task of instance segmentation and parsing which recognizes and localizes objects down to a pixel level base on deep CNN. Therefore, unlike some related work, a pixel cannot belong to multiple instances and parsing. Our model is based on a deep neural network trained for object masking that supervised with input image and follow incorporates a Conditional Random Field (CRF) with end-to-end trainable piecewise order potentials based on object parsing outputs. In each CRF unit we designed terms to capture the short range and long range dependencies from various neighbors. The accurate instance-level segmentation that our network produce is reflected by the considerable improvements obtained over previous work at high APr thresholds. We demonstrate the effectiveness of our model with extensive experiments on challenging dataset subset of PASCAL VOC2012.",http://aclweb.org/anthology/D18-1542,D18-1,D18-1542,https://arxiv.org/abs/1709.08019,"('Rob van der Goot', 'Gertjan van Noord')",10,Modeling Input Uncertainty in Neural Network Dependency Parsing,EMNLP,2018
"Previous work has suggested that parameter sharing between transition-based neural dependency parsers for related languages can lead to better performance, but there is no consensus on what parameters to share. We present an evaluation of 27 different parameter sharing strategies across 10 languages, representing five pairs of related languages, each pair from a different language family. We find that sharing transition classifier parameters always helps, whereas the usefulness of sharing word and/or character LSTM parameters varies. Based on this result, we propose an architecture where the transition classifier is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This model is linguistically motivated and obtains significant improvements over a monolingually trained baseline. We also find that sharing transition classifier parameters helps when training a parser on unrelated language pairs, but we find that, in the case of unrelated languages, sharing too many parameters does not help.",http://aclweb.org/anthology/D18-1543,D18-1,D18-1543,https://arxiv.org/abs/1808.09055,"('Miryam de Lhoneux', 'Johannes Bjerva', 'Isabelle Augenstein', 'Anders Søgaard')",10,Parameter sharing between dependency parsers for related languages,EMNLP,2018
"A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.",http://aclweb.org/anthology/D18-1544,D18-1,D18-1544,https://arxiv.org/abs/1808.10000,"('Phu Mon Htut', 'Kyunghyun Cho', 'Samuel Bowman')",10,Grammar Induction with Neural Language Models: An Unusual Replication,EMNLP,2018
"The concern of potential privacy violation has prevented efficient use of big data for improving deep learning based applications. In this paper, we propose Morphed Learning, a privacy-preserving technique for deep learning based on data morphing that, allows data owners to share their data without leaking sensitive privacy information. Morphed Learning allows the data owners to send securely morphed data and provides the server with an Augmented Convolutional layer to train the network on morphed data without performance loss. Morphed Learning has these three features: (1) Strong protection against reverse-engineering on the morphed data; (2) Acceptable computational and data transmission overhead with no correlation to the depth of the neural network; (3) No degradation of the neural network performance. Theoretical analyses on CIFAR-10 dataset and VGG-16 network show that our method is capable of providing 10^89 morphing possibilities with only 5% computational overhead and 10% transmission overhead under limited knowledge attack scenario. Further analyses also proved that our method can offer same resilience against full knowledge attack if more resources are provided.",http://aclweb.org/anthology/D18-1545,D18-1,D18-1545,https://arxiv.org/abs/1809.09968,"('Gozde Gul Sahin', 'Mark Steedman')",10,Data Augmentation via Dependency Tree Morphing for Low-Resource Languages,EMNLP,2018
"Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models often perform surprisingly well. On $14$ out of $20$ bAbI tasks, passage-only models achieve greater than $50\%$ accuracy, sometimes matching the full model. Interestingly, while CBT provides $20$-sentence stories only the last is needed for comparably accurate prediction. By comparison, SQuAD and CNN appear better-constructed.",http://aclweb.org/anthology/D18-1546,D18-1,D18-1546,https://arxiv.org/abs/1808.04926,"('Divyansh Kaushik', 'Zachary C. Lipton')",10,How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks,EMNLP,2018
"This position paper formalises an abstract model for complex negotiation dialogue. This model is to be used for the benchmark of optimisation algorithms ranging from Reinforcement Learning to Stochastic Games, through Transfer Learning, One-Shot Learning or others.",http://aclweb.org/anthology/D18-1547,D18-1,D18-1547,https://arxiv.org/abs/1707.01450,"('Paweł Budzianowski', 'Tsung-Hsien Wen', 'Bo-Hsiang Tseng', 'Iñigo Casanueva', 'Stefan Ultes', 'Osman Ramadan', 'Milica Gasic')",10,MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling,EMNLP,2018
"Semantic role labeling (SRL) is to recognize the predicate-argument structure of a sentence, including subtasks of predicate disambiguation and argument labeling. Previous studies usually formulate the entire SRL problem into two or more subtasks. For the first time, this paper introduces an end-to-end neural model which unifiedly tackles the predicate disambiguation and the argument labeling in one shot. Using a biaffine scorer, our model directly predicts all semantic role labels for all given word pairs in the sentence without relying on any syntactic parse information. Specifically, we augment the BiLSTM encoder with a non-linear transformation to further distinguish the predicate and the argument in a given sentence, and model the semantic role labeling process as a word pair classification task by employing the biaffine attentional mechanism. Though the proposed model is syntax-agnostic with local decoder, it outperforms the state-of-the-art syntax-aware SRL systems on the CoNLL-2008, 2009 benchmarks for both English and Chinese. To our best knowledge, we report the first syntax-agnostic SRL model that surpasses all known syntax-aware models.",http://aclweb.org/anthology/D18-1548,D18-1,D18-1548,https://arxiv.org/abs/1808.03815,"('Emma Strubell', 'Patrick Verga', 'Daniel Andor', 'David Weiss', 'Andrew McCallum')",10,Linguistically-Informed Self-Attention for Semantic Role Labeling,EMNLP,2018
"Recent work achieved remarkable results in training neural machine translation (NMT) systems in a fully unsupervised way, with new and dedicated architectures that rely on monolingual corpora only. In this work, we propose to define unsupervised NMT (UNMT) as NMT trained with the supervision of synthetic bilingual data. Our approach straightforwardly enables the use of state-of-the-art architectures proposed for supervised NMT by replacing human-made bilingual data with synthetic bilingual data for training. We propose to initialize the training of UNMT with synthetic bilingual data generated by unsupervised statistical machine translation (USMT). The UNMT system is then incrementally improved using back-translation. Our preliminary experiments show that our approach achieves a new state-of-the-art for unsupervised machine translation on the WMT16 German--English news translation task, for both translation directions.",http://aclweb.org/anthology/D18-1549,D18-1,D18-1549,https://arxiv.org/abs/1810.12703,"('Guillaume Lample', 'Myle Ott', 'Alexis Conneau', 'Ludovic Denoyer', ""Marc'Aurelio Ranzato"")",10,Phrase-Based & Neural Unsupervised Machine Translation,EMNLP,2018
